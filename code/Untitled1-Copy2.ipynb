{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfca335-29cc-423c-a5a1-16b167672c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Results will be saved in: results/20250508_082552\n",
      "Loading dataset...\n",
      "Dataset loaded.\n",
      "\n",
      "--- Training ComplexOscillatorNet ---\n",
      "Number of parameters: 4480\n",
      "Epoch 1/200, Train Loss: 29.2817, Val Loss: 27.0559\n",
      "  New best validation loss: 27.0559\n",
      "Epoch 2/200, Train Loss: 28.8193, Val Loss: 26.8465\n",
      "  New best validation loss: 26.8465\n",
      "Epoch 3/200, Train Loss: 28.4933, Val Loss: 26.7277\n",
      "  New best validation loss: 26.7277\n",
      "Epoch 4/200, Train Loss: 28.2453, Val Loss: 26.6262\n",
      "  New best validation loss: 26.6262\n",
      "Epoch 5/200, Train Loss: 28.0021, Val Loss: 26.4998\n",
      "  New best validation loss: 26.4998\n",
      "Epoch 6/200, Train Loss: 27.7891, Val Loss: 26.4488\n",
      "  New best validation loss: 26.4488\n",
      "Epoch 7/200, Train Loss: 27.5828, Val Loss: 26.3815\n",
      "  New best validation loss: 26.3815\n",
      "Epoch 8/200, Train Loss: 27.4085, Val Loss: 26.3616\n",
      "  New best validation loss: 26.3616\n",
      "Epoch 9/200, Train Loss: 27.2289, Val Loss: 26.2961\n",
      "  New best validation loss: 26.2961\n",
      "Epoch 10/200, Train Loss: 27.0854, Val Loss: 26.2716\n",
      "  New best validation loss: 26.2716\n",
      "Epoch 11/200, Train Loss: 26.9067, Val Loss: 26.2347\n",
      "  New best validation loss: 26.2347\n",
      "Epoch 12/200, Train Loss: 26.7645, Val Loss: 26.3185\n",
      "Epoch 13/200, Train Loss: 26.6429, Val Loss: 26.2535\n",
      "Epoch 14/200, Train Loss: 26.5084, Val Loss: 26.2837\n",
      "Epoch 15/200, Train Loss: 26.3607, Val Loss: 26.2660\n",
      "Epoch 16/200, Train Loss: 26.2389, Val Loss: 26.3206\n",
      "Epoch 17/200, Train Loss: 26.1534, Val Loss: 26.3074\n",
      "Epoch 18/200, Train Loss: 26.0188, Val Loss: 26.3156\n",
      "Epoch 19/200, Train Loss: 25.9129, Val Loss: 26.3565\n",
      "Epoch 20/200, Train Loss: 25.7997, Val Loss: 26.3767\n",
      "Epoch 21/200, Train Loss: 25.7615, Val Loss: 26.4182\n",
      "Epoch 22/200, Train Loss: 25.6525, Val Loss: 26.4537\n",
      "Epoch 23/200, Train Loss: 25.5169, Val Loss: 26.4826\n",
      "Epoch 24/200, Train Loss: 25.4122, Val Loss: 26.4863\n",
      "Epoch 25/200, Train Loss: 25.3286, Val Loss: 26.6282\n",
      "Epoch 26/200, Train Loss: 25.2539, Val Loss: 26.5398\n",
      "Epoch 27/200, Train Loss: 25.1377, Val Loss: 26.5672\n",
      "Epoch 28/200, Train Loss: 25.0403, Val Loss: 26.5574\n",
      "Epoch 29/200, Train Loss: 24.9562, Val Loss: 26.7147\n",
      "Epoch 30/200, Train Loss: 24.8685, Val Loss: 26.6196\n",
      "Epoch 31/200, Train Loss: 24.7799, Val Loss: 26.6646\n",
      "Epoch 32/200, Train Loss: 24.6938, Val Loss: 26.6386\n",
      "Epoch 33/200, Train Loss: 24.5762, Val Loss: 26.7956\n",
      "Epoch 34/200, Train Loss: 24.5082, Val Loss: 26.7958\n",
      "Epoch 35/200, Train Loss: 24.4026, Val Loss: 26.9725\n",
      "Epoch 36/200, Train Loss: 24.3016, Val Loss: 26.9319\n",
      "Epoch 37/200, Train Loss: 24.2531, Val Loss: 26.8704\n",
      "Epoch 38/200, Train Loss: 24.1946, Val Loss: 26.9310\n",
      "Epoch 39/200, Train Loss: 24.0694, Val Loss: 27.1208\n",
      "Epoch 40/200, Train Loss: 23.9679, Val Loss: 27.0905\n",
      "Epoch 41/200, Train Loss: 23.8936, Val Loss: 27.1604\n",
      "Epoch 42/200, Train Loss: 23.8503, Val Loss: 27.2994\n",
      "Epoch 43/200, Train Loss: 23.8079, Val Loss: 27.2270\n",
      "Epoch 44/200, Train Loss: 23.6880, Val Loss: 27.2533\n",
      "Epoch 45/200, Train Loss: 23.6356, Val Loss: 27.3866\n",
      "Epoch 46/200, Train Loss: 23.5656, Val Loss: 27.2536\n",
      "Epoch 47/200, Train Loss: 23.4863, Val Loss: 27.2947\n",
      "Epoch 48/200, Train Loss: 23.3765, Val Loss: 27.5749\n",
      "Epoch 49/200, Train Loss: 23.3305, Val Loss: 27.4691\n",
      "Epoch 50/200, Train Loss: 23.2512, Val Loss: 27.3656\n",
      "Epoch 51/200, Train Loss: 23.1967, Val Loss: 27.5357\n",
      "Epoch 52/200, Train Loss: 23.0886, Val Loss: 27.5275\n",
      "Epoch 53/200, Train Loss: 23.0100, Val Loss: 27.5716\n",
      "Epoch 54/200, Train Loss: 22.9943, Val Loss: 27.5808\n",
      "Epoch 55/200, Train Loss: 22.9122, Val Loss: 27.8425\n",
      "Epoch 56/200, Train Loss: 22.9109, Val Loss: 27.6113\n",
      "Epoch 57/200, Train Loss: 22.7919, Val Loss: 27.5987\n",
      "Epoch 58/200, Train Loss: 22.7514, Val Loss: 27.7267\n",
      "Epoch 59/200, Train Loss: 22.6832, Val Loss: 27.8526\n",
      "Epoch 60/200, Train Loss: 22.6434, Val Loss: 27.9105\n",
      "Epoch 61/200, Train Loss: 22.5837, Val Loss: 27.7034\n",
      "Epoch 62/200, Train Loss: 22.5348, Val Loss: 28.0009\n",
      "Epoch 63/200, Train Loss: 22.4506, Val Loss: 27.8115\n",
      "Epoch 64/200, Train Loss: 22.3901, Val Loss: 28.1612\n",
      "Epoch 65/200, Train Loss: 22.3924, Val Loss: 27.8328\n",
      "Epoch 66/200, Train Loss: 22.3700, Val Loss: 28.0141\n",
      "Epoch 67/200, Train Loss: 22.3701, Val Loss: 28.0502\n",
      "Epoch 68/200, Train Loss: 22.3044, Val Loss: 28.1371\n",
      "Epoch 69/200, Train Loss: 22.2671, Val Loss: 28.2398\n",
      "Epoch 70/200, Train Loss: 22.1949, Val Loss: 28.1769\n",
      "Epoch 71/200, Train Loss: 22.1305, Val Loss: 28.3351\n",
      "Epoch 72/200, Train Loss: 22.0940, Val Loss: 28.2000\n",
      "Epoch 73/200, Train Loss: 22.0464, Val Loss: 28.1942\n",
      "Epoch 74/200, Train Loss: 22.0038, Val Loss: 28.3373\n",
      "Epoch 75/200, Train Loss: 21.9967, Val Loss: 28.1953\n",
      "Epoch 76/200, Train Loss: 21.9594, Val Loss: 28.2183\n",
      "Epoch 77/200, Train Loss: 21.9140, Val Loss: 28.3944\n",
      "Epoch 78/200, Train Loss: 21.8713, Val Loss: 28.4778\n",
      "Epoch 79/200, Train Loss: 21.8396, Val Loss: 28.4658\n",
      "Epoch 80/200, Train Loss: 21.8290, Val Loss: 28.4374\n",
      "Epoch 81/200, Train Loss: 21.7856, Val Loss: 28.3490\n",
      "Epoch 82/200, Train Loss: 21.8465, Val Loss: 28.8266\n",
      "Epoch 83/200, Train Loss: 21.7822, Val Loss: 28.4498\n",
      "Epoch 84/200, Train Loss: 21.7766, Val Loss: 28.5424\n",
      "Epoch 85/200, Train Loss: 21.7133, Val Loss: 28.8659\n",
      "Epoch 86/200, Train Loss: 21.7003, Val Loss: 28.5712\n",
      "Epoch 87/200, Train Loss: 21.6067, Val Loss: 28.4851\n",
      "Epoch 88/200, Train Loss: 21.5344, Val Loss: 28.6456\n",
      "Epoch 89/200, Train Loss: 21.5543, Val Loss: 28.7398\n",
      "Epoch 90/200, Train Loss: 21.5122, Val Loss: 28.8356\n",
      "Epoch 91/200, Train Loss: 21.5506, Val Loss: 28.7166\n",
      "Epoch 92/200, Train Loss: 21.5237, Val Loss: 28.8865\n",
      "Epoch 93/200, Train Loss: 21.4512, Val Loss: 28.6740\n",
      "Epoch 94/200, Train Loss: 21.4917, Val Loss: 28.9829\n",
      "Epoch 95/200, Train Loss: 21.4805, Val Loss: 28.8540\n",
      "Epoch 96/200, Train Loss: 21.3915, Val Loss: 28.9936\n",
      "Epoch 97/200, Train Loss: 21.3399, Val Loss: 28.8362\n",
      "Epoch 98/200, Train Loss: 21.2700, Val Loss: 28.9993\n",
      "Epoch 99/200, Train Loss: 21.2261, Val Loss: 28.9063\n",
      "Epoch 100/200, Train Loss: 21.2058, Val Loss: 28.9359\n",
      "Epoch 101/200, Train Loss: 21.2576, Val Loss: 29.1552\n",
      "Epoch 102/200, Train Loss: 21.2120, Val Loss: 29.0497\n",
      "Epoch 103/200, Train Loss: 21.1499, Val Loss: 28.9760\n",
      "Epoch 104/200, Train Loss: 21.2069, Val Loss: 29.1683\n",
      "Epoch 105/200, Train Loss: 21.1387, Val Loss: 29.2147\n",
      "Epoch 106/200, Train Loss: 21.1337, Val Loss: 29.1762\n",
      "Epoch 107/200, Train Loss: 21.1172, Val Loss: 29.2933\n",
      "Epoch 108/200, Train Loss: 21.0788, Val Loss: 29.2527\n",
      "Epoch 109/200, Train Loss: 21.0418, Val Loss: 29.3022\n",
      "Epoch 110/200, Train Loss: 21.0435, Val Loss: 29.2560\n",
      "Epoch 111/200, Train Loss: 21.0347, Val Loss: 29.2150\n",
      "Epoch 112/200, Train Loss: 21.0735, Val Loss: 29.5294\n",
      "Epoch 113/200, Train Loss: 21.0311, Val Loss: 29.6475\n",
      "Epoch 114/200, Train Loss: 20.9698, Val Loss: 29.4025\n",
      "Epoch 115/200, Train Loss: 20.9306, Val Loss: 29.2911\n",
      "Epoch 116/200, Train Loss: 20.9098, Val Loss: 29.3204\n",
      "Epoch 117/200, Train Loss: 20.8800, Val Loss: 29.5317\n",
      "Epoch 118/200, Train Loss: 20.8557, Val Loss: 29.5087\n",
      "Epoch 119/200, Train Loss: 20.8017, Val Loss: 29.6054\n",
      "Epoch 120/200, Train Loss: 20.7700, Val Loss: 29.4026\n",
      "Epoch 121/200, Train Loss: 20.7682, Val Loss: 29.5930\n",
      "Epoch 122/200, Train Loss: 20.7358, Val Loss: 29.6297\n",
      "Epoch 123/200, Train Loss: 20.7153, Val Loss: 29.7055\n",
      "Epoch 124/200, Train Loss: 20.7447, Val Loss: 29.3676\n",
      "Epoch 125/200, Train Loss: 20.6857, Val Loss: 29.7597\n",
      "Epoch 126/200, Train Loss: 20.6306, Val Loss: 29.7191\n",
      "Epoch 127/200, Train Loss: 20.6441, Val Loss: 29.6722\n",
      "Epoch 128/200, Train Loss: 20.6542, Val Loss: 29.6304\n",
      "Epoch 129/200, Train Loss: 20.7137, Val Loss: 29.7977\n",
      "Epoch 130/200, Train Loss: 20.6838, Val Loss: 29.8213\n",
      "Epoch 131/200, Train Loss: 20.6379, Val Loss: 29.6694\n",
      "Epoch 132/200, Train Loss: 20.7186, Val Loss: 29.8535\n",
      "Epoch 133/200, Train Loss: 20.7194, Val Loss: 29.9928\n",
      "Epoch 134/200, Train Loss: 20.8086, Val Loss: 29.8524\n",
      "Epoch 135/200, Train Loss: 20.8633, Val Loss: 29.6449\n",
      "Epoch 136/200, Train Loss: 20.7475, Val Loss: 29.7224\n",
      "Epoch 137/200, Train Loss: 20.6798, Val Loss: 29.6492\n",
      "Epoch 138/200, Train Loss: 20.5948, Val Loss: 29.6683\n",
      "Epoch 139/200, Train Loss: 20.5407, Val Loss: 29.9590\n",
      "Epoch 140/200, Train Loss: 20.5377, Val Loss: 29.6266\n",
      "Epoch 141/200, Train Loss: 20.5765, Val Loss: 29.6845\n",
      "Epoch 142/200, Train Loss: 20.5342, Val Loss: 29.7491\n",
      "Epoch 143/200, Train Loss: 20.5672, Val Loss: 29.9131\n",
      "Epoch 144/200, Train Loss: 20.5206, Val Loss: 29.8653\n",
      "Epoch 145/200, Train Loss: 20.4773, Val Loss: 29.8086\n",
      "Epoch 146/200, Train Loss: 20.4535, Val Loss: 29.6949\n",
      "Epoch 147/200, Train Loss: 20.4988, Val Loss: 30.1651\n",
      "Epoch 148/200, Train Loss: 20.5108, Val Loss: 29.9383\n",
      "Epoch 149/200, Train Loss: 20.4469, Val Loss: 29.8735\n",
      "Epoch 150/200, Train Loss: 20.4343, Val Loss: 29.8056\n",
      "Epoch 151/200, Train Loss: 20.4010, Val Loss: 29.8383\n",
      "Epoch 152/200, Train Loss: 20.3565, Val Loss: 29.8851\n",
      "Epoch 153/200, Train Loss: 20.3780, Val Loss: 29.7552\n",
      "Epoch 154/200, Train Loss: 20.3803, Val Loss: 30.0248\n",
      "Epoch 155/200, Train Loss: 20.3481, Val Loss: 29.8120\n",
      "Epoch 156/200, Train Loss: 20.3357, Val Loss: 30.1130\n",
      "Epoch 157/200, Train Loss: 20.2617, Val Loss: 29.9121\n",
      "Epoch 158/200, Train Loss: 20.2401, Val Loss: 30.1830\n",
      "Epoch 159/200, Train Loss: 20.2405, Val Loss: 29.7834\n",
      "Epoch 160/200, Train Loss: 20.2037, Val Loss: 30.0577\n",
      "Epoch 161/200, Train Loss: 20.2110, Val Loss: 30.3287\n",
      "Epoch 162/200, Train Loss: 20.2087, Val Loss: 30.1142\n",
      "Epoch 163/200, Train Loss: 20.1976, Val Loss: 30.2635\n",
      "Epoch 164/200, Train Loss: 20.2157, Val Loss: 30.2924\n",
      "Epoch 165/200, Train Loss: 20.1847, Val Loss: 30.0861\n",
      "Epoch 166/200, Train Loss: 20.2241, Val Loss: 30.3622\n",
      "Epoch 167/200, Train Loss: 20.2492, Val Loss: 30.4111\n",
      "Epoch 168/200, Train Loss: 20.2531, Val Loss: 30.3928\n",
      "Epoch 169/200, Train Loss: 20.2306, Val Loss: 30.1758\n",
      "Epoch 170/200, Train Loss: 20.1716, Val Loss: 30.2675\n",
      "Epoch 171/200, Train Loss: 20.1859, Val Loss: 30.3395\n",
      "Epoch 172/200, Train Loss: 20.1537, Val Loss: 30.3418\n",
      "Epoch 173/200, Train Loss: 20.1272, Val Loss: 30.3511\n",
      "Epoch 174/200, Train Loss: 20.0465, Val Loss: 30.3071\n",
      "Epoch 175/200, Train Loss: 20.0163, Val Loss: 30.3937\n",
      "Epoch 176/200, Train Loss: 20.0071, Val Loss: 30.3348\n",
      "Epoch 177/200, Train Loss: 20.0109, Val Loss: 30.6162\n",
      "Epoch 178/200, Train Loss: 20.0511, Val Loss: 30.5512\n",
      "Epoch 179/200, Train Loss: 20.0308, Val Loss: 30.4573\n",
      "Epoch 180/200, Train Loss: 20.0273, Val Loss: 30.9320\n",
      "Epoch 181/200, Train Loss: 20.0571, Val Loss: 30.4984\n",
      "Epoch 182/200, Train Loss: 20.0501, Val Loss: 30.6610\n",
      "Epoch 183/200, Train Loss: 20.0383, Val Loss: 30.6111\n",
      "Epoch 184/200, Train Loss: 20.0324, Val Loss: 30.7531\n",
      "Epoch 185/200, Train Loss: 20.0089, Val Loss: 30.6809\n",
      "Epoch 186/200, Train Loss: 20.0820, Val Loss: 30.8982\n",
      "Epoch 187/200, Train Loss: 20.1941, Val Loss: 30.9163\n",
      "Epoch 188/200, Train Loss: 20.2368, Val Loss: 30.6682\n",
      "Epoch 189/200, Train Loss: 20.2418, Val Loss: 30.6198\n",
      "Epoch 190/200, Train Loss: 20.2979, Val Loss: 30.3803\n",
      "Epoch 191/200, Train Loss: 20.2252, Val Loss: 30.6628\n",
      "Epoch 192/200, Train Loss: 20.1602, Val Loss: 30.5388\n",
      "Epoch 193/200, Train Loss: 20.1903, Val Loss: 30.5922\n",
      "Epoch 194/200, Train Loss: 20.1143, Val Loss: 30.3414\n",
      "Epoch 195/200, Train Loss: 20.0877, Val Loss: 30.8498\n",
      "Epoch 196/200, Train Loss: 20.0002, Val Loss: 30.8298\n",
      "Epoch 197/200, Train Loss: 19.9554, Val Loss: 30.5060\n",
      "Epoch 198/200, Train Loss: 19.9034, Val Loss: 30.5771\n",
      "Epoch 199/200, Train Loss: 19.9120, Val Loss: 30.7640\n",
      "Epoch 200/200, Train Loss: 19.9204, Val Loss: 31.0025\n",
      "\n",
      "Loaded best model (Val Loss: 26.2347) for final hidden state extraction.\n",
      "Saved best model for ComplexOscillatorNet to results/20250508_082552/ComplexOscillatorNet_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for ComplexOscillatorNet ---\n",
      "  Analyzing decodability for ComplexOscillatorNet...\n",
      "  Hidden states shape: torch.Size([160, 200, 64])\n",
      "  Coefficients shape: torch.Size([160, 25])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for ComplexOscillatorNet - Test MSE: 1.9435, Test R2: 0.0197 (best alpha: 100.0000)\n",
      "Decodability (R2 score) for ComplexOscillatorNet: 0.0197\n",
      "\n",
      "--- Training RNN_GRU ---\n",
      "Number of parameters: 50561\n",
      "Epoch 1/200, Train Loss: 28.9905, Val Loss: 26.9323\n",
      "  New best validation loss: 26.9323\n",
      "Epoch 2/200, Train Loss: 28.8746, Val Loss: 26.8757\n",
      "  New best validation loss: 26.8757\n",
      "Epoch 3/200, Train Loss: 28.7561, Val Loss: 26.8288\n",
      "  New best validation loss: 26.8288\n",
      "Epoch 4/200, Train Loss: 28.6743, Val Loss: 26.9001\n",
      "Epoch 5/200, Train Loss: 28.6995, Val Loss: 26.8490\n",
      "Epoch 6/200, Train Loss: 28.6867, Val Loss: 26.8019\n",
      "  New best validation loss: 26.8019\n",
      "Epoch 7/200, Train Loss: 28.6547, Val Loss: 26.8064\n",
      "Epoch 8/200, Train Loss: 28.6559, Val Loss: 26.8093\n",
      "Epoch 9/200, Train Loss: 28.6430, Val Loss: 26.8298\n",
      "Epoch 10/200, Train Loss: 28.6341, Val Loss: 26.8200\n",
      "Epoch 11/200, Train Loss: 28.5926, Val Loss: 26.8174\n",
      "Epoch 12/200, Train Loss: 28.6163, Val Loss: 26.8166\n",
      "Epoch 13/200, Train Loss: 28.6263, Val Loss: 26.7772\n",
      "  New best validation loss: 26.7772\n",
      "Epoch 14/200, Train Loss: 28.6059, Val Loss: 26.7914\n",
      "Epoch 15/200, Train Loss: 28.5681, Val Loss: 26.7836\n",
      "Epoch 16/200, Train Loss: 28.5977, Val Loss: 26.8099\n",
      "Epoch 17/200, Train Loss: 28.5618, Val Loss: 26.7799\n",
      "Epoch 18/200, Train Loss: 28.5623, Val Loss: 26.7446\n",
      "  New best validation loss: 26.7446\n",
      "Epoch 19/200, Train Loss: 28.5986, Val Loss: 26.7755\n",
      "Epoch 20/200, Train Loss: 28.5793, Val Loss: 26.8081\n",
      "Epoch 21/200, Train Loss: 28.5741, Val Loss: 26.7436\n",
      "  New best validation loss: 26.7436\n",
      "Epoch 22/200, Train Loss: 28.5614, Val Loss: 26.7070\n",
      "  New best validation loss: 26.7070\n",
      "Epoch 23/200, Train Loss: 28.5301, Val Loss: 26.7238\n",
      "Epoch 24/200, Train Loss: 28.5026, Val Loss: 26.7378\n",
      "Epoch 25/200, Train Loss: 28.4966, Val Loss: 26.7016\n",
      "  New best validation loss: 26.7016\n",
      "Epoch 26/200, Train Loss: 28.4884, Val Loss: 26.6655\n",
      "  New best validation loss: 26.6655\n",
      "Epoch 27/200, Train Loss: 28.4814, Val Loss: 26.7309\n",
      "Epoch 28/200, Train Loss: 28.4755, Val Loss: 26.6423\n",
      "  New best validation loss: 26.6423\n",
      "Epoch 29/200, Train Loss: 28.4739, Val Loss: 26.6091\n",
      "  New best validation loss: 26.6091\n",
      "Epoch 30/200, Train Loss: 28.4314, Val Loss: 26.6640\n",
      "Epoch 31/200, Train Loss: 28.4399, Val Loss: 26.6239\n",
      "Epoch 32/200, Train Loss: 28.4474, Val Loss: 26.5954\n",
      "  New best validation loss: 26.5954\n",
      "Epoch 33/200, Train Loss: 28.4003, Val Loss: 26.5460\n",
      "  New best validation loss: 26.5460\n",
      "Epoch 34/200, Train Loss: 28.4155, Val Loss: 26.6463\n",
      "Epoch 35/200, Train Loss: 28.3433, Val Loss: 26.5311\n",
      "  New best validation loss: 26.5311\n",
      "Epoch 36/200, Train Loss: 28.2931, Val Loss: 26.5321\n",
      "Epoch 37/200, Train Loss: 28.2431, Val Loss: 26.4454\n",
      "  New best validation loss: 26.4454\n",
      "Epoch 38/200, Train Loss: 28.2293, Val Loss: 26.4737\n",
      "Epoch 39/200, Train Loss: 28.1960, Val Loss: 26.4794\n",
      "Epoch 40/200, Train Loss: 28.1243, Val Loss: 26.3667\n",
      "  New best validation loss: 26.3667\n",
      "Epoch 41/200, Train Loss: 28.1205, Val Loss: 26.3368\n",
      "  New best validation loss: 26.3368\n",
      "Epoch 42/200, Train Loss: 28.0586, Val Loss: 26.4526\n",
      "Epoch 43/200, Train Loss: 27.9983, Val Loss: 26.3695\n",
      "Epoch 44/200, Train Loss: 28.0579, Val Loss: 26.2739\n",
      "  New best validation loss: 26.2739\n",
      "Epoch 45/200, Train Loss: 28.0019, Val Loss: 26.3711\n",
      "Epoch 46/200, Train Loss: 28.0119, Val Loss: 26.2441\n",
      "  New best validation loss: 26.2441\n",
      "Epoch 47/200, Train Loss: 27.9023, Val Loss: 26.3401\n",
      "Epoch 48/200, Train Loss: 27.9032, Val Loss: 26.3994\n",
      "Epoch 49/200, Train Loss: 27.8070, Val Loss: 26.1277\n",
      "  New best validation loss: 26.1277\n",
      "Epoch 50/200, Train Loss: 27.8138, Val Loss: 26.3518\n",
      "Epoch 51/200, Train Loss: 27.6560, Val Loss: 26.2312\n",
      "Epoch 52/200, Train Loss: 27.6087, Val Loss: 26.1904\n",
      "Epoch 53/200, Train Loss: 27.4286, Val Loss: 26.1984\n",
      "Epoch 54/200, Train Loss: 27.5932, Val Loss: 26.0830\n",
      "  New best validation loss: 26.0830\n",
      "Epoch 55/200, Train Loss: 27.4822, Val Loss: 26.3720\n",
      "Epoch 56/200, Train Loss: 27.3348, Val Loss: 26.2643\n",
      "Epoch 57/200, Train Loss: 27.0939, Val Loss: 26.2861\n",
      "Epoch 58/200, Train Loss: 27.0928, Val Loss: 26.2503\n",
      "Epoch 59/200, Train Loss: 27.0930, Val Loss: 26.0726\n",
      "  New best validation loss: 26.0726\n",
      "Epoch 60/200, Train Loss: 26.8752, Val Loss: 26.2299\n",
      "Epoch 61/200, Train Loss: 27.1670, Val Loss: 26.3601\n",
      "Epoch 62/200, Train Loss: 27.4544, Val Loss: 26.3686\n",
      "Epoch 63/200, Train Loss: 26.9830, Val Loss: 26.3549\n",
      "Epoch 64/200, Train Loss: 26.6313, Val Loss: 26.4510\n",
      "Epoch 65/200, Train Loss: 26.9917, Val Loss: 26.2554\n",
      "Epoch 66/200, Train Loss: 26.6820, Val Loss: 26.1209\n",
      "Epoch 67/200, Train Loss: 26.4470, Val Loss: 26.2327\n",
      "Epoch 68/200, Train Loss: 27.0149, Val Loss: 26.5987\n",
      "Epoch 69/200, Train Loss: 26.9765, Val Loss: 26.4737\n",
      "Epoch 70/200, Train Loss: 26.7344, Val Loss: 26.5468\n",
      "Epoch 71/200, Train Loss: 26.7490, Val Loss: 26.4776\n",
      "Epoch 72/200, Train Loss: 26.4621, Val Loss: 26.1400\n",
      "Epoch 73/200, Train Loss: 26.6591, Val Loss: 26.4172\n",
      "Epoch 74/200, Train Loss: 26.9210, Val Loss: 27.0365\n",
      "Epoch 75/200, Train Loss: 26.9742, Val Loss: 26.2566\n",
      "Epoch 76/200, Train Loss: 26.4291, Val Loss: 26.4203\n",
      "Epoch 77/200, Train Loss: 26.6526, Val Loss: 26.4893\n",
      "Epoch 78/200, Train Loss: 26.2718, Val Loss: 26.7153\n",
      "Epoch 79/200, Train Loss: 26.2185, Val Loss: 26.3486\n",
      "Epoch 80/200, Train Loss: 26.2349, Val Loss: 26.8984\n",
      "Epoch 81/200, Train Loss: 26.3507, Val Loss: 26.8136\n",
      "Epoch 82/200, Train Loss: 26.1747, Val Loss: 26.5953\n",
      "Epoch 83/200, Train Loss: 26.0106, Val Loss: 26.4192\n",
      "Epoch 84/200, Train Loss: 26.2478, Val Loss: 26.5893\n",
      "Epoch 85/200, Train Loss: 25.7612, Val Loss: 26.1828\n",
      "Epoch 86/200, Train Loss: 25.7886, Val Loss: 26.3721\n",
      "Epoch 87/200, Train Loss: 25.9984, Val Loss: 26.4111\n",
      "Epoch 88/200, Train Loss: 25.8865, Val Loss: 26.4226\n",
      "Epoch 89/200, Train Loss: 25.5537, Val Loss: 26.6280\n",
      "Epoch 90/200, Train Loss: 25.3964, Val Loss: 26.4740\n",
      "Epoch 91/200, Train Loss: 25.4295, Val Loss: 26.8833\n",
      "Epoch 92/200, Train Loss: 25.2942, Val Loss: 26.6422\n",
      "Epoch 93/200, Train Loss: 25.5709, Val Loss: 26.7185\n",
      "Epoch 94/200, Train Loss: 25.5305, Val Loss: 26.4578\n",
      "Epoch 95/200, Train Loss: 25.2101, Val Loss: 26.5887\n",
      "Epoch 96/200, Train Loss: 25.3999, Val Loss: 26.7359\n",
      "Epoch 97/200, Train Loss: 25.2758, Val Loss: 27.0858\n",
      "Epoch 98/200, Train Loss: 25.4733, Val Loss: 26.5975\n",
      "Epoch 99/200, Train Loss: 25.3733, Val Loss: 26.6423\n",
      "Epoch 100/200, Train Loss: 25.2586, Val Loss: 26.7802\n",
      "Epoch 101/200, Train Loss: 25.2919, Val Loss: 26.4515\n",
      "Epoch 102/200, Train Loss: 24.8317, Val Loss: 26.6978\n",
      "Epoch 103/200, Train Loss: 25.0111, Val Loss: 26.8734\n",
      "Epoch 104/200, Train Loss: 25.4104, Val Loss: 26.7708\n",
      "Epoch 105/200, Train Loss: 25.3642, Val Loss: 26.8770\n",
      "Epoch 106/200, Train Loss: 24.9966, Val Loss: 26.6882\n",
      "Epoch 107/200, Train Loss: 24.6534, Val Loss: 27.0560\n",
      "Epoch 108/200, Train Loss: 25.9786, Val Loss: 27.6494\n",
      "Epoch 109/200, Train Loss: 27.5947, Val Loss: 26.3803\n",
      "Epoch 110/200, Train Loss: 26.2827, Val Loss: 26.3703\n",
      "Epoch 111/200, Train Loss: 25.8144, Val Loss: 26.3865\n",
      "Epoch 112/200, Train Loss: 25.4222, Val Loss: 26.5979\n",
      "Epoch 113/200, Train Loss: 25.1508, Val Loss: 26.8763\n",
      "Epoch 114/200, Train Loss: 25.4631, Val Loss: 26.9962\n",
      "Epoch 115/200, Train Loss: 25.7186, Val Loss: 26.8651\n",
      "Epoch 116/200, Train Loss: 25.7341, Val Loss: 26.6132\n",
      "Epoch 117/200, Train Loss: 26.1055, Val Loss: 26.2079\n",
      "Epoch 118/200, Train Loss: 26.3429, Val Loss: 26.5114\n",
      "Epoch 119/200, Train Loss: 26.0967, Val Loss: 26.3877\n",
      "Epoch 120/200, Train Loss: 25.5613, Val Loss: 26.5574\n",
      "Epoch 121/200, Train Loss: 25.0886, Val Loss: 26.4440\n",
      "Epoch 122/200, Train Loss: 24.8455, Val Loss: 26.6935\n",
      "Epoch 123/200, Train Loss: 24.6774, Val Loss: 26.7562\n",
      "Epoch 124/200, Train Loss: 24.6669, Val Loss: 26.8814\n",
      "Epoch 125/200, Train Loss: 24.6177, Val Loss: 26.5924\n",
      "Epoch 126/200, Train Loss: 24.3613, Val Loss: 27.0249\n",
      "Epoch 127/200, Train Loss: 24.5101, Val Loss: 26.7989\n",
      "Epoch 128/200, Train Loss: 24.3405, Val Loss: 26.8264\n",
      "Epoch 129/200, Train Loss: 24.6957, Val Loss: 26.8303\n",
      "Epoch 130/200, Train Loss: 24.7600, Val Loss: 26.5788\n",
      "Epoch 131/200, Train Loss: 24.4478, Val Loss: 26.9032\n",
      "Epoch 132/200, Train Loss: 24.2701, Val Loss: 27.0167\n",
      "Epoch 133/200, Train Loss: 23.9743, Val Loss: 27.0858\n",
      "Epoch 134/200, Train Loss: 24.0448, Val Loss: 26.9573\n",
      "Epoch 135/200, Train Loss: 24.3069, Val Loss: 27.0705\n",
      "Epoch 136/200, Train Loss: 24.0127, Val Loss: 27.1018\n",
      "Epoch 137/200, Train Loss: 24.4340, Val Loss: 27.2490\n",
      "Epoch 138/200, Train Loss: 24.5916, Val Loss: 26.9377\n",
      "Epoch 139/200, Train Loss: 24.5508, Val Loss: 27.1205\n",
      "Epoch 140/200, Train Loss: 24.7306, Val Loss: 26.9567\n",
      "Epoch 141/200, Train Loss: 24.6744, Val Loss: 26.6214\n",
      "Epoch 142/200, Train Loss: 24.0124, Val Loss: 26.9620\n",
      "Epoch 143/200, Train Loss: 23.9180, Val Loss: 27.3231\n",
      "Epoch 144/200, Train Loss: 23.6733, Val Loss: 27.5120\n",
      "Epoch 145/200, Train Loss: 23.4912, Val Loss: 27.4341\n",
      "Epoch 146/200, Train Loss: 23.4092, Val Loss: 27.3384\n",
      "Epoch 147/200, Train Loss: 23.3815, Val Loss: 27.5345\n",
      "Epoch 148/200, Train Loss: 23.8478, Val Loss: 27.2702\n",
      "Epoch 149/200, Train Loss: 24.5235, Val Loss: 26.9149\n",
      "Epoch 150/200, Train Loss: 24.4151, Val Loss: 27.4899\n",
      "Epoch 151/200, Train Loss: 24.6308, Val Loss: 27.8421\n",
      "Epoch 152/200, Train Loss: 24.1112, Val Loss: 27.2528\n",
      "Epoch 153/200, Train Loss: 25.6527, Val Loss: 27.2180\n",
      "Epoch 154/200, Train Loss: 27.0322, Val Loss: 26.8845\n",
      "Epoch 155/200, Train Loss: 26.8613, Val Loss: 26.8400\n",
      "Epoch 156/200, Train Loss: 25.8853, Val Loss: 26.6277\n",
      "Epoch 157/200, Train Loss: 25.0980, Val Loss: 26.2646\n",
      "Epoch 158/200, Train Loss: 24.4904, Val Loss: 26.8664\n",
      "Epoch 159/200, Train Loss: 24.3554, Val Loss: 27.2057\n",
      "Epoch 160/200, Train Loss: 23.9553, Val Loss: 27.4845\n",
      "Epoch 161/200, Train Loss: 23.9261, Val Loss: 27.3211\n",
      "Epoch 162/200, Train Loss: 23.7712, Val Loss: 27.4956\n",
      "Epoch 163/200, Train Loss: 23.7541, Val Loss: 27.4964\n",
      "Epoch 164/200, Train Loss: 23.6092, Val Loss: 27.7679\n",
      "Epoch 165/200, Train Loss: 23.4481, Val Loss: 27.5514\n",
      "Epoch 166/200, Train Loss: 23.5197, Val Loss: 27.7180\n",
      "Epoch 167/200, Train Loss: 23.1988, Val Loss: 27.4625\n",
      "Epoch 168/200, Train Loss: 23.5608, Val Loss: 28.0105\n",
      "Epoch 169/200, Train Loss: 23.9723, Val Loss: 27.3800\n",
      "Epoch 170/200, Train Loss: 24.1608, Val Loss: 27.2593\n",
      "Epoch 171/200, Train Loss: 23.9520, Val Loss: 27.2588\n",
      "Epoch 172/200, Train Loss: 23.6037, Val Loss: 27.6472\n",
      "Epoch 173/200, Train Loss: 23.3810, Val Loss: 27.1814\n",
      "Epoch 174/200, Train Loss: 22.9389, Val Loss: 27.9728\n",
      "Epoch 175/200, Train Loss: 22.7357, Val Loss: 28.0820\n",
      "Epoch 176/200, Train Loss: 22.7350, Val Loss: 27.9721\n",
      "Epoch 177/200, Train Loss: 22.8010, Val Loss: 27.8807\n",
      "Epoch 178/200, Train Loss: 22.7830, Val Loss: 27.8810\n",
      "Epoch 179/200, Train Loss: 22.6974, Val Loss: 28.2310\n",
      "Epoch 180/200, Train Loss: 22.5751, Val Loss: 28.2017\n",
      "Epoch 181/200, Train Loss: 23.2470, Val Loss: 28.6635\n",
      "Epoch 182/200, Train Loss: 24.2209, Val Loss: 27.4352\n",
      "Epoch 183/200, Train Loss: 25.4150, Val Loss: 26.8610\n",
      "Epoch 184/200, Train Loss: 24.3226, Val Loss: 26.7683\n",
      "Epoch 185/200, Train Loss: 23.5014, Val Loss: 27.2989\n",
      "Epoch 186/200, Train Loss: 23.1447, Val Loss: 27.5756\n",
      "Epoch 187/200, Train Loss: 23.0750, Val Loss: 27.8138\n",
      "Epoch 188/200, Train Loss: 24.4439, Val Loss: 27.8410\n",
      "Epoch 189/200, Train Loss: 24.8805, Val Loss: 26.8153\n",
      "Epoch 190/200, Train Loss: 24.2810, Val Loss: 27.1101\n",
      "Epoch 191/200, Train Loss: 23.6413, Val Loss: 27.2065\n",
      "Epoch 192/200, Train Loss: 23.7500, Val Loss: 27.4255\n",
      "Epoch 193/200, Train Loss: 23.5580, Val Loss: 27.0965\n",
      "Epoch 194/200, Train Loss: 23.0308, Val Loss: 27.5798\n",
      "Epoch 195/200, Train Loss: 22.8722, Val Loss: 27.6735\n",
      "Epoch 196/200, Train Loss: 22.5557, Val Loss: 27.9836\n",
      "Epoch 197/200, Train Loss: 22.2979, Val Loss: 28.0734\n",
      "Epoch 198/200, Train Loss: 21.9162, Val Loss: 27.8753\n",
      "Epoch 199/200, Train Loss: 21.8743, Val Loss: 28.1170\n",
      "Epoch 200/200, Train Loss: 21.7986, Val Loss: 28.2179\n",
      "\n",
      "Loaded best model (Val Loss: 26.0726) for final hidden state extraction.\n",
      "Saved best model for RNN_GRU to results/20250508_082552/RNN_GRU_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for RNN_GRU ---\n",
      "  Analyzing decodability for RNN_GRU...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 25])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for RNN_GRU - Test MSE: 2.0504, Test R2: -0.0138 (best alpha: 100.0000)\n",
      "Decodability (R2 score) for RNN_GRU: -0.0138\n",
      "\n",
      "--- Training Transformer ---\n",
      "Number of parameters: 281345\n",
      "Epoch 1/200, Train Loss: 30.4577, Val Loss: 27.2994\n",
      "  New best validation loss: 27.2994\n",
      "Epoch 2/200, Train Loss: 29.3786, Val Loss: 27.4270\n",
      "Epoch 3/200, Train Loss: 29.2044, Val Loss: 27.1546\n",
      "  New best validation loss: 27.1546\n",
      "Epoch 4/200, Train Loss: 29.0669, Val Loss: 27.1297\n",
      "  New best validation loss: 27.1297\n",
      "Epoch 5/200, Train Loss: 29.1172, Val Loss: 27.1599\n",
      "Epoch 6/200, Train Loss: 29.0119, Val Loss: 27.0418\n",
      "  New best validation loss: 27.0418\n",
      "Epoch 7/200, Train Loss: 29.0533, Val Loss: 27.0663\n",
      "Epoch 8/200, Train Loss: 29.0064, Val Loss: 27.0269\n",
      "  New best validation loss: 27.0269\n",
      "Epoch 9/200, Train Loss: 29.0012, Val Loss: 27.0739\n",
      "Epoch 10/200, Train Loss: 29.0311, Val Loss: 27.0153\n",
      "  New best validation loss: 27.0153\n",
      "Epoch 11/200, Train Loss: 29.0557, Val Loss: 27.1309\n",
      "Epoch 12/200, Train Loss: 29.1462, Val Loss: 27.0466\n",
      "Epoch 13/200, Train Loss: 28.9743, Val Loss: 27.0044\n",
      "  New best validation loss: 27.0044\n",
      "Epoch 14/200, Train Loss: 28.9957, Val Loss: 27.0252\n",
      "Epoch 15/200, Train Loss: 28.9913, Val Loss: 27.0122\n",
      "Epoch 16/200, Train Loss: 28.9418, Val Loss: 27.0907\n",
      "Epoch 17/200, Train Loss: 29.0063, Val Loss: 26.9859\n",
      "  New best validation loss: 26.9859\n",
      "Epoch 18/200, Train Loss: 28.9819, Val Loss: 26.9677\n",
      "  New best validation loss: 26.9677\n",
      "Epoch 19/200, Train Loss: 28.9683, Val Loss: 26.9694\n",
      "Epoch 20/200, Train Loss: 29.0129, Val Loss: 27.0894\n",
      "Epoch 21/200, Train Loss: 29.0549, Val Loss: 27.2227\n",
      "Epoch 22/200, Train Loss: 29.0503, Val Loss: 26.9732\n",
      "Epoch 23/200, Train Loss: 28.9586, Val Loss: 27.0027\n",
      "Epoch 24/200, Train Loss: 28.9253, Val Loss: 26.9355\n",
      "  New best validation loss: 26.9355\n",
      "Epoch 25/200, Train Loss: 28.9010, Val Loss: 26.9383\n",
      "Epoch 26/200, Train Loss: 28.8796, Val Loss: 26.8780\n",
      "  New best validation loss: 26.8780\n",
      "Epoch 27/200, Train Loss: 28.8516, Val Loss: 26.9277\n",
      "Epoch 28/200, Train Loss: 28.8686, Val Loss: 26.8705\n",
      "  New best validation loss: 26.8705\n",
      "Epoch 29/200, Train Loss: 28.8392, Val Loss: 26.8509\n",
      "  New best validation loss: 26.8509\n",
      "Epoch 30/200, Train Loss: 28.8843, Val Loss: 26.8717\n",
      "Epoch 31/200, Train Loss: 28.9273, Val Loss: 26.9427\n",
      "Epoch 32/200, Train Loss: 28.9387, Val Loss: 27.0579\n",
      "Epoch 33/200, Train Loss: 28.9230, Val Loss: 26.8705\n",
      "Epoch 34/200, Train Loss: 28.8653, Val Loss: 26.8633\n",
      "Epoch 35/200, Train Loss: 28.8946, Val Loss: 27.0621\n",
      "Epoch 36/200, Train Loss: 28.8612, Val Loss: 26.9394\n",
      "Epoch 37/200, Train Loss: 28.9213, Val Loss: 26.9821\n",
      "Epoch 38/200, Train Loss: 28.8165, Val Loss: 26.8100\n",
      "  New best validation loss: 26.8100\n",
      "Epoch 39/200, Train Loss: 28.7609, Val Loss: 26.8649\n",
      "Epoch 40/200, Train Loss: 28.8486, Val Loss: 26.7892\n",
      "  New best validation loss: 26.7892\n",
      "Epoch 41/200, Train Loss: 28.7827, Val Loss: 26.7125\n",
      "  New best validation loss: 26.7125\n",
      "Epoch 42/200, Train Loss: 28.7974, Val Loss: 27.0047\n",
      "Epoch 43/200, Train Loss: 28.8975, Val Loss: 26.9567\n",
      "Epoch 44/200, Train Loss: 28.8569, Val Loss: 26.8291\n",
      "Epoch 45/200, Train Loss: 28.7493, Val Loss: 26.7115\n",
      "  New best validation loss: 26.7115\n",
      "Epoch 46/200, Train Loss: 28.7346, Val Loss: 26.7896\n",
      "Epoch 47/200, Train Loss: 28.7507, Val Loss: 26.7826\n",
      "Epoch 48/200, Train Loss: 28.7505, Val Loss: 26.6236\n",
      "  New best validation loss: 26.6236\n",
      "Epoch 49/200, Train Loss: 28.7447, Val Loss: 26.6699\n",
      "Epoch 50/200, Train Loss: 28.7531, Val Loss: 26.8988\n",
      "Epoch 51/200, Train Loss: 28.6920, Val Loss: 26.6369\n",
      "Epoch 52/200, Train Loss: 28.6959, Val Loss: 26.6827\n",
      "Epoch 53/200, Train Loss: 28.6540, Val Loss: 26.6314\n",
      "Epoch 54/200, Train Loss: 28.6221, Val Loss: 26.6387\n",
      "Epoch 55/200, Train Loss: 28.5670, Val Loss: 26.5582\n",
      "  New best validation loss: 26.5582\n",
      "Epoch 56/200, Train Loss: 28.6277, Val Loss: 26.5126\n",
      "  New best validation loss: 26.5126\n",
      "Epoch 57/200, Train Loss: 28.5641, Val Loss: 26.6413\n",
      "Epoch 58/200, Train Loss: 28.5781, Val Loss: 26.5435\n",
      "Epoch 59/200, Train Loss: 28.5690, Val Loss: 26.4288\n",
      "  New best validation loss: 26.4288\n",
      "Epoch 60/200, Train Loss: 28.6836, Val Loss: 26.8010\n",
      "Epoch 61/200, Train Loss: 28.6112, Val Loss: 26.8202\n",
      "Epoch 62/200, Train Loss: 28.6240, Val Loss: 26.6315\n",
      "Epoch 63/200, Train Loss: 28.6710, Val Loss: 26.7223\n",
      "Epoch 64/200, Train Loss: 28.5170, Val Loss: 26.6781\n",
      "Epoch 65/200, Train Loss: 28.4675, Val Loss: 26.5391\n",
      "Epoch 66/200, Train Loss: 28.4829, Val Loss: 26.6198\n",
      "Epoch 67/200, Train Loss: 28.4632, Val Loss: 26.5579\n",
      "Epoch 68/200, Train Loss: 28.4965, Val Loss: 26.3283\n",
      "  New best validation loss: 26.3283\n",
      "Epoch 69/200, Train Loss: 28.5240, Val Loss: 26.8223\n",
      "Epoch 70/200, Train Loss: 28.5958, Val Loss: 26.6313\n",
      "Epoch 71/200, Train Loss: 28.4879, Val Loss: 26.4896\n",
      "Epoch 72/200, Train Loss: 28.3598, Val Loss: 26.4417\n",
      "Epoch 73/200, Train Loss: 28.3408, Val Loss: 26.3381\n",
      "Epoch 74/200, Train Loss: 28.3970, Val Loss: 26.6703\n",
      "Epoch 75/200, Train Loss: 28.3304, Val Loss: 26.2380\n",
      "  New best validation loss: 26.2380\n",
      "Epoch 76/200, Train Loss: 28.2862, Val Loss: 26.3822\n",
      "Epoch 77/200, Train Loss: 28.2757, Val Loss: 26.4257\n",
      "Epoch 78/200, Train Loss: 28.2869, Val Loss: 27.0953\n",
      "Epoch 79/200, Train Loss: 28.3925, Val Loss: 26.3963\n",
      "Epoch 80/200, Train Loss: 28.2776, Val Loss: 26.4076\n",
      "Epoch 81/200, Train Loss: 28.2590, Val Loss: 26.2611\n",
      "Epoch 82/200, Train Loss: 28.2623, Val Loss: 26.3962\n",
      "Epoch 83/200, Train Loss: 28.2012, Val Loss: 26.2522\n",
      "Epoch 84/200, Train Loss: 28.1288, Val Loss: 26.4254\n",
      "Epoch 85/200, Train Loss: 28.0940, Val Loss: 26.2252\n",
      "  New best validation loss: 26.2252\n",
      "Epoch 86/200, Train Loss: 28.2019, Val Loss: 26.2338\n",
      "Epoch 87/200, Train Loss: 28.1326, Val Loss: 26.4548\n",
      "Epoch 88/200, Train Loss: 28.1439, Val Loss: 26.0756\n",
      "  New best validation loss: 26.0756\n",
      "Epoch 89/200, Train Loss: 27.9851, Val Loss: 26.1698\n",
      "Epoch 90/200, Train Loss: 28.0983, Val Loss: 26.0696\n",
      "  New best validation loss: 26.0696\n",
      "Epoch 91/200, Train Loss: 28.0272, Val Loss: 26.1158\n",
      "Epoch 92/200, Train Loss: 28.1305, Val Loss: 26.2040\n",
      "Epoch 93/200, Train Loss: 28.1127, Val Loss: 26.3030\n",
      "Epoch 94/200, Train Loss: 28.0574, Val Loss: 26.3173\n",
      "Epoch 95/200, Train Loss: 28.0711, Val Loss: 26.1082\n",
      "Epoch 96/200, Train Loss: 27.9192, Val Loss: 26.0434\n",
      "  New best validation loss: 26.0434\n",
      "Epoch 97/200, Train Loss: 27.9115, Val Loss: 26.2594\n",
      "Epoch 98/200, Train Loss: 27.9171, Val Loss: 25.9760\n",
      "  New best validation loss: 25.9760\n",
      "Epoch 99/200, Train Loss: 27.8737, Val Loss: 26.0092\n",
      "Epoch 100/200, Train Loss: 27.8270, Val Loss: 26.2535\n",
      "Epoch 101/200, Train Loss: 27.8928, Val Loss: 26.1646\n",
      "Epoch 102/200, Train Loss: 27.8302, Val Loss: 26.3357\n",
      "Epoch 103/200, Train Loss: 27.8179, Val Loss: 26.0012\n",
      "Epoch 104/200, Train Loss: 27.8513, Val Loss: 26.0028\n",
      "Epoch 105/200, Train Loss: 27.7974, Val Loss: 26.2195\n",
      "Epoch 106/200, Train Loss: 27.7471, Val Loss: 25.9860\n",
      "Epoch 107/200, Train Loss: 27.6814, Val Loss: 26.0551\n",
      "Epoch 108/200, Train Loss: 27.6753, Val Loss: 26.0940\n",
      "Epoch 109/200, Train Loss: 27.6297, Val Loss: 26.0709\n",
      "Epoch 110/200, Train Loss: 27.7040, Val Loss: 26.0540\n",
      "Epoch 111/200, Train Loss: 27.6944, Val Loss: 26.0078\n",
      "Epoch 112/200, Train Loss: 27.6094, Val Loss: 25.9729\n",
      "  New best validation loss: 25.9729\n",
      "Epoch 113/200, Train Loss: 27.5792, Val Loss: 25.9069\n",
      "  New best validation loss: 25.9069\n",
      "Epoch 114/200, Train Loss: 27.5649, Val Loss: 25.9780\n",
      "Epoch 115/200, Train Loss: 27.5629, Val Loss: 25.8727\n",
      "  New best validation loss: 25.8727\n",
      "Epoch 116/200, Train Loss: 27.4269, Val Loss: 25.8643\n",
      "  New best validation loss: 25.8643\n",
      "Epoch 117/200, Train Loss: 27.4591, Val Loss: 25.8013\n",
      "  New best validation loss: 25.8013\n",
      "Epoch 118/200, Train Loss: 27.4593, Val Loss: 26.4279\n",
      "Epoch 119/200, Train Loss: 27.6094, Val Loss: 25.8816\n",
      "Epoch 120/200, Train Loss: 27.4950, Val Loss: 26.0499\n",
      "Epoch 121/200, Train Loss: 27.3261, Val Loss: 25.9501\n",
      "Epoch 122/200, Train Loss: 27.3338, Val Loss: 25.8770\n",
      "Epoch 123/200, Train Loss: 27.3357, Val Loss: 25.9515\n",
      "Epoch 124/200, Train Loss: 27.3233, Val Loss: 25.8286\n",
      "Epoch 125/200, Train Loss: 27.2627, Val Loss: 26.1292\n",
      "Epoch 126/200, Train Loss: 27.2577, Val Loss: 25.7459\n",
      "  New best validation loss: 25.7459\n",
      "Epoch 127/200, Train Loss: 27.2208, Val Loss: 25.8212\n",
      "Epoch 128/200, Train Loss: 27.2007, Val Loss: 26.1574\n",
      "Epoch 129/200, Train Loss: 27.1828, Val Loss: 25.7800\n",
      "Epoch 130/200, Train Loss: 27.0065, Val Loss: 25.9751\n",
      "Epoch 131/200, Train Loss: 26.9244, Val Loss: 25.8486\n",
      "Epoch 132/200, Train Loss: 26.7573, Val Loss: 25.7973\n",
      "Epoch 133/200, Train Loss: 26.7454, Val Loss: 25.7529\n",
      "Epoch 134/200, Train Loss: 26.9672, Val Loss: 26.0730\n",
      "Epoch 135/200, Train Loss: 26.7701, Val Loss: 25.9603\n",
      "Epoch 136/200, Train Loss: 26.7864, Val Loss: 25.8302\n",
      "Epoch 137/200, Train Loss: 26.5420, Val Loss: 25.7473\n",
      "Epoch 138/200, Train Loss: 26.4334, Val Loss: 25.7272\n",
      "  New best validation loss: 25.7272\n",
      "Epoch 139/200, Train Loss: 26.3007, Val Loss: 26.0339\n",
      "Epoch 140/200, Train Loss: 26.4555, Val Loss: 25.9169\n",
      "Epoch 141/200, Train Loss: 26.2418, Val Loss: 25.8271\n",
      "Epoch 142/200, Train Loss: 26.2106, Val Loss: 26.1191\n",
      "Epoch 143/200, Train Loss: 25.8555, Val Loss: 26.0009\n",
      "Epoch 144/200, Train Loss: 25.8967, Val Loss: 25.9128\n",
      "Epoch 145/200, Train Loss: 25.8922, Val Loss: 25.9229\n",
      "Epoch 146/200, Train Loss: 25.8550, Val Loss: 26.0249\n",
      "Epoch 147/200, Train Loss: 25.7340, Val Loss: 26.1514\n",
      "Epoch 148/200, Train Loss: 25.9284, Val Loss: 25.9757\n",
      "Epoch 149/200, Train Loss: 25.7539, Val Loss: 25.8277\n",
      "Epoch 150/200, Train Loss: 25.3901, Val Loss: 25.9132\n",
      "Epoch 151/200, Train Loss: 25.3631, Val Loss: 25.8924\n",
      "Epoch 152/200, Train Loss: 25.2411, Val Loss: 26.4767\n",
      "Epoch 153/200, Train Loss: 25.3618, Val Loss: 25.8065\n",
      "Epoch 154/200, Train Loss: 25.2487, Val Loss: 25.6219\n",
      "  New best validation loss: 25.6219\n",
      "Epoch 155/200, Train Loss: 25.0908, Val Loss: 25.6156\n",
      "  New best validation loss: 25.6156\n",
      "Epoch 156/200, Train Loss: 24.9501, Val Loss: 25.5519\n",
      "  New best validation loss: 25.5519\n",
      "Epoch 157/200, Train Loss: 24.8574, Val Loss: 25.9908\n",
      "Epoch 158/200, Train Loss: 24.7403, Val Loss: 25.5921\n",
      "Epoch 159/200, Train Loss: 24.7164, Val Loss: 25.6268\n",
      "Epoch 160/200, Train Loss: 24.5090, Val Loss: 25.6266\n",
      "Epoch 161/200, Train Loss: 24.3876, Val Loss: 26.1136\n",
      "Epoch 162/200, Train Loss: 24.5924, Val Loss: 25.7581\n",
      "Epoch 163/200, Train Loss: 24.4337, Val Loss: 25.6375\n",
      "Epoch 164/200, Train Loss: 24.2499, Val Loss: 25.6598\n",
      "Epoch 165/200, Train Loss: 24.3421, Val Loss: 26.0156\n",
      "Epoch 166/200, Train Loss: 24.1633, Val Loss: 25.7840\n",
      "Epoch 167/200, Train Loss: 24.0186, Val Loss: 25.4846\n",
      "  New best validation loss: 25.4846\n",
      "Epoch 168/200, Train Loss: 23.7715, Val Loss: 25.8153\n",
      "Epoch 169/200, Train Loss: 23.8060, Val Loss: 25.9313\n",
      "Epoch 170/200, Train Loss: 23.9322, Val Loss: 25.5326\n",
      "Epoch 171/200, Train Loss: 24.0189, Val Loss: 26.2548\n",
      "Epoch 172/200, Train Loss: 23.8443, Val Loss: 25.5885\n",
      "Epoch 173/200, Train Loss: 23.4683, Val Loss: 26.1678\n",
      "Epoch 174/200, Train Loss: 23.3518, Val Loss: 25.7643\n",
      "Epoch 175/200, Train Loss: 23.5630, Val Loss: 26.0738\n",
      "Epoch 176/200, Train Loss: 23.4973, Val Loss: 25.5141\n",
      "Epoch 177/200, Train Loss: 23.2373, Val Loss: 25.9379\n",
      "Epoch 178/200, Train Loss: 23.1776, Val Loss: 25.7767\n",
      "Epoch 179/200, Train Loss: 23.2931, Val Loss: 25.6540\n",
      "Epoch 180/200, Train Loss: 23.3315, Val Loss: 25.4057\n",
      "  New best validation loss: 25.4057\n",
      "Epoch 181/200, Train Loss: 22.7888, Val Loss: 26.1390\n",
      "Epoch 182/200, Train Loss: 22.9744, Val Loss: 25.9093\n",
      "Epoch 183/200, Train Loss: 22.7297, Val Loss: 25.8078\n",
      "Epoch 184/200, Train Loss: 22.6036, Val Loss: 26.0130\n",
      "Epoch 185/200, Train Loss: 22.7486, Val Loss: 26.0170\n",
      "Epoch 186/200, Train Loss: 22.7248, Val Loss: 26.0402\n",
      "Epoch 187/200, Train Loss: 22.5359, Val Loss: 25.7336\n",
      "Epoch 188/200, Train Loss: 22.4379, Val Loss: 25.8887\n",
      "Epoch 189/200, Train Loss: 22.4009, Val Loss: 26.3451\n",
      "Epoch 190/200, Train Loss: 22.4192, Val Loss: 26.0103\n",
      "Epoch 191/200, Train Loss: 22.5681, Val Loss: 25.9534\n",
      "Epoch 192/200, Train Loss: 22.3169, Val Loss: 25.9544\n",
      "Epoch 193/200, Train Loss: 22.0004, Val Loss: 26.0330\n",
      "Epoch 194/200, Train Loss: 21.8975, Val Loss: 26.0362\n",
      "Epoch 195/200, Train Loss: 21.8281, Val Loss: 25.9009\n",
      "Epoch 196/200, Train Loss: 21.8817, Val Loss: 25.9221\n",
      "Epoch 197/200, Train Loss: 21.9688, Val Loss: 26.7202\n",
      "Epoch 198/200, Train Loss: 22.0224, Val Loss: 25.9020\n",
      "Epoch 199/200, Train Loss: 21.7309, Val Loss: 25.8302\n",
      "Epoch 200/200, Train Loss: 21.4987, Val Loss: 26.0008\n",
      "\n",
      "Loaded best model (Val Loss: 25.4057) for final hidden state extraction.\n",
      "Saved best model for Transformer to results/20250508_082552/Transformer_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for Transformer ---\n",
      "  Analyzing decodability for Transformer...\n",
      "  Hidden states shape: torch.Size([160, 200, 64])\n",
      "  Coefficients shape: torch.Size([160, 25])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 64])\n",
      "  RidgeCV Decoder for Transformer - Test MSE: 2.1456, Test R2: -0.0607 (best alpha: 100.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decodability (R2 score) for Transformer: -0.0607\n",
      "\n",
      "--- Training HIPPORNN_LegT ---\n",
      "Number of parameters: 99716\n",
      "Epoch 1/200, Train Loss: 29.1846, Val Loss: 27.1487\n",
      "  New best validation loss: 27.1487\n",
      "Epoch 2/200, Train Loss: 29.1803, Val Loss: 27.1451\n",
      "  New best validation loss: 27.1451\n",
      "Epoch 3/200, Train Loss: 29.1789, Val Loss: 27.1427\n",
      "  New best validation loss: 27.1427\n",
      "Epoch 4/200, Train Loss: 29.1761, Val Loss: 27.1397\n",
      "  New best validation loss: 27.1397\n",
      "Epoch 5/200, Train Loss: 29.1567, Val Loss: 27.1133\n",
      "  New best validation loss: 27.1133\n",
      "Epoch 6/200, Train Loss: 28.9731, Val Loss: 26.8958\n",
      "  New best validation loss: 26.8958\n",
      "Epoch 7/200, Train Loss: 28.3446, Val Loss: 26.6900\n",
      "  New best validation loss: 26.6900\n",
      "Epoch 8/200, Train Loss: 27.9358, Val Loss: 26.3796\n",
      "  New best validation loss: 26.3796\n",
      "Epoch 9/200, Train Loss: 27.6824, Val Loss: 26.8921\n",
      "Epoch 10/200, Train Loss: 27.6186, Val Loss: 26.3572\n",
      "  New best validation loss: 26.3572\n",
      "Epoch 11/200, Train Loss: 27.4479, Val Loss: 26.3753\n",
      "Epoch 12/200, Train Loss: 27.1951, Val Loss: 26.1306\n",
      "  New best validation loss: 26.1306\n",
      "Epoch 13/200, Train Loss: 27.0611, Val Loss: 26.0239\n",
      "  New best validation loss: 26.0239\n",
      "Epoch 14/200, Train Loss: 26.8021, Val Loss: 25.7417\n",
      "  New best validation loss: 25.7417\n",
      "Epoch 15/200, Train Loss: 26.6583, Val Loss: 26.0444\n",
      "Epoch 16/200, Train Loss: 26.6271, Val Loss: 25.2995\n",
      "  New best validation loss: 25.2995\n",
      "Epoch 17/200, Train Loss: 26.3844, Val Loss: 25.3113\n",
      "Epoch 18/200, Train Loss: 26.3381, Val Loss: 25.5817\n",
      "Epoch 19/200, Train Loss: 26.0714, Val Loss: 25.1392\n",
      "  New best validation loss: 25.1392\n",
      "Epoch 20/200, Train Loss: 25.9248, Val Loss: 25.3266\n",
      "Epoch 21/200, Train Loss: 25.8555, Val Loss: 24.9202\n",
      "  New best validation loss: 24.9202\n",
      "Epoch 22/200, Train Loss: 25.7308, Val Loss: 24.6180\n",
      "  New best validation loss: 24.6180\n",
      "Epoch 23/200, Train Loss: 25.5342, Val Loss: 24.5763\n",
      "  New best validation loss: 24.5763\n",
      "Epoch 24/200, Train Loss: 25.3760, Val Loss: 24.5950\n",
      "Epoch 25/200, Train Loss: 25.3999, Val Loss: 24.7655\n",
      "Epoch 26/200, Train Loss: 25.2171, Val Loss: 24.5878\n",
      "Epoch 27/200, Train Loss: 25.1689, Val Loss: 24.3913\n",
      "  New best validation loss: 24.3913\n",
      "Epoch 28/200, Train Loss: 25.0965, Val Loss: 24.8323\n",
      "Epoch 29/200, Train Loss: 25.0200, Val Loss: 24.5685\n",
      "Epoch 30/200, Train Loss: 24.8911, Val Loss: 24.6167\n",
      "Epoch 31/200, Train Loss: 24.7767, Val Loss: 24.1280\n",
      "  New best validation loss: 24.1280\n",
      "Epoch 32/200, Train Loss: 24.5282, Val Loss: 24.7523\n",
      "Epoch 33/200, Train Loss: 24.5683, Val Loss: 24.3053\n",
      "Epoch 34/200, Train Loss: 24.4219, Val Loss: 24.3628\n",
      "Epoch 35/200, Train Loss: 24.4308, Val Loss: 24.3063\n",
      "Epoch 36/200, Train Loss: 24.1481, Val Loss: 24.4274\n",
      "Epoch 37/200, Train Loss: 24.0584, Val Loss: 24.3509\n",
      "Epoch 38/200, Train Loss: 23.9032, Val Loss: 24.1939\n",
      "Epoch 39/200, Train Loss: 23.7637, Val Loss: 24.0647\n",
      "  New best validation loss: 24.0647\n",
      "Epoch 40/200, Train Loss: 23.3820, Val Loss: 24.7309\n",
      "Epoch 41/200, Train Loss: 23.2522, Val Loss: 24.2756\n",
      "Epoch 42/200, Train Loss: 23.2607, Val Loss: 24.1313\n",
      "Epoch 43/200, Train Loss: 22.9790, Val Loss: 24.3704\n",
      "Epoch 44/200, Train Loss: 22.8921, Val Loss: 24.4392\n",
      "Epoch 45/200, Train Loss: 22.7407, Val Loss: 24.8031\n",
      "Epoch 46/200, Train Loss: 22.5620, Val Loss: 24.4616\n",
      "Epoch 47/200, Train Loss: 22.2559, Val Loss: 24.5513\n",
      "Epoch 48/200, Train Loss: 22.1942, Val Loss: 24.3574\n",
      "Epoch 49/200, Train Loss: 21.9799, Val Loss: 24.8308\n",
      "Epoch 50/200, Train Loss: 21.6682, Val Loss: 24.2388\n",
      "Epoch 51/200, Train Loss: 21.3931, Val Loss: 24.4885\n",
      "Epoch 52/200, Train Loss: 21.1912, Val Loss: 24.6427\n",
      "Epoch 53/200, Train Loss: 20.8331, Val Loss: 24.8861\n",
      "Epoch 54/200, Train Loss: 20.3313, Val Loss: 24.4677\n",
      "Epoch 55/200, Train Loss: 20.3370, Val Loss: 25.0146\n",
      "Epoch 56/200, Train Loss: 20.0299, Val Loss: 25.0503\n",
      "Epoch 57/200, Train Loss: 19.9528, Val Loss: 24.7039\n",
      "Epoch 58/200, Train Loss: 19.6199, Val Loss: 24.4771\n",
      "Epoch 59/200, Train Loss: 19.8597, Val Loss: 24.9923\n",
      "Epoch 60/200, Train Loss: 19.2589, Val Loss: 25.5418\n",
      "Epoch 61/200, Train Loss: 18.8456, Val Loss: 25.9245\n",
      "Epoch 62/200, Train Loss: 18.4230, Val Loss: 25.7703\n",
      "Epoch 63/200, Train Loss: 18.3104, Val Loss: 25.6411\n",
      "Epoch 64/200, Train Loss: 17.9084, Val Loss: 26.2292\n",
      "Epoch 65/200, Train Loss: 17.7074, Val Loss: 26.0856\n",
      "Epoch 66/200, Train Loss: 17.7185, Val Loss: 26.4820\n",
      "Epoch 67/200, Train Loss: 17.4744, Val Loss: 26.4507\n",
      "Epoch 68/200, Train Loss: 17.0581, Val Loss: 26.4426\n",
      "Epoch 69/200, Train Loss: 17.0028, Val Loss: 26.5122\n",
      "Epoch 70/200, Train Loss: 16.5654, Val Loss: 26.4133\n",
      "Epoch 71/200, Train Loss: 15.9816, Val Loss: 27.1235\n",
      "Epoch 72/200, Train Loss: 16.0916, Val Loss: 27.0093\n",
      "Epoch 73/200, Train Loss: 15.9388, Val Loss: 27.4222\n",
      "Epoch 74/200, Train Loss: 15.8074, Val Loss: 27.7208\n",
      "Epoch 75/200, Train Loss: 15.2880, Val Loss: 27.5044\n",
      "Epoch 76/200, Train Loss: 15.2303, Val Loss: 27.7749\n",
      "Epoch 77/200, Train Loss: 14.8643, Val Loss: 27.5050\n",
      "Epoch 78/200, Train Loss: 14.7130, Val Loss: 28.1848\n",
      "Epoch 79/200, Train Loss: 14.6046, Val Loss: 28.1859\n",
      "Epoch 80/200, Train Loss: 14.7172, Val Loss: 28.0139\n",
      "Epoch 81/200, Train Loss: 14.3341, Val Loss: 27.6218\n",
      "Epoch 82/200, Train Loss: 14.0851, Val Loss: 28.0093\n",
      "Epoch 83/200, Train Loss: 14.0668, Val Loss: 28.2013\n",
      "Epoch 84/200, Train Loss: 13.8730, Val Loss: 28.2699\n",
      "Epoch 85/200, Train Loss: 13.6869, Val Loss: 27.4996\n",
      "Epoch 86/200, Train Loss: 13.4804, Val Loss: 28.7914\n",
      "Epoch 87/200, Train Loss: 13.3133, Val Loss: 28.6293\n",
      "Epoch 88/200, Train Loss: 13.0613, Val Loss: 29.2341\n",
      "Epoch 89/200, Train Loss: 12.9067, Val Loss: 28.7928\n",
      "Epoch 90/200, Train Loss: 12.6695, Val Loss: 28.9803\n",
      "Epoch 91/200, Train Loss: 12.4371, Val Loss: 28.9288\n",
      "Epoch 92/200, Train Loss: 12.3698, Val Loss: 29.1633\n",
      "Epoch 93/200, Train Loss: 12.2307, Val Loss: 29.0475\n",
      "Epoch 94/200, Train Loss: 12.0798, Val Loss: 28.9145\n",
      "Epoch 95/200, Train Loss: 11.9760, Val Loss: 29.0096\n",
      "Epoch 96/200, Train Loss: 11.8568, Val Loss: 29.5330\n",
      "Epoch 97/200, Train Loss: 11.8556, Val Loss: 29.8819\n",
      "Epoch 98/200, Train Loss: 11.7602, Val Loss: 29.9139\n",
      "Epoch 99/200, Train Loss: 11.7933, Val Loss: 29.3941\n",
      "Epoch 100/200, Train Loss: 11.5993, Val Loss: 29.3757\n",
      "Epoch 101/200, Train Loss: 11.6981, Val Loss: 29.3863\n",
      "Epoch 102/200, Train Loss: 11.4964, Val Loss: 29.6422\n",
      "Epoch 103/200, Train Loss: 11.2297, Val Loss: 30.0900\n",
      "Epoch 104/200, Train Loss: 11.0350, Val Loss: 29.6775\n",
      "Epoch 105/200, Train Loss: 10.8669, Val Loss: 29.9080\n",
      "Epoch 106/200, Train Loss: 10.6157, Val Loss: 30.0083\n",
      "Epoch 107/200, Train Loss: 10.5774, Val Loss: 30.6004\n",
      "Epoch 108/200, Train Loss: 10.8316, Val Loss: 30.7388\n",
      "Epoch 109/200, Train Loss: 10.8892, Val Loss: 30.0780\n",
      "Epoch 110/200, Train Loss: 10.6159, Val Loss: 31.0971\n",
      "Epoch 111/200, Train Loss: 10.3805, Val Loss: 30.7078\n",
      "Epoch 112/200, Train Loss: 10.2515, Val Loss: 30.5747\n",
      "Epoch 113/200, Train Loss: 10.1427, Val Loss: 30.5358\n",
      "Epoch 114/200, Train Loss: 10.0790, Val Loss: 30.7815\n",
      "Epoch 115/200, Train Loss: 10.1055, Val Loss: 30.2270\n",
      "Epoch 116/200, Train Loss: 9.9559, Val Loss: 31.2153\n",
      "Epoch 117/200, Train Loss: 9.9188, Val Loss: 30.3366\n",
      "Epoch 118/200, Train Loss: 9.8590, Val Loss: 31.1678\n",
      "Epoch 119/200, Train Loss: 9.6367, Val Loss: 31.2112\n",
      "Epoch 120/200, Train Loss: 9.5033, Val Loss: 30.9994\n",
      "Epoch 121/200, Train Loss: 9.5621, Val Loss: 31.6125\n",
      "Epoch 122/200, Train Loss: 9.8511, Val Loss: 30.2645\n",
      "Epoch 123/200, Train Loss: 9.6851, Val Loss: 31.1990\n",
      "Epoch 124/200, Train Loss: 9.5062, Val Loss: 31.2528\n",
      "Epoch 125/200, Train Loss: 9.3106, Val Loss: 31.7424\n",
      "Epoch 126/200, Train Loss: 9.1471, Val Loss: 31.3950\n",
      "Epoch 127/200, Train Loss: 9.0956, Val Loss: 31.4200\n",
      "Epoch 128/200, Train Loss: 9.1480, Val Loss: 32.0439\n",
      "Epoch 129/200, Train Loss: 9.0602, Val Loss: 31.8582\n",
      "Epoch 130/200, Train Loss: 9.0278, Val Loss: 31.6663\n",
      "Epoch 131/200, Train Loss: 8.8814, Val Loss: 32.3093\n",
      "Epoch 132/200, Train Loss: 8.7891, Val Loss: 32.0567\n",
      "Epoch 133/200, Train Loss: 8.6702, Val Loss: 31.9325\n",
      "Epoch 134/200, Train Loss: 8.6412, Val Loss: 31.4661\n",
      "Epoch 135/200, Train Loss: 8.5979, Val Loss: 31.9821\n",
      "Epoch 136/200, Train Loss: 8.4712, Val Loss: 31.8546\n",
      "Epoch 137/200, Train Loss: 8.4625, Val Loss: 32.1442\n",
      "Epoch 138/200, Train Loss: 8.3284, Val Loss: 32.0710\n",
      "Epoch 139/200, Train Loss: 8.4178, Val Loss: 31.8762\n",
      "Epoch 140/200, Train Loss: 8.2727, Val Loss: 32.5930\n",
      "Epoch 141/200, Train Loss: 8.2073, Val Loss: 31.5925\n",
      "Epoch 142/200, Train Loss: 8.1314, Val Loss: 31.6445\n",
      "Epoch 143/200, Train Loss: 8.1216, Val Loss: 31.8018\n",
      "Epoch 144/200, Train Loss: 8.0481, Val Loss: 31.6875\n",
      "Epoch 145/200, Train Loss: 8.0561, Val Loss: 32.0751\n",
      "Epoch 146/200, Train Loss: 7.9837, Val Loss: 32.5701\n",
      "Epoch 147/200, Train Loss: 7.8787, Val Loss: 32.1558\n",
      "Epoch 148/200, Train Loss: 7.8055, Val Loss: 32.0194\n",
      "Epoch 149/200, Train Loss: 7.7195, Val Loss: 31.9626\n",
      "Epoch 150/200, Train Loss: 7.9551, Val Loss: 31.9485\n",
      "Epoch 151/200, Train Loss: 7.8818, Val Loss: 31.9721\n",
      "Epoch 152/200, Train Loss: 7.6750, Val Loss: 31.8484\n",
      "Epoch 153/200, Train Loss: 7.6759, Val Loss: 32.4706\n",
      "Epoch 154/200, Train Loss: 7.6494, Val Loss: 32.1671\n",
      "Epoch 155/200, Train Loss: 7.6062, Val Loss: 32.5191\n",
      "Epoch 156/200, Train Loss: 7.5621, Val Loss: 32.6451\n",
      "Epoch 157/200, Train Loss: 7.5060, Val Loss: 32.3990\n",
      "Epoch 158/200, Train Loss: 7.3621, Val Loss: 32.7844\n",
      "Epoch 159/200, Train Loss: 7.2742, Val Loss: 31.7975\n",
      "Epoch 160/200, Train Loss: 7.2872, Val Loss: 31.9858\n",
      "Epoch 161/200, Train Loss: 7.1987, Val Loss: 32.9294\n",
      "Epoch 162/200, Train Loss: 7.1817, Val Loss: 31.6889\n",
      "Epoch 163/200, Train Loss: 7.1417, Val Loss: 32.3400\n",
      "Epoch 164/200, Train Loss: 7.0815, Val Loss: 32.5680\n",
      "Epoch 165/200, Train Loss: 7.0079, Val Loss: 32.1952\n",
      "Epoch 166/200, Train Loss: 6.9250, Val Loss: 32.6356\n",
      "Epoch 167/200, Train Loss: 6.9432, Val Loss: 32.3457\n",
      "Epoch 168/200, Train Loss: 6.9409, Val Loss: 32.9340\n",
      "Epoch 169/200, Train Loss: 6.9635, Val Loss: 32.3337\n",
      "Epoch 170/200, Train Loss: 7.2314, Val Loss: 32.9238\n",
      "Epoch 171/200, Train Loss: 7.0567, Val Loss: 32.0531\n",
      "Epoch 172/200, Train Loss: 7.1023, Val Loss: 33.4897\n",
      "Epoch 173/200, Train Loss: 7.0848, Val Loss: 32.7034\n",
      "Epoch 174/200, Train Loss: 7.0841, Val Loss: 31.3948\n",
      "Epoch 175/200, Train Loss: 6.9803, Val Loss: 32.1781\n",
      "Epoch 176/200, Train Loss: 6.7914, Val Loss: 32.2562\n",
      "Epoch 177/200, Train Loss: 6.8997, Val Loss: 33.2509\n",
      "Epoch 178/200, Train Loss: 6.9585, Val Loss: 33.4449\n",
      "Epoch 179/200, Train Loss: 7.1354, Val Loss: 33.2762\n",
      "Epoch 180/200, Train Loss: 7.1687, Val Loss: 32.8278\n",
      "Epoch 181/200, Train Loss: 6.9340, Val Loss: 32.9858\n",
      "Epoch 182/200, Train Loss: 6.8952, Val Loss: 33.1163\n",
      "Epoch 183/200, Train Loss: 6.7570, Val Loss: 31.9662\n",
      "Epoch 184/200, Train Loss: 6.5847, Val Loss: 32.5795\n",
      "Epoch 185/200, Train Loss: 6.3984, Val Loss: 32.7641\n",
      "Epoch 186/200, Train Loss: 6.2983, Val Loss: 32.9791\n",
      "Epoch 187/200, Train Loss: 6.2666, Val Loss: 33.1687\n",
      "Epoch 188/200, Train Loss: 6.1643, Val Loss: 33.0427\n",
      "Epoch 189/200, Train Loss: 6.0954, Val Loss: 33.1819\n",
      "Epoch 190/200, Train Loss: 6.1514, Val Loss: 32.7929\n",
      "Epoch 191/200, Train Loss: 6.1274, Val Loss: 32.5376\n",
      "Epoch 192/200, Train Loss: 6.1535, Val Loss: 33.0466\n",
      "Epoch 193/200, Train Loss: 6.1629, Val Loss: 33.1294\n",
      "Epoch 194/200, Train Loss: 6.1070, Val Loss: 33.0020\n",
      "Epoch 195/200, Train Loss: 6.1453, Val Loss: 32.6684\n",
      "Epoch 196/200, Train Loss: 6.1474, Val Loss: 33.5253\n",
      "Epoch 197/200, Train Loss: 6.2294, Val Loss: 32.8470\n",
      "Epoch 198/200, Train Loss: 6.1421, Val Loss: 33.1062\n",
      "Epoch 199/200, Train Loss: 6.0388, Val Loss: 33.3201\n",
      "Epoch 200/200, Train Loss: 6.0482, Val Loss: 32.5572\n",
      "\n",
      "Loaded best model (Val Loss: 24.0647) for final hidden state extraction.\n",
      "Saved best model for HIPPORNN_LegT to results/20250508_082552/HIPPORNN_LegT_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for HIPPORNN_LegT ---\n",
      "  Analyzing decodability for HIPPORNN_LegT...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 25])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for HIPPORNN_LegT - Test MSE: 1.9466, Test R2: 0.0255 (best alpha: 100.0000)\n",
      "Decodability (R2 score) for HIPPORNN_LegT: 0.0255\n",
      "\n",
      "--- Training NMRNN_Spatial_ModReadout ---\n",
      "Number of parameters: 66832\n",
      "Epoch 1/200, Train Loss: 29.1701, Val Loss: 27.1344\n",
      "  New best validation loss: 27.1344\n",
      "Epoch 2/200, Train Loss: 29.1389, Val Loss: 27.1077\n",
      "  New best validation loss: 27.1077\n",
      "Epoch 3/200, Train Loss: 29.0919, Val Loss: 27.0870\n",
      "  New best validation loss: 27.0870\n",
      "Epoch 4/200, Train Loss: 29.0718, Val Loss: 27.0821\n",
      "  New best validation loss: 27.0821\n",
      "Epoch 5/200, Train Loss: 29.0333, Val Loss: 27.0661\n",
      "  New best validation loss: 27.0661\n",
      "Epoch 6/200, Train Loss: 29.0101, Val Loss: 27.0494\n",
      "  New best validation loss: 27.0494\n",
      "Epoch 7/200, Train Loss: 28.9528, Val Loss: 27.0778\n",
      "Epoch 8/200, Train Loss: 28.9064, Val Loss: 27.0522\n",
      "Epoch 9/200, Train Loss: 28.8871, Val Loss: 27.0795\n",
      "Epoch 10/200, Train Loss: 28.8165, Val Loss: 27.0432\n",
      "  New best validation loss: 27.0432\n",
      "Epoch 11/200, Train Loss: 28.7969, Val Loss: 27.0087\n",
      "  New best validation loss: 27.0087\n",
      "Epoch 12/200, Train Loss: 28.7478, Val Loss: 27.0349\n",
      "Epoch 13/200, Train Loss: 28.7548, Val Loss: 27.1044\n",
      "Epoch 14/200, Train Loss: 28.7356, Val Loss: 27.0210\n",
      "Epoch 15/200, Train Loss: 28.6836, Val Loss: 27.0668\n",
      "Epoch 16/200, Train Loss: 28.6428, Val Loss: 27.0475\n",
      "Epoch 17/200, Train Loss: 28.6125, Val Loss: 27.0602\n",
      "Epoch 18/200, Train Loss: 28.5747, Val Loss: 27.1064\n",
      "Epoch 19/200, Train Loss: 28.5574, Val Loss: 27.1036\n",
      "Epoch 20/200, Train Loss: 28.5060, Val Loss: 27.0630\n",
      "Epoch 21/200, Train Loss: 28.4977, Val Loss: 27.0755\n",
      "Epoch 22/200, Train Loss: 28.4547, Val Loss: 27.1049\n",
      "Epoch 23/200, Train Loss: 28.4185, Val Loss: 27.1116\n",
      "Epoch 24/200, Train Loss: 28.3678, Val Loss: 27.1274\n",
      "Epoch 25/200, Train Loss: 28.3568, Val Loss: 27.1319\n",
      "Epoch 26/200, Train Loss: 28.3099, Val Loss: 27.1549\n",
      "Epoch 27/200, Train Loss: 28.2326, Val Loss: 27.1241\n",
      "Epoch 28/200, Train Loss: 28.1911, Val Loss: 27.0993\n",
      "Epoch 29/200, Train Loss: 28.1476, Val Loss: 27.1480\n",
      "Epoch 30/200, Train Loss: 28.1166, Val Loss: 27.0617\n",
      "Epoch 31/200, Train Loss: 28.0475, Val Loss: 27.1620\n",
      "Epoch 32/200, Train Loss: 27.9842, Val Loss: 27.1552\n",
      "Epoch 33/200, Train Loss: 27.9546, Val Loss: 27.0681\n",
      "Epoch 34/200, Train Loss: 27.9009, Val Loss: 27.1864\n",
      "Epoch 35/200, Train Loss: 27.8171, Val Loss: 27.0438\n",
      "Epoch 36/200, Train Loss: 27.8208, Val Loss: 27.1379\n",
      "Epoch 37/200, Train Loss: 27.6472, Val Loss: 27.0150\n",
      "Epoch 38/200, Train Loss: 27.6496, Val Loss: 27.2111\n",
      "Epoch 39/200, Train Loss: 27.5896, Val Loss: 27.1201\n",
      "Epoch 40/200, Train Loss: 27.5202, Val Loss: 27.1126\n",
      "Epoch 41/200, Train Loss: 27.4143, Val Loss: 27.3291\n",
      "Epoch 42/200, Train Loss: 27.3339, Val Loss: 27.1116\n",
      "Epoch 43/200, Train Loss: 27.2419, Val Loss: 27.2720\n",
      "Epoch 44/200, Train Loss: 27.1311, Val Loss: 27.4516\n",
      "Epoch 45/200, Train Loss: 26.9843, Val Loss: 27.4015\n",
      "Epoch 46/200, Train Loss: 26.8993, Val Loss: 27.1646\n",
      "Epoch 47/200, Train Loss: 26.7955, Val Loss: 27.0725\n",
      "Epoch 48/200, Train Loss: 26.7530, Val Loss: 27.3521\n",
      "Epoch 49/200, Train Loss: 26.7375, Val Loss: 27.0906\n",
      "Epoch 50/200, Train Loss: 26.5221, Val Loss: 27.5269\n",
      "Epoch 51/200, Train Loss: 26.3901, Val Loss: 27.4582\n",
      "Epoch 52/200, Train Loss: 26.3404, Val Loss: 27.8662\n",
      "Epoch 53/200, Train Loss: 26.1915, Val Loss: 27.6587\n",
      "Epoch 54/200, Train Loss: 26.1823, Val Loss: 27.4819\n",
      "Epoch 55/200, Train Loss: 26.0215, Val Loss: 27.6033\n",
      "Epoch 56/200, Train Loss: 26.0617, Val Loss: 27.5613\n",
      "Epoch 57/200, Train Loss: 25.9085, Val Loss: 27.8097\n",
      "Epoch 58/200, Train Loss: 25.8314, Val Loss: 27.4728\n",
      "Epoch 59/200, Train Loss: 25.6258, Val Loss: 27.7008\n",
      "Epoch 60/200, Train Loss: 25.4552, Val Loss: 27.8879\n",
      "Epoch 61/200, Train Loss: 25.4491, Val Loss: 28.1732\n",
      "Epoch 62/200, Train Loss: 25.2930, Val Loss: 27.7424\n",
      "Epoch 63/200, Train Loss: 25.3566, Val Loss: 27.9165\n",
      "Epoch 64/200, Train Loss: 25.2214, Val Loss: 28.1622\n",
      "Epoch 65/200, Train Loss: 25.0336, Val Loss: 27.8665\n",
      "Epoch 66/200, Train Loss: 25.0429, Val Loss: 28.5825\n",
      "Epoch 67/200, Train Loss: 25.0240, Val Loss: 28.5124\n",
      "Epoch 68/200, Train Loss: 24.8629, Val Loss: 28.5591\n",
      "Epoch 69/200, Train Loss: 24.8004, Val Loss: 28.2351\n",
      "Epoch 70/200, Train Loss: 24.7646, Val Loss: 28.4919\n",
      "Epoch 71/200, Train Loss: 24.4451, Val Loss: 28.4755\n",
      "Epoch 72/200, Train Loss: 24.4138, Val Loss: 28.2087\n",
      "Epoch 73/200, Train Loss: 24.4304, Val Loss: 28.2507\n",
      "Epoch 74/200, Train Loss: 24.2859, Val Loss: 28.2486\n",
      "Epoch 75/200, Train Loss: 24.2607, Val Loss: 27.9449\n",
      "Epoch 76/200, Train Loss: 24.3019, Val Loss: 29.0117\n",
      "Epoch 77/200, Train Loss: 24.2364, Val Loss: 28.6622\n",
      "Epoch 78/200, Train Loss: 23.9912, Val Loss: 29.1661\n",
      "Epoch 79/200, Train Loss: 23.9691, Val Loss: 28.9259\n",
      "Epoch 80/200, Train Loss: 23.8013, Val Loss: 28.9417\n",
      "Epoch 81/200, Train Loss: 23.6890, Val Loss: 28.7691\n",
      "Epoch 82/200, Train Loss: 23.5391, Val Loss: 28.9292\n",
      "Epoch 83/200, Train Loss: 23.4318, Val Loss: 29.2427\n",
      "Epoch 84/200, Train Loss: 23.3063, Val Loss: 29.0413\n",
      "Epoch 85/200, Train Loss: 23.3459, Val Loss: 28.6324\n",
      "Epoch 86/200, Train Loss: 23.1435, Val Loss: 29.2121\n",
      "Epoch 87/200, Train Loss: 23.1406, Val Loss: 28.9523\n",
      "Epoch 88/200, Train Loss: 23.0402, Val Loss: 28.8644\n",
      "Epoch 89/200, Train Loss: 23.0150, Val Loss: 28.9931\n",
      "Epoch 90/200, Train Loss: 22.8055, Val Loss: 29.2459\n",
      "Epoch 91/200, Train Loss: 22.7361, Val Loss: 29.3863\n",
      "Epoch 92/200, Train Loss: 22.6827, Val Loss: 28.9666\n",
      "Epoch 93/200, Train Loss: 22.4872, Val Loss: 29.2712\n",
      "Epoch 94/200, Train Loss: 22.4306, Val Loss: 29.3485\n",
      "Epoch 95/200, Train Loss: 22.4887, Val Loss: 29.8276\n",
      "Epoch 96/200, Train Loss: 22.4800, Val Loss: 29.4810\n",
      "Epoch 97/200, Train Loss: 22.2765, Val Loss: 30.0157\n",
      "Epoch 98/200, Train Loss: 22.2921, Val Loss: 30.2479\n",
      "Epoch 99/200, Train Loss: 22.1873, Val Loss: 29.7004\n",
      "Epoch 100/200, Train Loss: 22.2167, Val Loss: 30.1596\n",
      "Epoch 101/200, Train Loss: 22.1393, Val Loss: 29.8447\n",
      "Epoch 102/200, Train Loss: 21.8736, Val Loss: 29.6029\n",
      "Epoch 103/200, Train Loss: 21.8432, Val Loss: 29.5792\n",
      "Epoch 104/200, Train Loss: 21.7365, Val Loss: 29.4295\n",
      "Epoch 105/200, Train Loss: 21.6489, Val Loss: 29.9600\n",
      "Epoch 106/200, Train Loss: 21.7745, Val Loss: 29.8238\n",
      "Epoch 107/200, Train Loss: 21.5615, Val Loss: 29.9426\n",
      "Epoch 108/200, Train Loss: 21.5868, Val Loss: 29.9113\n",
      "Epoch 109/200, Train Loss: 21.4390, Val Loss: 29.6006\n",
      "Epoch 110/200, Train Loss: 21.3775, Val Loss: 30.1485\n",
      "Epoch 111/200, Train Loss: 21.2951, Val Loss: 29.7722\n",
      "Epoch 112/200, Train Loss: 21.2314, Val Loss: 29.9314\n",
      "Epoch 113/200, Train Loss: 21.1075, Val Loss: 30.2444\n",
      "Epoch 114/200, Train Loss: 21.0014, Val Loss: 30.5922\n",
      "Epoch 115/200, Train Loss: 20.9200, Val Loss: 30.2881\n",
      "Epoch 116/200, Train Loss: 20.9029, Val Loss: 30.4622\n",
      "Epoch 117/200, Train Loss: 20.7843, Val Loss: 30.6299\n",
      "Epoch 118/200, Train Loss: 20.7320, Val Loss: 30.5892\n",
      "Epoch 119/200, Train Loss: 20.8780, Val Loss: 30.3216\n",
      "Epoch 120/200, Train Loss: 20.5490, Val Loss: 30.3110\n",
      "Epoch 121/200, Train Loss: 20.5208, Val Loss: 30.5150\n",
      "Epoch 122/200, Train Loss: 20.4990, Val Loss: 31.0258\n",
      "Epoch 123/200, Train Loss: 20.3255, Val Loss: 30.7657\n",
      "Epoch 124/200, Train Loss: 20.3884, Val Loss: 30.9749\n",
      "Epoch 125/200, Train Loss: 20.4546, Val Loss: 31.2529\n",
      "Epoch 126/200, Train Loss: 20.3982, Val Loss: 30.5957\n",
      "Epoch 127/200, Train Loss: 20.2686, Val Loss: 31.2350\n",
      "Epoch 128/200, Train Loss: 20.1821, Val Loss: 31.2369\n",
      "Epoch 129/200, Train Loss: 20.0588, Val Loss: 31.3724\n",
      "Epoch 130/200, Train Loss: 20.0723, Val Loss: 31.4577\n",
      "Epoch 131/200, Train Loss: 19.9058, Val Loss: 31.1146\n",
      "Epoch 132/200, Train Loss: 20.1223, Val Loss: 31.5783\n",
      "Epoch 133/200, Train Loss: 19.9731, Val Loss: 31.7476\n",
      "Epoch 134/200, Train Loss: 19.8813, Val Loss: 31.6287\n",
      "Epoch 135/200, Train Loss: 19.8377, Val Loss: 31.3325\n",
      "Epoch 136/200, Train Loss: 19.6864, Val Loss: 31.7881\n",
      "Epoch 137/200, Train Loss: 19.6200, Val Loss: 31.4957\n",
      "Epoch 138/200, Train Loss: 19.5549, Val Loss: 31.9939\n",
      "Epoch 139/200, Train Loss: 19.4676, Val Loss: 31.4319\n",
      "Epoch 140/200, Train Loss: 19.6050, Val Loss: 32.4821\n",
      "Epoch 141/200, Train Loss: 19.5798, Val Loss: 31.9513\n",
      "Epoch 142/200, Train Loss: 19.4512, Val Loss: 31.8999\n",
      "Epoch 143/200, Train Loss: 19.3709, Val Loss: 31.9858\n",
      "Epoch 144/200, Train Loss: 19.3130, Val Loss: 32.1685\n",
      "Epoch 145/200, Train Loss: 19.3140, Val Loss: 31.3879\n",
      "Epoch 146/200, Train Loss: 19.2175, Val Loss: 32.0133\n",
      "Epoch 147/200, Train Loss: 19.0735, Val Loss: 31.9180\n",
      "Epoch 148/200, Train Loss: 19.0201, Val Loss: 32.5904\n",
      "Epoch 149/200, Train Loss: 19.0860, Val Loss: 32.0840\n",
      "Epoch 150/200, Train Loss: 19.1400, Val Loss: 31.8103\n",
      "Epoch 151/200, Train Loss: 19.0435, Val Loss: 32.2568\n",
      "Epoch 152/200, Train Loss: 19.0052, Val Loss: 32.0122\n",
      "Epoch 153/200, Train Loss: 18.8193, Val Loss: 32.5002\n",
      "Epoch 154/200, Train Loss: 18.7961, Val Loss: 32.8939\n",
      "Epoch 155/200, Train Loss: 18.6879, Val Loss: 33.0957\n",
      "Epoch 156/200, Train Loss: 18.8041, Val Loss: 32.5445\n",
      "Epoch 157/200, Train Loss: 18.6965, Val Loss: 33.0182\n",
      "Epoch 158/200, Train Loss: 18.5853, Val Loss: 32.6956\n",
      "Epoch 159/200, Train Loss: 18.4739, Val Loss: 32.5416\n",
      "Epoch 160/200, Train Loss: 18.4650, Val Loss: 33.1098\n",
      "Epoch 161/200, Train Loss: 18.3984, Val Loss: 32.7907\n",
      "Epoch 162/200, Train Loss: 18.2784, Val Loss: 32.9398\n",
      "Epoch 163/200, Train Loss: 18.4098, Val Loss: 32.6977\n",
      "Epoch 164/200, Train Loss: 18.2604, Val Loss: 33.0136\n",
      "Epoch 165/200, Train Loss: 18.1983, Val Loss: 33.0553\n",
      "Epoch 166/200, Train Loss: 18.2673, Val Loss: 33.0121\n",
      "Epoch 167/200, Train Loss: 18.1314, Val Loss: 32.6570\n",
      "Epoch 168/200, Train Loss: 17.9779, Val Loss: 33.6803\n",
      "Epoch 169/200, Train Loss: 18.0268, Val Loss: 33.3071\n",
      "Epoch 170/200, Train Loss: 17.9249, Val Loss: 33.3056\n",
      "Epoch 171/200, Train Loss: 17.9738, Val Loss: 33.0746\n",
      "Epoch 172/200, Train Loss: 18.0622, Val Loss: 33.1327\n",
      "Epoch 173/200, Train Loss: 17.8922, Val Loss: 33.1734\n",
      "Epoch 174/200, Train Loss: 17.7688, Val Loss: 33.2309\n",
      "Epoch 175/200, Train Loss: 17.8060, Val Loss: 33.2657\n",
      "Epoch 176/200, Train Loss: 17.8543, Val Loss: 33.1832\n",
      "Epoch 177/200, Train Loss: 17.6609, Val Loss: 33.3136\n",
      "Epoch 178/200, Train Loss: 17.5865, Val Loss: 33.7305\n",
      "Epoch 179/200, Train Loss: 17.6664, Val Loss: 33.8399\n",
      "Epoch 180/200, Train Loss: 17.6878, Val Loss: 33.4116\n",
      "Epoch 181/200, Train Loss: 17.6901, Val Loss: 33.7656\n",
      "Epoch 182/200, Train Loss: 17.5910, Val Loss: 33.4172\n",
      "Epoch 183/200, Train Loss: 17.3555, Val Loss: 33.0547\n",
      "Epoch 184/200, Train Loss: 17.4264, Val Loss: 33.0224\n",
      "Epoch 185/200, Train Loss: 17.4003, Val Loss: 33.6213\n",
      "Epoch 186/200, Train Loss: 17.2119, Val Loss: 33.0928\n",
      "Epoch 187/200, Train Loss: 17.1506, Val Loss: 33.9406\n",
      "Epoch 188/200, Train Loss: 17.1426, Val Loss: 33.5366\n",
      "Epoch 189/200, Train Loss: 17.1860, Val Loss: 34.1953\n",
      "Epoch 190/200, Train Loss: 17.1905, Val Loss: 33.2285\n",
      "Epoch 191/200, Train Loss: 17.1181, Val Loss: 33.8960\n",
      "Epoch 192/200, Train Loss: 17.0881, Val Loss: 33.7024\n",
      "Epoch 193/200, Train Loss: 16.9157, Val Loss: 33.9313\n",
      "Epoch 194/200, Train Loss: 16.9425, Val Loss: 33.3987\n",
      "Epoch 195/200, Train Loss: 16.8694, Val Loss: 33.7521\n",
      "Epoch 196/200, Train Loss: 16.7897, Val Loss: 34.0612\n",
      "Epoch 197/200, Train Loss: 16.8976, Val Loss: 33.4449\n",
      "Epoch 198/200, Train Loss: 16.9400, Val Loss: 34.0041\n",
      "Epoch 199/200, Train Loss: 16.9432, Val Loss: 33.6778\n",
      "Epoch 200/200, Train Loss: 16.7649, Val Loss: 34.2177\n",
      "\n",
      "Loaded best model (Val Loss: 27.0087) for final hidden state extraction.\n",
      "Saved best model for NMRNN_Spatial_ModReadout to results/20250508_082552/NMRNN_Spatial_ModReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_Spatial_ModReadout ---\n",
      "  Analyzing decodability for NMRNN_Spatial_ModReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 25])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_Spatial_ModReadout - Test MSE: 2.1325, Test R2: -0.0514 (best alpha: 100.0000)\n",
      "Decodability (R2 score) for NMRNN_Spatial_ModReadout: -0.0514\n",
      "\n",
      "--- Training NMRNN_NoSpatial_ModReadout ---\n",
      "Number of parameters: 66832\n",
      "Epoch 1/200, Train Loss: 29.1609, Val Loss: 27.1411\n",
      "  New best validation loss: 27.1411\n",
      "Epoch 2/200, Train Loss: 29.1348, Val Loss: 27.1301\n",
      "  New best validation loss: 27.1301\n",
      "Epoch 3/200, Train Loss: 29.1311, Val Loss: 27.0978\n",
      "  New best validation loss: 27.0978\n",
      "Epoch 4/200, Train Loss: 29.0981, Val Loss: 27.0950\n",
      "  New best validation loss: 27.0950\n",
      "Epoch 5/200, Train Loss: 29.0582, Val Loss: 27.0803\n",
      "  New best validation loss: 27.0803\n",
      "Epoch 6/200, Train Loss: 29.0365, Val Loss: 27.0803\n",
      "  New best validation loss: 27.0803\n",
      "Epoch 7/200, Train Loss: 28.9733, Val Loss: 27.0308\n",
      "  New best validation loss: 27.0308\n",
      "Epoch 8/200, Train Loss: 28.9389, Val Loss: 27.0580\n",
      "Epoch 9/200, Train Loss: 28.8833, Val Loss: 27.0378\n",
      "Epoch 10/200, Train Loss: 28.8248, Val Loss: 27.0390\n",
      "Epoch 11/200, Train Loss: 28.7784, Val Loss: 27.0388\n",
      "Epoch 12/200, Train Loss: 28.7632, Val Loss: 27.0773\n",
      "Epoch 13/200, Train Loss: 28.7249, Val Loss: 27.1017\n",
      "Epoch 14/200, Train Loss: 28.6993, Val Loss: 27.0822\n",
      "Epoch 15/200, Train Loss: 28.6597, Val Loss: 27.0398\n",
      "Epoch 16/200, Train Loss: 28.6606, Val Loss: 27.0973\n",
      "Epoch 17/200, Train Loss: 28.6026, Val Loss: 27.0428\n",
      "Epoch 18/200, Train Loss: 28.6015, Val Loss: 27.0187\n",
      "  New best validation loss: 27.0187\n",
      "Epoch 19/200, Train Loss: 28.5226, Val Loss: 27.0557\n",
      "Epoch 20/200, Train Loss: 28.4794, Val Loss: 27.0814\n",
      "Epoch 21/200, Train Loss: 28.4761, Val Loss: 27.0826\n",
      "Epoch 22/200, Train Loss: 28.4332, Val Loss: 27.0747\n",
      "Epoch 23/200, Train Loss: 28.3963, Val Loss: 27.1399\n",
      "Epoch 24/200, Train Loss: 28.3918, Val Loss: 27.1033\n",
      "Epoch 25/200, Train Loss: 28.3914, Val Loss: 27.2039\n",
      "Epoch 26/200, Train Loss: 28.3508, Val Loss: 27.0539\n",
      "Epoch 27/200, Train Loss: 28.2569, Val Loss: 27.1438\n",
      "Epoch 28/200, Train Loss: 28.1778, Val Loss: 27.1102\n",
      "Epoch 29/200, Train Loss: 28.1826, Val Loss: 27.1383\n",
      "Epoch 30/200, Train Loss: 28.1344, Val Loss: 27.1454\n",
      "Epoch 31/200, Train Loss: 28.0420, Val Loss: 27.1594\n",
      "Epoch 32/200, Train Loss: 27.9710, Val Loss: 27.1936\n",
      "Epoch 33/200, Train Loss: 27.9424, Val Loss: 27.1940\n",
      "Epoch 34/200, Train Loss: 27.9332, Val Loss: 27.2297\n",
      "Epoch 35/200, Train Loss: 27.9479, Val Loss: 27.2292\n",
      "Epoch 36/200, Train Loss: 27.7321, Val Loss: 27.2377\n",
      "Epoch 37/200, Train Loss: 27.6242, Val Loss: 27.2147\n",
      "Epoch 38/200, Train Loss: 27.5659, Val Loss: 27.1862\n",
      "Epoch 39/200, Train Loss: 27.4044, Val Loss: 27.1731\n",
      "Epoch 40/200, Train Loss: 27.2192, Val Loss: 27.2592\n",
      "Epoch 41/200, Train Loss: 27.1630, Val Loss: 27.1550\n",
      "Epoch 42/200, Train Loss: 27.0260, Val Loss: 27.3110\n",
      "Epoch 43/200, Train Loss: 26.9442, Val Loss: 27.1715\n",
      "Epoch 44/200, Train Loss: 26.7916, Val Loss: 27.2827\n",
      "Epoch 45/200, Train Loss: 26.7831, Val Loss: 27.2089\n",
      "Epoch 46/200, Train Loss: 26.5876, Val Loss: 27.3208\n",
      "Epoch 47/200, Train Loss: 26.5267, Val Loss: 27.3388\n",
      "Epoch 48/200, Train Loss: 26.3444, Val Loss: 27.5248\n",
      "Epoch 49/200, Train Loss: 26.2763, Val Loss: 27.3294\n",
      "Epoch 50/200, Train Loss: 26.1167, Val Loss: 27.4055\n",
      "Epoch 51/200, Train Loss: 26.0490, Val Loss: 27.4475\n",
      "Epoch 52/200, Train Loss: 25.9030, Val Loss: 27.5637\n",
      "Epoch 53/200, Train Loss: 25.7357, Val Loss: 27.4369\n",
      "Epoch 54/200, Train Loss: 25.6404, Val Loss: 27.6610\n",
      "Epoch 55/200, Train Loss: 25.5829, Val Loss: 27.8128\n",
      "Epoch 56/200, Train Loss: 25.3983, Val Loss: 27.7203\n",
      "Epoch 57/200, Train Loss: 25.3457, Val Loss: 27.6646\n",
      "Epoch 58/200, Train Loss: 25.2414, Val Loss: 27.7781\n",
      "Epoch 59/200, Train Loss: 25.1258, Val Loss: 27.8246\n",
      "Epoch 60/200, Train Loss: 24.9624, Val Loss: 27.8695\n",
      "Epoch 61/200, Train Loss: 24.8626, Val Loss: 27.5998\n",
      "Epoch 62/200, Train Loss: 24.7692, Val Loss: 28.0442\n",
      "Epoch 63/200, Train Loss: 24.7079, Val Loss: 27.6863\n",
      "Epoch 64/200, Train Loss: 24.6220, Val Loss: 27.8476\n",
      "Epoch 65/200, Train Loss: 24.4818, Val Loss: 27.8856\n",
      "Epoch 66/200, Train Loss: 24.3190, Val Loss: 27.7529\n",
      "Epoch 67/200, Train Loss: 24.2257, Val Loss: 28.1167\n",
      "Epoch 68/200, Train Loss: 24.1044, Val Loss: 28.2623\n",
      "Epoch 69/200, Train Loss: 24.1089, Val Loss: 28.0282\n",
      "Epoch 70/200, Train Loss: 24.0883, Val Loss: 28.3533\n",
      "Epoch 71/200, Train Loss: 24.0223, Val Loss: 28.3459\n",
      "Epoch 72/200, Train Loss: 23.8671, Val Loss: 28.6800\n",
      "Epoch 73/200, Train Loss: 23.8982, Val Loss: 27.9943\n",
      "Epoch 74/200, Train Loss: 23.9227, Val Loss: 28.1229\n",
      "Epoch 75/200, Train Loss: 23.7137, Val Loss: 28.3910\n",
      "Epoch 76/200, Train Loss: 23.6015, Val Loss: 28.2090\n",
      "Epoch 77/200, Train Loss: 23.6335, Val Loss: 28.3972\n",
      "Epoch 78/200, Train Loss: 23.5821, Val Loss: 28.5400\n",
      "Epoch 79/200, Train Loss: 23.4524, Val Loss: 28.3489\n",
      "Epoch 80/200, Train Loss: 23.4470, Val Loss: 28.5123\n",
      "Epoch 81/200, Train Loss: 23.3658, Val Loss: 28.4668\n",
      "Epoch 82/200, Train Loss: 23.1420, Val Loss: 28.8919\n",
      "Epoch 83/200, Train Loss: 23.1405, Val Loss: 28.9677\n",
      "Epoch 84/200, Train Loss: 23.0092, Val Loss: 28.8992\n",
      "Epoch 85/200, Train Loss: 22.9050, Val Loss: 29.0511\n",
      "Epoch 86/200, Train Loss: 22.7987, Val Loss: 29.1246\n",
      "Epoch 87/200, Train Loss: 22.7035, Val Loss: 29.2443\n",
      "Epoch 88/200, Train Loss: 22.6676, Val Loss: 29.2766\n",
      "Epoch 89/200, Train Loss: 22.5494, Val Loss: 29.2464\n",
      "Epoch 90/200, Train Loss: 22.4648, Val Loss: 29.3102\n",
      "Epoch 91/200, Train Loss: 22.5473, Val Loss: 29.2164\n",
      "Epoch 92/200, Train Loss: 22.5080, Val Loss: 29.3405\n",
      "Epoch 93/200, Train Loss: 22.3603, Val Loss: 29.4418\n",
      "Epoch 94/200, Train Loss: 22.2244, Val Loss: 29.3410\n",
      "Epoch 95/200, Train Loss: 22.1392, Val Loss: 29.3543\n",
      "Epoch 96/200, Train Loss: 22.1503, Val Loss: 29.4585\n",
      "Epoch 97/200, Train Loss: 22.1135, Val Loss: 29.6781\n",
      "Epoch 98/200, Train Loss: 21.9573, Val Loss: 29.7353\n",
      "Epoch 99/200, Train Loss: 21.8515, Val Loss: 29.9758\n",
      "Epoch 100/200, Train Loss: 21.6890, Val Loss: 29.7880\n",
      "Epoch 101/200, Train Loss: 21.6475, Val Loss: 29.9657\n",
      "Epoch 102/200, Train Loss: 21.6861, Val Loss: 30.1050\n",
      "Epoch 103/200, Train Loss: 21.4168, Val Loss: 30.1659\n",
      "Epoch 104/200, Train Loss: 21.3989, Val Loss: 30.1838\n",
      "Epoch 105/200, Train Loss: 21.5508, Val Loss: 30.3157\n",
      "Epoch 106/200, Train Loss: 21.3754, Val Loss: 30.4792\n",
      "Epoch 107/200, Train Loss: 21.2510, Val Loss: 30.3451\n",
      "Epoch 108/200, Train Loss: 21.2107, Val Loss: 30.3490\n",
      "Epoch 109/200, Train Loss: 21.1034, Val Loss: 30.4237\n",
      "Epoch 110/200, Train Loss: 21.0384, Val Loss: 30.6033\n",
      "Epoch 111/200, Train Loss: 20.8946, Val Loss: 30.5363\n",
      "Epoch 112/200, Train Loss: 20.9062, Val Loss: 30.6829\n",
      "Epoch 113/200, Train Loss: 20.7893, Val Loss: 30.6397\n",
      "Epoch 114/200, Train Loss: 20.7427, Val Loss: 31.0982\n",
      "Epoch 115/200, Train Loss: 20.6819, Val Loss: 31.0375\n",
      "Epoch 116/200, Train Loss: 20.6525, Val Loss: 30.7553\n",
      "Epoch 117/200, Train Loss: 20.4689, Val Loss: 31.0776\n",
      "Epoch 118/200, Train Loss: 20.5094, Val Loss: 30.7037\n",
      "Epoch 119/200, Train Loss: 20.4478, Val Loss: 30.8268\n",
      "Epoch 120/200, Train Loss: 20.4890, Val Loss: 30.5388\n",
      "Epoch 121/200, Train Loss: 20.2830, Val Loss: 31.1114\n",
      "Epoch 122/200, Train Loss: 20.1508, Val Loss: 31.0798\n",
      "Epoch 123/200, Train Loss: 20.0800, Val Loss: 31.1697\n",
      "Epoch 124/200, Train Loss: 20.1383, Val Loss: 31.2132\n",
      "Epoch 125/200, Train Loss: 19.9870, Val Loss: 31.4014\n",
      "Epoch 126/200, Train Loss: 19.9197, Val Loss: 31.5227\n",
      "Epoch 127/200, Train Loss: 19.9801, Val Loss: 31.3819\n",
      "Epoch 128/200, Train Loss: 19.7736, Val Loss: 31.3977\n",
      "Epoch 129/200, Train Loss: 19.7705, Val Loss: 31.2607\n",
      "Epoch 130/200, Train Loss: 19.8371, Val Loss: 31.4735\n",
      "Epoch 131/200, Train Loss: 20.0185, Val Loss: 31.2150\n",
      "Epoch 132/200, Train Loss: 19.7017, Val Loss: 31.6852\n",
      "Epoch 133/200, Train Loss: 19.6843, Val Loss: 31.3495\n",
      "Epoch 134/200, Train Loss: 19.4953, Val Loss: 31.3762\n",
      "Epoch 135/200, Train Loss: 19.5970, Val Loss: 31.5982\n",
      "Epoch 136/200, Train Loss: 19.4952, Val Loss: 31.6990\n",
      "Epoch 137/200, Train Loss: 19.3433, Val Loss: 32.0118\n",
      "Epoch 138/200, Train Loss: 19.3341, Val Loss: 31.5444\n",
      "Epoch 139/200, Train Loss: 19.2299, Val Loss: 31.4868\n",
      "Epoch 140/200, Train Loss: 19.1222, Val Loss: 31.6118\n",
      "Epoch 141/200, Train Loss: 19.1167, Val Loss: 31.8880\n",
      "Epoch 142/200, Train Loss: 19.0586, Val Loss: 31.9759\n",
      "Epoch 143/200, Train Loss: 19.1067, Val Loss: 31.8931\n",
      "Epoch 144/200, Train Loss: 18.9106, Val Loss: 32.0686\n",
      "Epoch 145/200, Train Loss: 18.9486, Val Loss: 32.1998\n",
      "Epoch 146/200, Train Loss: 18.8190, Val Loss: 32.4969\n",
      "Epoch 147/200, Train Loss: 18.7370, Val Loss: 32.2475\n",
      "Epoch 148/200, Train Loss: 18.6105, Val Loss: 32.3947\n",
      "Epoch 149/200, Train Loss: 18.6040, Val Loss: 32.5608\n",
      "Epoch 150/200, Train Loss: 18.5699, Val Loss: 32.4287\n",
      "Epoch 151/200, Train Loss: 18.5664, Val Loss: 32.5476\n",
      "Epoch 152/200, Train Loss: 18.6642, Val Loss: 33.0162\n",
      "Epoch 153/200, Train Loss: 18.7253, Val Loss: 32.6727\n",
      "Epoch 154/200, Train Loss: 18.6280, Val Loss: 32.2842\n",
      "Epoch 155/200, Train Loss: 18.3407, Val Loss: 32.7601\n",
      "Epoch 156/200, Train Loss: 18.2874, Val Loss: 32.5838\n",
      "Epoch 157/200, Train Loss: 18.2706, Val Loss: 32.8272\n",
      "Epoch 158/200, Train Loss: 18.1710, Val Loss: 32.3626\n",
      "Epoch 159/200, Train Loss: 18.0755, Val Loss: 32.3401\n",
      "Epoch 160/200, Train Loss: 18.1206, Val Loss: 32.7646\n",
      "Epoch 161/200, Train Loss: 18.0381, Val Loss: 32.7602\n",
      "Epoch 162/200, Train Loss: 17.8432, Val Loss: 32.7846\n",
      "Epoch 163/200, Train Loss: 17.8945, Val Loss: 33.1328\n",
      "Epoch 164/200, Train Loss: 18.2855, Val Loss: 32.7410\n",
      "Epoch 165/200, Train Loss: 17.9562, Val Loss: 32.6510\n",
      "Epoch 166/200, Train Loss: 17.8230, Val Loss: 33.0762\n",
      "Epoch 167/200, Train Loss: 17.8914, Val Loss: 32.4232\n",
      "Epoch 168/200, Train Loss: 18.0188, Val Loss: 32.5069\n",
      "Epoch 169/200, Train Loss: 17.8959, Val Loss: 32.6278\n",
      "Epoch 170/200, Train Loss: 17.6972, Val Loss: 33.0609\n",
      "Epoch 171/200, Train Loss: 17.5597, Val Loss: 33.4372\n",
      "Epoch 172/200, Train Loss: 17.5236, Val Loss: 33.2507\n",
      "Epoch 173/200, Train Loss: 17.4849, Val Loss: 33.0496\n",
      "Epoch 174/200, Train Loss: 17.4529, Val Loss: 33.5518\n",
      "Epoch 175/200, Train Loss: 17.4703, Val Loss: 32.6858\n",
      "Epoch 176/200, Train Loss: 17.3525, Val Loss: 33.3462\n",
      "Epoch 177/200, Train Loss: 17.3373, Val Loss: 33.2840\n",
      "Epoch 178/200, Train Loss: 17.2948, Val Loss: 33.6650\n",
      "Epoch 179/200, Train Loss: 17.3247, Val Loss: 33.3124\n",
      "Epoch 180/200, Train Loss: 17.3178, Val Loss: 33.4743\n",
      "Epoch 181/200, Train Loss: 17.4056, Val Loss: 33.5076\n",
      "Epoch 182/200, Train Loss: 17.3004, Val Loss: 33.2446\n",
      "Epoch 183/200, Train Loss: 17.1984, Val Loss: 33.5349\n",
      "Epoch 184/200, Train Loss: 17.1045, Val Loss: 33.4969\n",
      "Epoch 185/200, Train Loss: 16.9939, Val Loss: 33.3280\n",
      "Epoch 186/200, Train Loss: 16.9687, Val Loss: 33.3033\n",
      "Epoch 187/200, Train Loss: 16.9551, Val Loss: 33.8252\n",
      "Epoch 188/200, Train Loss: 16.9173, Val Loss: 33.8726\n",
      "Epoch 189/200, Train Loss: 16.9648, Val Loss: 33.1613\n",
      "Epoch 190/200, Train Loss: 16.8115, Val Loss: 33.5735\n",
      "Epoch 191/200, Train Loss: 16.7732, Val Loss: 33.8620\n",
      "Epoch 192/200, Train Loss: 16.8040, Val Loss: 34.0338\n",
      "Epoch 193/200, Train Loss: 16.8102, Val Loss: 33.7521\n",
      "Epoch 194/200, Train Loss: 16.7005, Val Loss: 33.6243\n",
      "Epoch 195/200, Train Loss: 16.8711, Val Loss: 33.6135\n",
      "Epoch 196/200, Train Loss: 16.8973, Val Loss: 34.0220\n",
      "Epoch 197/200, Train Loss: 16.7917, Val Loss: 33.9487\n",
      "Epoch 198/200, Train Loss: 16.9579, Val Loss: 34.0839\n",
      "Epoch 199/200, Train Loss: 16.6836, Val Loss: 33.6359\n",
      "Epoch 200/200, Train Loss: 16.3837, Val Loss: 33.9823\n",
      "\n",
      "Loaded best model (Val Loss: 27.0187) for final hidden state extraction.\n",
      "Saved best model for NMRNN_NoSpatial_ModReadout to results/20250508_082552/NMRNN_NoSpatial_ModReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_NoSpatial_ModReadout ---\n",
      "  Analyzing decodability for NMRNN_NoSpatial_ModReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 25])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_NoSpatial_ModReadout - Test MSE: 2.2094, Test R2: -0.0950 (best alpha: 100.0000)\n",
      "Decodability (R2 score) for NMRNN_NoSpatial_ModReadout: -0.0950\n",
      "\n",
      "--- Training NMRNN_Spatial_FixedReadout ---\n",
      "Number of parameters: 66449\n",
      "Epoch 1/200, Train Loss: 29.2476, Val Loss: 27.1694\n",
      "  New best validation loss: 27.1694\n",
      "Epoch 2/200, Train Loss: 29.1388, Val Loss: 27.1253\n",
      "  New best validation loss: 27.1253\n",
      "Epoch 3/200, Train Loss: 29.1254, Val Loss: 27.1215\n",
      "  New best validation loss: 27.1215\n",
      "Epoch 4/200, Train Loss: 29.1134, Val Loss: 27.1010\n",
      "  New best validation loss: 27.1010\n",
      "Epoch 5/200, Train Loss: 29.0973, Val Loss: 27.1018\n",
      "Epoch 6/200, Train Loss: 29.0643, Val Loss: 27.1138\n",
      "Epoch 7/200, Train Loss: 29.0373, Val Loss: 27.1050\n",
      "Epoch 8/200, Train Loss: 29.0018, Val Loss: 27.1277\n",
      "Epoch 9/200, Train Loss: 28.9768, Val Loss: 27.1047\n",
      "Epoch 10/200, Train Loss: 28.9088, Val Loss: 27.1537\n",
      "Epoch 11/200, Train Loss: 28.7991, Val Loss: 27.1349\n",
      "Epoch 12/200, Train Loss: 28.6865, Val Loss: 27.0771\n",
      "  New best validation loss: 27.0771\n",
      "Epoch 13/200, Train Loss: 28.5869, Val Loss: 27.0660\n",
      "  New best validation loss: 27.0660\n",
      "Epoch 14/200, Train Loss: 28.5413, Val Loss: 27.0111\n",
      "  New best validation loss: 27.0111\n",
      "Epoch 15/200, Train Loss: 28.4175, Val Loss: 27.0108\n",
      "  New best validation loss: 27.0108\n",
      "Epoch 16/200, Train Loss: 28.2908, Val Loss: 27.1385\n",
      "Epoch 17/200, Train Loss: 28.2502, Val Loss: 27.1105\n",
      "Epoch 18/200, Train Loss: 28.1303, Val Loss: 27.0070\n",
      "  New best validation loss: 27.0070\n",
      "Epoch 19/200, Train Loss: 28.0267, Val Loss: 27.1710\n",
      "Epoch 20/200, Train Loss: 27.9662, Val Loss: 26.9722\n",
      "  New best validation loss: 26.9722\n",
      "Epoch 21/200, Train Loss: 27.8744, Val Loss: 27.1616\n",
      "Epoch 22/200, Train Loss: 27.8451, Val Loss: 27.0021\n",
      "Epoch 23/200, Train Loss: 27.7187, Val Loss: 27.0230\n",
      "Epoch 24/200, Train Loss: 27.5814, Val Loss: 26.8892\n",
      "  New best validation loss: 26.8892\n",
      "Epoch 25/200, Train Loss: 27.3839, Val Loss: 27.1136\n",
      "Epoch 26/200, Train Loss: 27.3887, Val Loss: 27.1053\n",
      "Epoch 27/200, Train Loss: 27.2309, Val Loss: 27.1646\n",
      "Epoch 28/200, Train Loss: 27.0163, Val Loss: 27.1452\n",
      "Epoch 29/200, Train Loss: 26.9973, Val Loss: 27.1085\n",
      "Epoch 30/200, Train Loss: 26.8131, Val Loss: 27.1837\n",
      "Epoch 31/200, Train Loss: 26.8602, Val Loss: 27.2819\n",
      "Epoch 32/200, Train Loss: 26.8376, Val Loss: 27.0582\n",
      "Epoch 33/200, Train Loss: 26.4756, Val Loss: 27.2432\n",
      "Epoch 34/200, Train Loss: 26.3382, Val Loss: 27.2812\n",
      "Epoch 35/200, Train Loss: 26.4148, Val Loss: 27.3584\n",
      "Epoch 36/200, Train Loss: 26.1850, Val Loss: 27.2310\n",
      "Epoch 37/200, Train Loss: 26.0817, Val Loss: 27.3737\n",
      "Epoch 38/200, Train Loss: 25.9366, Val Loss: 27.5594\n",
      "Epoch 39/200, Train Loss: 25.8066, Val Loss: 27.5536\n",
      "Epoch 40/200, Train Loss: 25.8294, Val Loss: 27.4511\n",
      "Epoch 41/200, Train Loss: 25.7828, Val Loss: 27.6492\n",
      "Epoch 42/200, Train Loss: 25.4490, Val Loss: 27.5480\n",
      "Epoch 43/200, Train Loss: 25.3130, Val Loss: 27.9005\n",
      "Epoch 44/200, Train Loss: 25.1344, Val Loss: 27.8284\n",
      "Epoch 45/200, Train Loss: 24.9038, Val Loss: 28.1193\n",
      "Epoch 46/200, Train Loss: 24.9559, Val Loss: 27.8387\n",
      "Epoch 47/200, Train Loss: 24.8655, Val Loss: 28.0318\n",
      "Epoch 48/200, Train Loss: 24.7904, Val Loss: 27.9897\n",
      "Epoch 49/200, Train Loss: 24.6906, Val Loss: 27.8142\n",
      "Epoch 50/200, Train Loss: 24.6403, Val Loss: 28.2767\n",
      "Epoch 51/200, Train Loss: 24.4133, Val Loss: 27.9306\n",
      "Epoch 52/200, Train Loss: 24.2989, Val Loss: 28.5954\n",
      "Epoch 53/200, Train Loss: 24.3658, Val Loss: 28.3926\n",
      "Epoch 54/200, Train Loss: 24.0778, Val Loss: 28.3071\n",
      "Epoch 55/200, Train Loss: 23.9416, Val Loss: 28.0677\n",
      "Epoch 56/200, Train Loss: 24.0575, Val Loss: 28.5318\n",
      "Epoch 57/200, Train Loss: 24.1162, Val Loss: 28.9329\n",
      "Epoch 58/200, Train Loss: 24.0340, Val Loss: 28.4429\n",
      "Epoch 59/200, Train Loss: 24.0736, Val Loss: 28.4863\n",
      "Epoch 60/200, Train Loss: 23.8542, Val Loss: 28.6468\n",
      "Epoch 61/200, Train Loss: 23.6117, Val Loss: 28.6982\n",
      "Epoch 62/200, Train Loss: 23.4836, Val Loss: 28.2892\n",
      "Epoch 63/200, Train Loss: 23.3418, Val Loss: 28.6184\n",
      "Epoch 64/200, Train Loss: 23.4460, Val Loss: 28.6998\n",
      "Epoch 65/200, Train Loss: 23.3022, Val Loss: 28.7786\n",
      "Epoch 66/200, Train Loss: 23.3733, Val Loss: 28.5990\n",
      "Epoch 67/200, Train Loss: 23.1561, Val Loss: 28.7356\n",
      "Epoch 68/200, Train Loss: 22.9633, Val Loss: 28.9924\n",
      "Epoch 69/200, Train Loss: 22.9268, Val Loss: 29.2095\n",
      "Epoch 70/200, Train Loss: 23.0273, Val Loss: 28.8479\n",
      "Epoch 71/200, Train Loss: 22.7516, Val Loss: 28.8991\n",
      "Epoch 72/200, Train Loss: 22.7603, Val Loss: 28.9027\n",
      "Epoch 73/200, Train Loss: 22.6740, Val Loss: 29.1678\n",
      "Epoch 74/200, Train Loss: 22.8394, Val Loss: 28.9363\n",
      "Epoch 75/200, Train Loss: 22.6137, Val Loss: 29.2033\n",
      "Epoch 76/200, Train Loss: 22.6461, Val Loss: 28.8743\n",
      "Epoch 77/200, Train Loss: 22.4268, Val Loss: 29.1936\n",
      "Epoch 78/200, Train Loss: 22.5800, Val Loss: 29.0702\n",
      "Epoch 79/200, Train Loss: 22.3628, Val Loss: 28.9454\n",
      "Epoch 80/200, Train Loss: 22.2054, Val Loss: 29.5167\n",
      "Epoch 81/200, Train Loss: 22.4605, Val Loss: 29.1705\n",
      "Epoch 82/200, Train Loss: 22.1200, Val Loss: 29.6653\n",
      "Epoch 83/200, Train Loss: 21.9724, Val Loss: 29.3416\n",
      "Epoch 84/200, Train Loss: 21.9521, Val Loss: 29.1331\n",
      "Epoch 85/200, Train Loss: 22.0085, Val Loss: 29.3012\n",
      "Epoch 86/200, Train Loss: 22.0706, Val Loss: 29.8438\n",
      "Epoch 87/200, Train Loss: 22.1107, Val Loss: 29.2922\n",
      "Epoch 88/200, Train Loss: 21.9247, Val Loss: 29.5415\n",
      "Epoch 89/200, Train Loss: 21.5668, Val Loss: 29.2380\n",
      "Epoch 90/200, Train Loss: 21.5626, Val Loss: 29.0685\n",
      "Epoch 91/200, Train Loss: 21.7043, Val Loss: 29.3396\n",
      "Epoch 92/200, Train Loss: 21.7948, Val Loss: 29.8260\n",
      "Epoch 93/200, Train Loss: 22.1134, Val Loss: 29.9203\n",
      "Epoch 94/200, Train Loss: 21.8871, Val Loss: 29.4031\n",
      "Epoch 95/200, Train Loss: 21.7220, Val Loss: 29.1745\n",
      "Epoch 96/200, Train Loss: 21.6362, Val Loss: 29.1501\n",
      "Epoch 97/200, Train Loss: 21.5578, Val Loss: 29.8096\n",
      "Epoch 98/200, Train Loss: 21.4972, Val Loss: 29.5721\n",
      "Epoch 99/200, Train Loss: 21.5191, Val Loss: 29.7244\n",
      "Epoch 100/200, Train Loss: 21.6924, Val Loss: 29.9515\n",
      "Epoch 101/200, Train Loss: 21.2746, Val Loss: 29.9213\n",
      "Epoch 102/200, Train Loss: 21.2356, Val Loss: 30.0958\n",
      "Epoch 103/200, Train Loss: 21.1489, Val Loss: 29.6884\n",
      "Epoch 104/200, Train Loss: 21.0165, Val Loss: 30.3081\n",
      "Epoch 105/200, Train Loss: 20.9405, Val Loss: 29.4440\n",
      "Epoch 106/200, Train Loss: 20.8177, Val Loss: 30.5490\n",
      "Epoch 107/200, Train Loss: 20.8755, Val Loss: 30.2715\n",
      "Epoch 108/200, Train Loss: 20.9861, Val Loss: 30.3090\n",
      "Epoch 109/200, Train Loss: 20.7975, Val Loss: 30.8469\n",
      "Epoch 110/200, Train Loss: 20.8908, Val Loss: 30.2377\n",
      "Epoch 111/200, Train Loss: 20.9204, Val Loss: 30.1810\n",
      "Epoch 112/200, Train Loss: 20.9034, Val Loss: 30.4219\n",
      "Epoch 113/200, Train Loss: 20.7634, Val Loss: 29.9015\n",
      "Epoch 114/200, Train Loss: 20.6599, Val Loss: 30.3867\n",
      "Epoch 115/200, Train Loss: 20.7285, Val Loss: 29.5389\n",
      "Epoch 116/200, Train Loss: 20.3792, Val Loss: 30.8090\n",
      "Epoch 117/200, Train Loss: 20.6090, Val Loss: 30.7405\n",
      "Epoch 118/200, Train Loss: 20.4797, Val Loss: 30.1133\n",
      "Epoch 119/200, Train Loss: 20.4415, Val Loss: 30.6809\n",
      "Epoch 120/200, Train Loss: 20.3817, Val Loss: 30.3219\n",
      "Epoch 121/200, Train Loss: 20.2824, Val Loss: 30.3769\n",
      "Epoch 122/200, Train Loss: 20.2149, Val Loss: 30.8495\n",
      "Epoch 123/200, Train Loss: 20.3047, Val Loss: 31.2702\n",
      "Epoch 124/200, Train Loss: 20.5587, Val Loss: 30.2049\n",
      "Epoch 125/200, Train Loss: 20.1439, Val Loss: 30.6721\n",
      "Epoch 126/200, Train Loss: 20.2174, Val Loss: 31.1976\n",
      "Epoch 127/200, Train Loss: 20.2451, Val Loss: 30.7452\n",
      "Epoch 128/200, Train Loss: 19.9762, Val Loss: 30.8393\n",
      "Epoch 129/200, Train Loss: 19.9222, Val Loss: 30.7522\n",
      "Epoch 130/200, Train Loss: 19.8681, Val Loss: 31.1735\n",
      "Epoch 131/200, Train Loss: 19.9144, Val Loss: 30.9741\n",
      "Epoch 132/200, Train Loss: 20.0745, Val Loss: 31.6335\n",
      "Epoch 133/200, Train Loss: 20.1988, Val Loss: 30.8720\n",
      "Epoch 134/200, Train Loss: 19.8355, Val Loss: 30.9776\n",
      "Epoch 135/200, Train Loss: 19.8956, Val Loss: 31.1312\n",
      "Epoch 136/200, Train Loss: 19.6710, Val Loss: 31.7748\n",
      "Epoch 137/200, Train Loss: 19.6647, Val Loss: 31.4964\n",
      "Epoch 138/200, Train Loss: 19.8404, Val Loss: 30.8441\n",
      "Epoch 139/200, Train Loss: 19.7296, Val Loss: 32.1256\n",
      "Epoch 140/200, Train Loss: 19.5521, Val Loss: 30.7201\n",
      "Epoch 141/200, Train Loss: 19.9429, Val Loss: 31.9250\n",
      "Epoch 142/200, Train Loss: 19.5857, Val Loss: 31.0703\n",
      "Epoch 143/200, Train Loss: 19.5077, Val Loss: 31.0604\n",
      "Epoch 144/200, Train Loss: 19.3844, Val Loss: 31.1660\n",
      "Epoch 145/200, Train Loss: 19.3809, Val Loss: 31.1401\n",
      "Epoch 146/200, Train Loss: 19.2723, Val Loss: 31.8371\n",
      "Epoch 147/200, Train Loss: 19.4888, Val Loss: 31.6804\n",
      "Epoch 148/200, Train Loss: 19.4074, Val Loss: 31.0579\n",
      "Epoch 149/200, Train Loss: 19.6161, Val Loss: 31.4382\n",
      "Epoch 150/200, Train Loss: 19.7458, Val Loss: 32.0702\n",
      "Epoch 151/200, Train Loss: 19.5151, Val Loss: 31.2719\n",
      "Epoch 152/200, Train Loss: 19.5659, Val Loss: 31.7111\n",
      "Epoch 153/200, Train Loss: 19.2681, Val Loss: 31.5409\n",
      "Epoch 154/200, Train Loss: 19.1597, Val Loss: 31.9456\n",
      "Epoch 155/200, Train Loss: 19.1696, Val Loss: 32.2796\n",
      "Epoch 156/200, Train Loss: 19.2222, Val Loss: 32.5011\n",
      "Epoch 157/200, Train Loss: 19.1551, Val Loss: 31.5119\n",
      "Epoch 158/200, Train Loss: 19.0213, Val Loss: 31.7443\n",
      "Epoch 159/200, Train Loss: 18.9025, Val Loss: 31.5899\n",
      "Epoch 160/200, Train Loss: 18.9781, Val Loss: 32.2546\n",
      "Epoch 161/200, Train Loss: 18.8477, Val Loss: 31.8537\n",
      "Epoch 162/200, Train Loss: 19.1128, Val Loss: 30.7498\n",
      "Epoch 163/200, Train Loss: 18.9667, Val Loss: 32.9002\n",
      "Epoch 164/200, Train Loss: 18.8784, Val Loss: 31.2176\n",
      "Epoch 165/200, Train Loss: 18.9685, Val Loss: 32.1253\n",
      "Epoch 166/200, Train Loss: 18.8880, Val Loss: 31.6951\n",
      "Epoch 167/200, Train Loss: 19.1150, Val Loss: 32.1688\n",
      "Epoch 168/200, Train Loss: 18.9284, Val Loss: 31.9986\n",
      "Epoch 169/200, Train Loss: 19.1734, Val Loss: 32.5400\n",
      "Epoch 170/200, Train Loss: 18.9473, Val Loss: 31.7837\n",
      "Epoch 171/200, Train Loss: 18.8636, Val Loss: 32.4977\n",
      "Epoch 172/200, Train Loss: 18.8970, Val Loss: 32.3899\n",
      "Epoch 173/200, Train Loss: 18.7040, Val Loss: 32.0626\n",
      "Epoch 174/200, Train Loss: 18.6160, Val Loss: 31.7790\n",
      "Epoch 175/200, Train Loss: 18.6506, Val Loss: 31.7291\n",
      "Epoch 176/200, Train Loss: 18.6236, Val Loss: 32.1698\n",
      "Epoch 177/200, Train Loss: 18.4994, Val Loss: 33.0218\n",
      "Epoch 178/200, Train Loss: 18.6263, Val Loss: 32.3367\n",
      "Epoch 179/200, Train Loss: 18.7874, Val Loss: 32.0839\n",
      "Epoch 180/200, Train Loss: 18.6786, Val Loss: 32.4389\n",
      "Epoch 181/200, Train Loss: 18.4450, Val Loss: 32.3522\n",
      "Epoch 182/200, Train Loss: 18.2798, Val Loss: 31.6233\n",
      "Epoch 183/200, Train Loss: 18.4697, Val Loss: 32.9583\n",
      "Epoch 184/200, Train Loss: 18.6768, Val Loss: 32.8240\n",
      "Epoch 185/200, Train Loss: 18.3219, Val Loss: 32.2195\n",
      "Epoch 186/200, Train Loss: 18.0757, Val Loss: 32.4727\n",
      "Epoch 187/200, Train Loss: 18.0287, Val Loss: 31.3063\n",
      "Epoch 188/200, Train Loss: 18.0081, Val Loss: 32.8020\n",
      "Epoch 189/200, Train Loss: 17.8478, Val Loss: 31.9328\n",
      "Epoch 190/200, Train Loss: 18.1024, Val Loss: 32.9706\n",
      "Epoch 191/200, Train Loss: 17.9172, Val Loss: 33.5504\n",
      "Epoch 192/200, Train Loss: 17.9252, Val Loss: 33.1804\n",
      "Epoch 193/200, Train Loss: 17.9678, Val Loss: 32.7393\n",
      "Epoch 194/200, Train Loss: 18.1948, Val Loss: 32.3771\n",
      "Epoch 195/200, Train Loss: 17.8904, Val Loss: 32.7003\n",
      "Epoch 196/200, Train Loss: 17.8687, Val Loss: 33.0946\n",
      "Epoch 197/200, Train Loss: 17.7646, Val Loss: 32.6717\n",
      "Epoch 198/200, Train Loss: 17.7586, Val Loss: 32.3514\n",
      "Epoch 199/200, Train Loss: 17.6476, Val Loss: 32.5151\n",
      "Epoch 200/200, Train Loss: 18.0806, Val Loss: 33.0953\n",
      "\n",
      "Loaded best model (Val Loss: 26.8892) for final hidden state extraction.\n",
      "Saved best model for NMRNN_Spatial_FixedReadout to results/20250508_082552/NMRNN_Spatial_FixedReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_Spatial_FixedReadout ---\n",
      "  Analyzing decodability for NMRNN_Spatial_FixedReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 25])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_Spatial_FixedReadout - Test MSE: 2.1470, Test R2: -0.0601 (best alpha: 100.0000)\n",
      "Decodability (R2 score) for NMRNN_Spatial_FixedReadout: -0.0601\n",
      "Learning curves saved to results/20250508_082552/learning_curves.png\n",
      "\n",
      "Learning curves plotted to results/20250508_082552/learning_curves.png\n",
      "\n",
      "--- Decodability Results (R2 Score) ---\n",
      "ComplexOscillatorNet: 0.0197\n",
      "RNN_GRU: -0.0138\n",
      "Transformer: -0.0607\n",
      "HIPPORNN_LegT: 0.0255\n",
      "NMRNN_Spatial_ModReadout: -0.0514\n",
      "NMRNN_NoSpatial_ModReadout: -0.0950\n",
      "NMRNN_Spatial_FixedReadout: -0.0601\n",
      "\n",
      "Experiment finished. All results in results/20250508_082552\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Project specific imports\n",
    "from dataset import CompositionalDataset, create_dataloaders\n",
    "from model import NonlinearOscillatorNet, RNNModel, TransformerModel, HippoRNNModel, NMRNN_Spatial_ModulatedReadout, NMRNN_NoSpatial_ModulatedReadout, NMRNN_Spatial_FixedReadout\n",
    "from training import train_model_comparative\n",
    "from analysis import plot_learning_curves, perform_decodability_analysis\n",
    "from utils import set_seed, get_device, count_parameters\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"seed\": 0,\n",
    "    \"num_task_coefficients\": 25, \n",
    "    \"seq_length\": 200,          \n",
    "    \"train_samples\": 32 * 10,  # Reduced for quicker testing, increase for real runs\n",
    "    \"test_samples\": 32 * 5,   # Reduced for quicker testing\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200, # Reduced for quick test, increase for real runs (e.g., 50-200)\n",
    "    \"lr\": 1e-3,\n",
    "    \n",
    "    # Model-specific hidden sizes / main dimension\n",
    "    \"hidden_size_oscillator\": 64, \n",
    "    \"hidden_size_rnn\": 128,       # For standard GRU/LSTM\n",
    "    \"d_model_transformer\": 64,   \n",
    "    \"hidden_size_hippo\": 128,     # N for HIPPO\n",
    "    \"hidden_size_nm_rnn\": 128,    # n_rnn for nmRNN variants\n",
    "\n",
    "    # Transformer specific\n",
    "    \"nhead_transformer\": 1,\n",
    "    \"num_layers_transformer\": 1,\n",
    "    \n",
    "    # HIPPORNN specific\n",
    "    \"hippo_method\": 'legt', # 'legs' or 'legt'\n",
    "    \"hippo_theta\": 1.0,     # Required for 'legt'\n",
    "    \"hippo_dt\": 1.0 / 200,  # Discretization step for HIPPO (e.g., 1.0 / seq_length)\n",
    "    \"hippo_inv_eps\": 1e-6, # Epsilon for LegS matrix inversion regularization\n",
    "    \"hippo_clip_val\": 50.0, # Clipping for HIPPO state c_t\n",
    "\n",
    "    # nmRNN specific (shared for variants where applicable)\n",
    "    \"nm_N_NM\": 4,               # Number of neuromodulators\n",
    "    \"nm_activation\": 'tanh',    # 'relu', 'tanh' (original code had 'relu-tanh', simplified here)\n",
    "    \"nm_decay\": 0.05, # dt_sec / tau_rnn, e.g., (20ms/step) / (100ms tau) -> exp(-0.2)\n",
    "                                     # Original: math.exp(-20/100) - assuming 20ms step, 100ms tau\n",
    "    \"nm_bias\": True,\n",
    "    \"nm_keepW0_spatial\": False, # For the version with spatial connections\n",
    "    \"nm_keepW0_no_spatial\": False,\n",
    "    \"nm_grad_clip\": 1.0,\n",
    "    \"nm_spatial_ell\": 0.1,      # For SpatialWeight\n",
    "    \"nm_spatial_scale\": 1.0,    # For SpatialWeight\n",
    "\n",
    "    # General task params\n",
    "    \"output_dim\": 1, \n",
    "    \"input_dim\": 1,  \n",
    "    \"noise_level_data\": 0.01,\n",
    "    \"run_timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"results_dir\": \"results\"\n",
    "}\n",
    "\n",
    "def run_experiment():\n",
    "    \"\"\"\n",
    "    Runs the full comparative analysis experiment.\n",
    "    \"\"\"\n",
    "    set_seed(CONFIG[\"seed\"])\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    os.makedirs(CONFIG[\"results_dir\"], exist_ok=True)\n",
    "    run_results_dir = os.path.join(CONFIG[\"results_dir\"], CONFIG[\"run_timestamp\"])\n",
    "    os.makedirs(run_results_dir, exist_ok=True)\n",
    "    print(f\"Results will be saved in: {run_results_dir}\")\n",
    "\n",
    "    # --- 1. Dataset ---\n",
    "    print(\"Loading dataset...\")\n",
    "    train_loader, val_loader, test_loader, (input_basis, output_basis) = create_dataloaders(\n",
    "        num_train_samples=CONFIG[\"train_samples\"],\n",
    "        num_val_samples=CONFIG[\"test_samples\"], \n",
    "        num_test_samples=CONFIG[\"test_samples\"],\n",
    "        num_basis=CONFIG[\"num_task_coefficients\"],\n",
    "        seq_length=CONFIG[\"seq_length\"],\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        noise=CONFIG[\"noise_level_data\"]\n",
    "    )\n",
    "    print(\"Dataset loaded.\")\n",
    "\n",
    "    # --- 2. Models ---\n",
    "    models_to_test = {\n",
    "        \"ComplexOscillatorNet\": NonlinearOscillatorNet(\n",
    "            N_oscillators=CONFIG[\"hidden_size_oscillator\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            seq_length=CONFIG[\"seq_length\"], \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"RNN_GRU\": RNNModel(\n",
    "            hidden_size=CONFIG[\"hidden_size_rnn\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            num_layers=1, \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"Transformer\": TransformerModel(\n",
    "            d_model=CONFIG[\"d_model_transformer\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            num_heads=CONFIG[\"nhead_transformer\"],\n",
    "            num_layers=CONFIG[\"num_layers_transformer\"],\n",
    "            seq_length=CONFIG[\"seq_length\"], \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"HIPPORNN_LegT\": HippoRNNModel( # Using LegT by default as per config\n",
    "            hidden_size=CONFIG[\"hidden_size_hippo\"],\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            method=CONFIG[\"hippo_method\"], \n",
    "            theta=CONFIG[\"hippo_theta\"],\n",
    "            dt=CONFIG[\"hippo_dt\"],\n",
    "            inv_eps=CONFIG[\"hippo_inv_eps\"],\n",
    "            clip_val=CONFIG[\"hippo_clip_val\"],\n",
    "            device=device, # Pass device\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_Spatial_ModReadout\": NMRNN_Spatial_ModulatedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"],\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_spatial\"],\n",
    "            spatial_ell=CONFIG[\"nm_spatial_ell\"],\n",
    "            spatial_scale=CONFIG[\"nm_spatial_scale\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_NoSpatial_ModReadout\": NMRNN_NoSpatial_ModulatedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"],\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_no_spatial\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_Spatial_FixedReadout\": NMRNN_Spatial_FixedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"], # N_nm still needed for the core recurrence, just not readout\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_spatial\"],\n",
    "            spatial_ell=CONFIG[\"nm_spatial_ell\"],\n",
    "            spatial_scale=CONFIG[\"nm_spatial_scale\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    all_val_losses = {}\n",
    "    decodability_results = {}\n",
    "    trained_models_paths = {}\n",
    "\n",
    "    # --- 3. Training & Evaluation Loop ---\n",
    "    for model_name, model in models_to_test.items():\n",
    "        print(f\"\\n--- Training {model_name} ---\")\n",
    "        model.to(device)\n",
    "        print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "\n",
    "        try:\n",
    "            val_losses, best_model_state, hidden_states_test, coeffs_test = train_model_comparative(\n",
    "                model,\n",
    "                model_name,\n",
    "                train_loader,\n",
    "                val_loader, \n",
    "                test_loader, \n",
    "                CONFIG[\"epochs\"],\n",
    "                CONFIG[\"lr\"],\n",
    "                device,\n",
    "                CONFIG[\"num_task_coefficients\"], \n",
    "                run_results_dir,\n",
    "                plot_intermediate_results=(len(models_to_test) == 1) \n",
    "            )\n",
    "            all_val_losses[model_name] = val_losses\n",
    "            \n",
    "            if best_model_state:\n",
    "                model_path = os.path.join(run_results_dir, f\"{model_name}_best.pt\")\n",
    "                torch.save(best_model_state, model_path)\n",
    "                trained_models_paths[model_name] = model_path\n",
    "                print(f\"Saved best model for {model_name} to {model_path}\")\n",
    "            else:\n",
    "                print(f\"No best model state saved for {model_name} (possibly due to training issues).\")\n",
    "\n",
    "            # --- 4. Decodability Analysis ---\n",
    "            if hidden_states_test is not None and coeffs_test is not None:\n",
    "                print(f\"\\n--- Performing Decodability Analysis for {model_name} ---\")\n",
    "                decodability_score = perform_decodability_analysis(\n",
    "                    model_name=model_name, \n",
    "                    hidden_states=hidden_states_test, \n",
    "                    coefficients=coeffs_test,       \n",
    "                    decoder_type='ridge', # Using RidgeCV as a robust default\n",
    "                    decoding_metric='r2', # R-squared is often more interpretable than MSE here\n",
    "                    results_dir=run_results_dir,\n",
    "                    device=device,\n",
    "                )\n",
    "                decodability_results[model_name] = decodability_score\n",
    "                print(f\"Decodability (R2 score) for {model_name}: {decodability_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"Skipping decodability for {model_name} due to missing hidden states or coefficients.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"!!!!!! ERROR during training or analysis for {model_name}: {e} !!!!!!\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            all_val_losses[model_name] = [float('nan')] * CONFIG[\"epochs\"] # Log error for this model\n",
    "            decodability_results[model_name] = float('nan')\n",
    "\n",
    "\n",
    "    # --- 5. Plot Learning Curves ---\n",
    "    if any(all_val_losses.values()): # Check if there's anything to plot\n",
    "        plot_learning_curves(all_val_losses, title=\"Validation Learning Curves\", save_path=os.path.join(run_results_dir, \"learning_curves.png\"))\n",
    "        print(f\"\\nLearning curves plotted to {os.path.join(run_results_dir, 'learning_curves.png')}\")\n",
    "\n",
    "    # --- 6. Report Decodability ---\n",
    "    print(\"\\n--- Decodability Results (R2 Score) ---\")\n",
    "    if decodability_results:\n",
    "        for model_name, score in decodability_results.items():\n",
    "            print(f\"{model_name}: {score:.4f}\")\n",
    "        with open(os.path.join(run_results_dir, \"decodability_summary.txt\"), \"w\") as f:\n",
    "            f.write(\"Model,R2_Score\\n\")\n",
    "            for model_name, score in decodability_results.items():\n",
    "                f.write(f\"{model_name},{score:.4f}\\n\")\n",
    "    else:\n",
    "        print(\"No decodability results to report.\")\n",
    "        \n",
    "    print(f\"\\nExperiment finished. All results in {run_results_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ec43da-6ade-4862-81be-ed723ffe257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951229424500714"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-1.0 / 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba850af2-8fe7-4ac3-a31b-87c844aef3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
