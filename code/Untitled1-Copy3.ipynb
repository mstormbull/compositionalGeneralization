{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfca335-29cc-423c-a5a1-16b167672c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Results will be saved in: results/20250508_090642\n",
      "Loading dataset...\n",
      "Dataset loaded.\n",
      "\n",
      "--- Training ComplexOscillatorNet ---\n",
      "Number of parameters: 4480\n",
      "Epoch 1/200, Train Loss: 11.6030, Val Loss: 10.3057\n",
      "  New best validation loss: 10.3057\n",
      "Epoch 2/200, Train Loss: 11.1090, Val Loss: 9.9933\n",
      "  New best validation loss: 9.9933\n",
      "Epoch 3/200, Train Loss: 10.7814, Val Loss: 9.7488\n",
      "  New best validation loss: 9.7488\n",
      "Epoch 4/200, Train Loss: 10.5311, Val Loss: 9.5342\n",
      "  New best validation loss: 9.5342\n",
      "Epoch 5/200, Train Loss: 10.3239, Val Loss: 9.3870\n",
      "  New best validation loss: 9.3870\n",
      "Epoch 6/200, Train Loss: 10.1541, Val Loss: 9.2522\n",
      "  New best validation loss: 9.2522\n",
      "Epoch 7/200, Train Loss: 10.0302, Val Loss: 9.1912\n",
      "  New best validation loss: 9.1912\n",
      "Epoch 8/200, Train Loss: 9.9141, Val Loss: 9.1017\n",
      "  New best validation loss: 9.1017\n",
      "Epoch 9/200, Train Loss: 9.8186, Val Loss: 9.0746\n",
      "  New best validation loss: 9.0746\n",
      "Epoch 10/200, Train Loss: 9.7362, Val Loss: 9.0192\n",
      "  New best validation loss: 9.0192\n",
      "Epoch 11/200, Train Loss: 9.6722, Val Loss: 9.0035\n",
      "  New best validation loss: 9.0035\n",
      "Epoch 12/200, Train Loss: 9.6118, Val Loss: 8.9692\n",
      "  New best validation loss: 8.9692\n",
      "Epoch 13/200, Train Loss: 9.5649, Val Loss: 8.9573\n",
      "  New best validation loss: 8.9573\n",
      "Epoch 14/200, Train Loss: 9.5192, Val Loss: 8.9403\n",
      "  New best validation loss: 8.9403\n",
      "Epoch 15/200, Train Loss: 9.4749, Val Loss: 8.9248\n",
      "  New best validation loss: 8.9248\n",
      "Epoch 16/200, Train Loss: 9.4401, Val Loss: 8.9167\n",
      "  New best validation loss: 8.9167\n",
      "Epoch 17/200, Train Loss: 9.4152, Val Loss: 8.9018\n",
      "  New best validation loss: 8.9018\n",
      "Epoch 18/200, Train Loss: 9.4011, Val Loss: 8.8893\n",
      "  New best validation loss: 8.8893\n",
      "Epoch 19/200, Train Loss: 9.3666, Val Loss: 8.8875\n",
      "  New best validation loss: 8.8875\n",
      "Epoch 20/200, Train Loss: 9.3387, Val Loss: 8.8835\n",
      "  New best validation loss: 8.8835\n",
      "Epoch 21/200, Train Loss: 9.3143, Val Loss: 8.8850\n",
      "Epoch 22/200, Train Loss: 9.2956, Val Loss: 8.8698\n",
      "  New best validation loss: 8.8698\n",
      "Epoch 23/200, Train Loss: 9.2627, Val Loss: 8.8542\n",
      "  New best validation loss: 8.8542\n",
      "Epoch 24/200, Train Loss: 9.2434, Val Loss: 8.8692\n",
      "Epoch 25/200, Train Loss: 9.2222, Val Loss: 8.8704\n",
      "Epoch 26/200, Train Loss: 9.1956, Val Loss: 8.8808\n",
      "Epoch 27/200, Train Loss: 9.1854, Val Loss: 8.8864\n",
      "Epoch 28/200, Train Loss: 9.1932, Val Loss: 8.8702\n",
      "Epoch 29/200, Train Loss: 9.1478, Val Loss: 8.9003\n",
      "Epoch 30/200, Train Loss: 9.1337, Val Loss: 8.8715\n",
      "Epoch 31/200, Train Loss: 9.0899, Val Loss: 8.8850\n",
      "Epoch 32/200, Train Loss: 9.0754, Val Loss: 8.8944\n",
      "Epoch 33/200, Train Loss: 9.0580, Val Loss: 8.9223\n",
      "Epoch 34/200, Train Loss: 9.0405, Val Loss: 8.8778\n",
      "Epoch 35/200, Train Loss: 9.0371, Val Loss: 8.9255\n",
      "Epoch 36/200, Train Loss: 9.0135, Val Loss: 8.9227\n",
      "Epoch 37/200, Train Loss: 8.9973, Val Loss: 8.9103\n",
      "Epoch 38/200, Train Loss: 8.9765, Val Loss: 8.8952\n",
      "Epoch 39/200, Train Loss: 8.9618, Val Loss: 8.9257\n",
      "Epoch 40/200, Train Loss: 8.9478, Val Loss: 8.9238\n",
      "Epoch 41/200, Train Loss: 8.9412, Val Loss: 8.9397\n",
      "Epoch 42/200, Train Loss: 8.9283, Val Loss: 8.9366\n",
      "Epoch 43/200, Train Loss: 8.9211, Val Loss: 8.9311\n",
      "Epoch 44/200, Train Loss: 8.8907, Val Loss: 8.9586\n",
      "Epoch 45/200, Train Loss: 8.8792, Val Loss: 8.9562\n",
      "Epoch 46/200, Train Loss: 8.8518, Val Loss: 8.9604\n",
      "Epoch 47/200, Train Loss: 8.8548, Val Loss: 8.9831\n",
      "Epoch 48/200, Train Loss: 8.8355, Val Loss: 8.9377\n",
      "Epoch 49/200, Train Loss: 8.8324, Val Loss: 8.9934\n",
      "Epoch 50/200, Train Loss: 8.8369, Val Loss: 8.9304\n",
      "Epoch 51/200, Train Loss: 8.8207, Val Loss: 8.9784\n",
      "Epoch 52/200, Train Loss: 8.8190, Val Loss: 9.0097\n",
      "Epoch 53/200, Train Loss: 8.8227, Val Loss: 8.9867\n",
      "Epoch 54/200, Train Loss: 8.8200, Val Loss: 8.9859\n",
      "Epoch 55/200, Train Loss: 8.8088, Val Loss: 9.0384\n",
      "Epoch 56/200, Train Loss: 8.7711, Val Loss: 8.9858\n",
      "Epoch 57/200, Train Loss: 8.7746, Val Loss: 8.9964\n",
      "Epoch 58/200, Train Loss: 8.7627, Val Loss: 9.0360\n",
      "Epoch 59/200, Train Loss: 8.7327, Val Loss: 8.9925\n",
      "Epoch 60/200, Train Loss: 8.7326, Val Loss: 9.0289\n",
      "Epoch 61/200, Train Loss: 8.7156, Val Loss: 9.0006\n",
      "Epoch 62/200, Train Loss: 8.7116, Val Loss: 9.0331\n",
      "Epoch 63/200, Train Loss: 8.6858, Val Loss: 9.0076\n",
      "Epoch 64/200, Train Loss: 8.6886, Val Loss: 9.0027\n",
      "Epoch 65/200, Train Loss: 8.6669, Val Loss: 8.9983\n",
      "Epoch 66/200, Train Loss: 8.6629, Val Loss: 9.0395\n",
      "Epoch 67/200, Train Loss: 8.6585, Val Loss: 9.0226\n",
      "Epoch 68/200, Train Loss: 8.6458, Val Loss: 9.0011\n",
      "Epoch 69/200, Train Loss: 8.6446, Val Loss: 9.0047\n",
      "Epoch 70/200, Train Loss: 8.6312, Val Loss: 9.0259\n",
      "Epoch 71/200, Train Loss: 8.6317, Val Loss: 9.0242\n",
      "Epoch 72/200, Train Loss: 8.6596, Val Loss: 8.9919\n",
      "Epoch 73/200, Train Loss: 8.6222, Val Loss: 9.0215\n",
      "Epoch 74/200, Train Loss: 8.6157, Val Loss: 9.0277\n",
      "Epoch 75/200, Train Loss: 8.5979, Val Loss: 9.0153\n",
      "Epoch 76/200, Train Loss: 8.5842, Val Loss: 9.0615\n",
      "Epoch 77/200, Train Loss: 8.5695, Val Loss: 9.0560\n",
      "Epoch 78/200, Train Loss: 8.5736, Val Loss: 9.0244\n",
      "Epoch 79/200, Train Loss: 8.5585, Val Loss: 9.0402\n",
      "Epoch 80/200, Train Loss: 8.5712, Val Loss: 9.0608\n",
      "Epoch 81/200, Train Loss: 8.5746, Val Loss: 9.0492\n",
      "Epoch 82/200, Train Loss: 8.5798, Val Loss: 9.0383\n",
      "Epoch 83/200, Train Loss: 8.5758, Val Loss: 9.0467\n",
      "Epoch 84/200, Train Loss: 8.5682, Val Loss: 9.0966\n",
      "Epoch 85/200, Train Loss: 8.5572, Val Loss: 9.0428\n",
      "Epoch 86/200, Train Loss: 8.5589, Val Loss: 9.0419\n",
      "Epoch 87/200, Train Loss: 8.5314, Val Loss: 9.0169\n",
      "Epoch 88/200, Train Loss: 8.5292, Val Loss: 9.0586\n",
      "Epoch 89/200, Train Loss: 8.5020, Val Loss: 9.0161\n",
      "Epoch 90/200, Train Loss: 8.5036, Val Loss: 9.0301\n",
      "Epoch 91/200, Train Loss: 8.5044, Val Loss: 9.0856\n",
      "Epoch 92/200, Train Loss: 8.5035, Val Loss: 9.0597\n",
      "Epoch 93/200, Train Loss: 8.4881, Val Loss: 9.0127\n",
      "Epoch 94/200, Train Loss: 8.4777, Val Loss: 9.0408\n",
      "Epoch 95/200, Train Loss: 8.4691, Val Loss: 9.0636\n",
      "Epoch 96/200, Train Loss: 8.4692, Val Loss: 9.0445\n",
      "Epoch 97/200, Train Loss: 8.4541, Val Loss: 9.1105\n",
      "Epoch 98/200, Train Loss: 8.4720, Val Loss: 9.0560\n",
      "Epoch 99/200, Train Loss: 8.4664, Val Loss: 9.0934\n",
      "Epoch 100/200, Train Loss: 8.4573, Val Loss: 9.0622\n",
      "Epoch 101/200, Train Loss: 8.4591, Val Loss: 9.0937\n",
      "Epoch 102/200, Train Loss: 8.4495, Val Loss: 9.0457\n",
      "Epoch 103/200, Train Loss: 8.4400, Val Loss: 9.0378\n",
      "Epoch 104/200, Train Loss: 8.4606, Val Loss: 9.0578\n",
      "Epoch 105/200, Train Loss: 8.4269, Val Loss: 9.0595\n",
      "Epoch 106/200, Train Loss: 8.4240, Val Loss: 9.0136\n",
      "Epoch 107/200, Train Loss: 8.4160, Val Loss: 9.0626\n",
      "Epoch 108/200, Train Loss: 8.4056, Val Loss: 9.0502\n",
      "Epoch 109/200, Train Loss: 8.4159, Val Loss: 9.0580\n",
      "Epoch 110/200, Train Loss: 8.4168, Val Loss: 9.0460\n",
      "Epoch 111/200, Train Loss: 8.4133, Val Loss: 9.0724\n",
      "Epoch 112/200, Train Loss: 8.4042, Val Loss: 9.0478\n",
      "Epoch 113/200, Train Loss: 8.3787, Val Loss: 9.0607\n",
      "Epoch 114/200, Train Loss: 8.3712, Val Loss: 9.0726\n",
      "Epoch 115/200, Train Loss: 8.3681, Val Loss: 9.0892\n",
      "Epoch 116/200, Train Loss: 8.3581, Val Loss: 9.0463\n",
      "Epoch 117/200, Train Loss: 8.3717, Val Loss: 9.0767\n",
      "Epoch 118/200, Train Loss: 8.3635, Val Loss: 9.0797\n",
      "Epoch 119/200, Train Loss: 8.3663, Val Loss: 9.1254\n",
      "Epoch 120/200, Train Loss: 8.3647, Val Loss: 9.0644\n",
      "Epoch 121/200, Train Loss: 8.3631, Val Loss: 9.1032\n",
      "Epoch 122/200, Train Loss: 8.3669, Val Loss: 9.0893\n",
      "Epoch 123/200, Train Loss: 8.3625, Val Loss: 9.1329\n",
      "Epoch 124/200, Train Loss: 8.3688, Val Loss: 9.0499\n",
      "Epoch 125/200, Train Loss: 8.3495, Val Loss: 9.0855\n",
      "Epoch 126/200, Train Loss: 8.3205, Val Loss: 9.0894\n",
      "Epoch 127/200, Train Loss: 8.3112, Val Loss: 9.1094\n",
      "Epoch 128/200, Train Loss: 8.3143, Val Loss: 9.0695\n",
      "Epoch 129/200, Train Loss: 8.3165, Val Loss: 9.0948\n",
      "Epoch 130/200, Train Loss: 8.3018, Val Loss: 9.0913\n",
      "Epoch 131/200, Train Loss: 8.3070, Val Loss: 9.0970\n",
      "Epoch 132/200, Train Loss: 8.3219, Val Loss: 9.1089\n",
      "Epoch 133/200, Train Loss: 8.3224, Val Loss: 9.0956\n",
      "Epoch 134/200, Train Loss: 8.3164, Val Loss: 9.0918\n",
      "Epoch 135/200, Train Loss: 8.3157, Val Loss: 9.0953\n",
      "Epoch 136/200, Train Loss: 8.3370, Val Loss: 9.0684\n",
      "Epoch 137/200, Train Loss: 8.3582, Val Loss: 9.0826\n",
      "Epoch 138/200, Train Loss: 8.3353, Val Loss: 9.0478\n",
      "Epoch 139/200, Train Loss: 8.3256, Val Loss: 9.0744\n",
      "Epoch 140/200, Train Loss: 8.3495, Val Loss: 9.1627\n",
      "Epoch 141/200, Train Loss: 8.3726, Val Loss: 9.0822\n",
      "Epoch 142/200, Train Loss: 8.3136, Val Loss: 9.0083\n",
      "Epoch 143/200, Train Loss: 8.2836, Val Loss: 9.0199\n",
      "Epoch 144/200, Train Loss: 8.2489, Val Loss: 9.0292\n",
      "Epoch 145/200, Train Loss: 8.2556, Val Loss: 9.0734\n",
      "Epoch 146/200, Train Loss: 8.2348, Val Loss: 9.0620\n",
      "Epoch 147/200, Train Loss: 8.2237, Val Loss: 9.0809\n",
      "Epoch 148/200, Train Loss: 8.2258, Val Loss: 9.0711\n",
      "Epoch 149/200, Train Loss: 8.2141, Val Loss: 9.0679\n",
      "Epoch 150/200, Train Loss: 8.2081, Val Loss: 9.0728\n",
      "Epoch 151/200, Train Loss: 8.2015, Val Loss: 9.0968\n",
      "Epoch 152/200, Train Loss: 8.2014, Val Loss: 9.0816\n",
      "Epoch 153/200, Train Loss: 8.2167, Val Loss: 9.0712\n",
      "Epoch 154/200, Train Loss: 8.2279, Val Loss: 9.1128\n",
      "Epoch 155/200, Train Loss: 8.2198, Val Loss: 9.1138\n",
      "Epoch 156/200, Train Loss: 8.2040, Val Loss: 9.0917\n",
      "Epoch 157/200, Train Loss: 8.2104, Val Loss: 9.1148\n",
      "Epoch 158/200, Train Loss: 8.2100, Val Loss: 9.1073\n",
      "Epoch 159/200, Train Loss: 8.2170, Val Loss: 9.0509\n",
      "Epoch 160/200, Train Loss: 8.2117, Val Loss: 9.1273\n",
      "Epoch 161/200, Train Loss: 8.2078, Val Loss: 9.1535\n",
      "Epoch 162/200, Train Loss: 8.2135, Val Loss: 9.0858\n",
      "Epoch 163/200, Train Loss: 8.2005, Val Loss: 9.1063\n",
      "Epoch 164/200, Train Loss: 8.1952, Val Loss: 9.0883\n",
      "Epoch 165/200, Train Loss: 8.2002, Val Loss: 9.1229\n",
      "Epoch 166/200, Train Loss: 8.1974, Val Loss: 9.0905\n",
      "Epoch 167/200, Train Loss: 8.1824, Val Loss: 9.1099\n",
      "Epoch 168/200, Train Loss: 8.1695, Val Loss: 9.0343\n",
      "Epoch 169/200, Train Loss: 8.1768, Val Loss: 9.1829\n",
      "Epoch 170/200, Train Loss: 8.1869, Val Loss: 9.0985\n",
      "Epoch 171/200, Train Loss: 8.1758, Val Loss: 9.0561\n",
      "Epoch 172/200, Train Loss: 8.1601, Val Loss: 9.1515\n",
      "Epoch 173/200, Train Loss: 8.1659, Val Loss: 9.1220\n",
      "Epoch 174/200, Train Loss: 8.1788, Val Loss: 9.1004\n",
      "Epoch 175/200, Train Loss: 8.1885, Val Loss: 9.1596\n",
      "Epoch 176/200, Train Loss: 8.1785, Val Loss: 9.1324\n",
      "Epoch 177/200, Train Loss: 8.1675, Val Loss: 9.0792\n",
      "Epoch 178/200, Train Loss: 8.1651, Val Loss: 9.1341\n",
      "Epoch 179/200, Train Loss: 8.1454, Val Loss: 9.0733\n",
      "Epoch 180/200, Train Loss: 8.1416, Val Loss: 9.1476\n",
      "Epoch 181/200, Train Loss: 8.1329, Val Loss: 9.1179\n",
      "Epoch 182/200, Train Loss: 8.1204, Val Loss: 9.1081\n",
      "Epoch 183/200, Train Loss: 8.1348, Val Loss: 9.0975\n",
      "Epoch 184/200, Train Loss: 8.1277, Val Loss: 9.1529\n",
      "Epoch 185/200, Train Loss: 8.1247, Val Loss: 9.1389\n",
      "Epoch 186/200, Train Loss: 8.1119, Val Loss: 9.1267\n",
      "Epoch 187/200, Train Loss: 8.1228, Val Loss: 9.1449\n",
      "Epoch 188/200, Train Loss: 8.1254, Val Loss: 9.1947\n",
      "Epoch 189/200, Train Loss: 8.1210, Val Loss: 9.1495\n",
      "Epoch 190/200, Train Loss: 8.1095, Val Loss: 9.0989\n",
      "Epoch 191/200, Train Loss: 8.0959, Val Loss: 9.1657\n",
      "Epoch 192/200, Train Loss: 8.1059, Val Loss: 9.0855\n",
      "Epoch 193/200, Train Loss: 8.1051, Val Loss: 9.1560\n",
      "Epoch 194/200, Train Loss: 8.0858, Val Loss: 9.1411\n",
      "Epoch 195/200, Train Loss: 8.0725, Val Loss: 9.1864\n",
      "Epoch 196/200, Train Loss: 8.0647, Val Loss: 9.1573\n",
      "Epoch 197/200, Train Loss: 8.0646, Val Loss: 9.1469\n",
      "Epoch 198/200, Train Loss: 8.0653, Val Loss: 9.1870\n",
      "Epoch 199/200, Train Loss: 8.0788, Val Loss: 9.1749\n",
      "Epoch 200/200, Train Loss: 8.0804, Val Loss: 9.1691\n",
      "\n",
      "Loaded best model (Val Loss: 8.8542) for final hidden state extraction.\n",
      "Saved best model for ComplexOscillatorNet to results/20250508_090642/ComplexOscillatorNet_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for ComplexOscillatorNet ---\n",
      "  Analyzing decodability for ComplexOscillatorNet...\n",
      "  Hidden states shape: torch.Size([160, 200, 64])\n",
      "  Coefficients shape: torch.Size([160, 10])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for ComplexOscillatorNet - Test MSE: 0.9731, Test R2: 0.4856 (best alpha: 48.3293)\n",
      "Decodability (R2 score) for ComplexOscillatorNet: 0.4856\n",
      "\n",
      "--- Training RNN_GRU ---\n",
      "Number of parameters: 50561\n",
      "Epoch 1/200, Train Loss: 11.5692, Val Loss: 10.5647\n",
      "  New best validation loss: 10.5647\n",
      "Epoch 2/200, Train Loss: 11.5412, Val Loss: 10.5350\n",
      "  New best validation loss: 10.5350\n",
      "Epoch 3/200, Train Loss: 11.5245, Val Loss: 10.5287\n",
      "  New best validation loss: 10.5287\n",
      "Epoch 4/200, Train Loss: 11.5168, Val Loss: 10.5208\n",
      "  New best validation loss: 10.5208\n",
      "Epoch 5/200, Train Loss: 11.5068, Val Loss: 10.5127\n",
      "  New best validation loss: 10.5127\n",
      "Epoch 6/200, Train Loss: 11.5263, Val Loss: 10.5105\n",
      "  New best validation loss: 10.5105\n",
      "Epoch 7/200, Train Loss: 11.5025, Val Loss: 10.5197\n",
      "Epoch 8/200, Train Loss: 11.4876, Val Loss: 10.5057\n",
      "  New best validation loss: 10.5057\n",
      "Epoch 9/200, Train Loss: 11.4961, Val Loss: 10.5015\n",
      "  New best validation loss: 10.5015\n",
      "Epoch 10/200, Train Loss: 11.4949, Val Loss: 10.4985\n",
      "  New best validation loss: 10.4985\n",
      "Epoch 11/200, Train Loss: 11.4966, Val Loss: 10.4926\n",
      "  New best validation loss: 10.4926\n",
      "Epoch 12/200, Train Loss: 11.4909, Val Loss: 10.4915\n",
      "  New best validation loss: 10.4915\n",
      "Epoch 13/200, Train Loss: 11.4805, Val Loss: 10.4868\n",
      "  New best validation loss: 10.4868\n",
      "Epoch 14/200, Train Loss: 11.4795, Val Loss: 10.4874\n",
      "Epoch 15/200, Train Loss: 11.4767, Val Loss: 10.4849\n",
      "  New best validation loss: 10.4849\n",
      "Epoch 16/200, Train Loss: 11.4786, Val Loss: 10.4823\n",
      "  New best validation loss: 10.4823\n",
      "Epoch 17/200, Train Loss: 11.4857, Val Loss: 10.4800\n",
      "  New best validation loss: 10.4800\n",
      "Epoch 18/200, Train Loss: 11.4703, Val Loss: 10.4777\n",
      "  New best validation loss: 10.4777\n",
      "Epoch 19/200, Train Loss: 11.4656, Val Loss: 10.4753\n",
      "  New best validation loss: 10.4753\n",
      "Epoch 20/200, Train Loss: 11.4584, Val Loss: 10.4659\n",
      "  New best validation loss: 10.4659\n",
      "Epoch 21/200, Train Loss: 11.4364, Val Loss: 10.4570\n",
      "  New best validation loss: 10.4570\n",
      "Epoch 22/200, Train Loss: 11.4097, Val Loss: 10.4817\n",
      "Epoch 23/200, Train Loss: 11.4101, Val Loss: 10.4398\n",
      "  New best validation loss: 10.4398\n",
      "Epoch 24/200, Train Loss: 11.3590, Val Loss: 10.4013\n",
      "  New best validation loss: 10.4013\n",
      "Epoch 25/200, Train Loss: 11.3246, Val Loss: 10.3898\n",
      "  New best validation loss: 10.3898\n",
      "Epoch 26/200, Train Loss: 11.3023, Val Loss: 10.3549\n",
      "  New best validation loss: 10.3549\n",
      "Epoch 27/200, Train Loss: 11.2578, Val Loss: 10.3449\n",
      "  New best validation loss: 10.3449\n",
      "Epoch 28/200, Train Loss: 11.1713, Val Loss: 10.1739\n",
      "  New best validation loss: 10.1739\n",
      "Epoch 29/200, Train Loss: 11.1214, Val Loss: 10.0925\n",
      "  New best validation loss: 10.0925\n",
      "Epoch 30/200, Train Loss: 11.0532, Val Loss: 10.3095\n",
      "Epoch 31/200, Train Loss: 10.9908, Val Loss: 10.0332\n",
      "  New best validation loss: 10.0332\n",
      "Epoch 32/200, Train Loss: 11.1073, Val Loss: 10.1967\n",
      "Epoch 33/200, Train Loss: 11.0806, Val Loss: 10.2204\n",
      "Epoch 34/200, Train Loss: 11.0542, Val Loss: 10.0784\n",
      "Epoch 35/200, Train Loss: 10.9815, Val Loss: 10.1629\n",
      "Epoch 36/200, Train Loss: 10.9726, Val Loss: 9.9881\n",
      "  New best validation loss: 9.9881\n",
      "Epoch 37/200, Train Loss: 10.8992, Val Loss: 10.0066\n",
      "Epoch 38/200, Train Loss: 10.9878, Val Loss: 10.3698\n",
      "Epoch 39/200, Train Loss: 11.1475, Val Loss: 10.1537\n",
      "Epoch 40/200, Train Loss: 11.0500, Val Loss: 10.1540\n",
      "Epoch 41/200, Train Loss: 10.9372, Val Loss: 10.0794\n",
      "Epoch 42/200, Train Loss: 10.8341, Val Loss: 10.0333\n",
      "Epoch 43/200, Train Loss: 10.6943, Val Loss: 10.0403\n",
      "Epoch 44/200, Train Loss: 10.7830, Val Loss: 10.1007\n",
      "Epoch 45/200, Train Loss: 10.7826, Val Loss: 9.9824\n",
      "  New best validation loss: 9.9824\n",
      "Epoch 46/200, Train Loss: 10.7397, Val Loss: 9.9582\n",
      "  New best validation loss: 9.9582\n",
      "Epoch 47/200, Train Loss: 10.6898, Val Loss: 9.9580\n",
      "  New best validation loss: 9.9580\n",
      "Epoch 48/200, Train Loss: 10.6098, Val Loss: 9.9361\n",
      "  New best validation loss: 9.9361\n",
      "Epoch 49/200, Train Loss: 10.5986, Val Loss: 9.7983\n",
      "  New best validation loss: 9.7983\n",
      "Epoch 50/200, Train Loss: 10.4724, Val Loss: 9.6938\n",
      "  New best validation loss: 9.6938\n",
      "Epoch 51/200, Train Loss: 10.6577, Val Loss: 9.7996\n",
      "Epoch 52/200, Train Loss: 10.5256, Val Loss: 9.6530\n",
      "  New best validation loss: 9.6530\n",
      "Epoch 53/200, Train Loss: 10.3233, Val Loss: 9.7651\n",
      "Epoch 54/200, Train Loss: 10.3736, Val Loss: 9.5527\n",
      "  New best validation loss: 9.5527\n",
      "Epoch 55/200, Train Loss: 10.2648, Val Loss: 9.4084\n",
      "  New best validation loss: 9.4084\n",
      "Epoch 56/200, Train Loss: 10.2366, Val Loss: 9.2950\n",
      "  New best validation loss: 9.2950\n",
      "Epoch 57/200, Train Loss: 10.4974, Val Loss: 9.6260\n",
      "Epoch 58/200, Train Loss: 10.5606, Val Loss: 9.7635\n",
      "Epoch 59/200, Train Loss: 10.3461, Val Loss: 9.4320\n",
      "Epoch 60/200, Train Loss: 10.0416, Val Loss: 9.4518\n",
      "Epoch 61/200, Train Loss: 10.0348, Val Loss: 9.4375\n",
      "Epoch 62/200, Train Loss: 10.1663, Val Loss: 9.1306\n",
      "  New best validation loss: 9.1306\n",
      "Epoch 63/200, Train Loss: 9.8629, Val Loss: 8.9311\n",
      "  New best validation loss: 8.9311\n",
      "Epoch 64/200, Train Loss: 10.1468, Val Loss: 9.3283\n",
      "Epoch 65/200, Train Loss: 9.8138, Val Loss: 8.7761\n",
      "  New best validation loss: 8.7761\n",
      "Epoch 66/200, Train Loss: 9.6420, Val Loss: 8.7309\n",
      "  New best validation loss: 8.7309\n",
      "Epoch 67/200, Train Loss: 9.7858, Val Loss: 8.9545\n",
      "Epoch 68/200, Train Loss: 9.7585, Val Loss: 8.8228\n",
      "Epoch 69/200, Train Loss: 9.8961, Val Loss: 9.1880\n",
      "Epoch 70/200, Train Loss: 9.6484, Val Loss: 8.6431\n",
      "  New best validation loss: 8.6431\n",
      "Epoch 71/200, Train Loss: 9.8469, Val Loss: 8.8127\n",
      "Epoch 72/200, Train Loss: 9.6268, Val Loss: 8.4443\n",
      "  New best validation loss: 8.4443\n",
      "Epoch 73/200, Train Loss: 9.2455, Val Loss: 8.4480\n",
      "Epoch 74/200, Train Loss: 9.3144, Val Loss: 8.3213\n",
      "  New best validation loss: 8.3213\n",
      "Epoch 75/200, Train Loss: 9.1859, Val Loss: 8.4351\n",
      "Epoch 76/200, Train Loss: 9.2881, Val Loss: 8.5232\n",
      "Epoch 77/200, Train Loss: 9.3493, Val Loss: 8.7076\n",
      "Epoch 78/200, Train Loss: 9.0667, Val Loss: 8.1359\n",
      "  New best validation loss: 8.1359\n",
      "Epoch 79/200, Train Loss: 8.8403, Val Loss: 8.3420\n",
      "Epoch 80/200, Train Loss: 9.1256, Val Loss: 8.4214\n",
      "Epoch 81/200, Train Loss: 9.0411, Val Loss: 8.1908\n",
      "Epoch 82/200, Train Loss: 9.0899, Val Loss: 8.4544\n",
      "Epoch 83/200, Train Loss: 9.1903, Val Loss: 8.4194\n",
      "Epoch 84/200, Train Loss: 8.9950, Val Loss: 8.1805\n",
      "Epoch 85/200, Train Loss: 8.7532, Val Loss: 8.1554\n",
      "Epoch 86/200, Train Loss: 8.7673, Val Loss: 8.2108\n",
      "Epoch 87/200, Train Loss: 8.7114, Val Loss: 8.1677\n",
      "Epoch 88/200, Train Loss: 8.9261, Val Loss: 8.0528\n",
      "  New best validation loss: 8.0528\n",
      "Epoch 89/200, Train Loss: 8.7336, Val Loss: 8.1020\n",
      "Epoch 90/200, Train Loss: 8.5872, Val Loss: 8.0291\n",
      "  New best validation loss: 8.0291\n",
      "Epoch 91/200, Train Loss: 8.6115, Val Loss: 8.0438\n",
      "Epoch 92/200, Train Loss: 8.7764, Val Loss: 8.4021\n",
      "Epoch 93/200, Train Loss: 8.6163, Val Loss: 8.2764\n",
      "Epoch 94/200, Train Loss: 8.7506, Val Loss: 8.0129\n",
      "  New best validation loss: 8.0129\n",
      "Epoch 95/200, Train Loss: 8.5692, Val Loss: 8.0256\n",
      "Epoch 96/200, Train Loss: 8.5313, Val Loss: 8.0422\n",
      "Epoch 97/200, Train Loss: 8.5057, Val Loss: 7.9403\n",
      "  New best validation loss: 7.9403\n",
      "Epoch 98/200, Train Loss: 8.3872, Val Loss: 8.0597\n",
      "Epoch 99/200, Train Loss: 8.2443, Val Loss: 7.8197\n",
      "  New best validation loss: 7.8197\n",
      "Epoch 100/200, Train Loss: 8.2079, Val Loss: 8.0797\n",
      "Epoch 101/200, Train Loss: 8.3605, Val Loss: 8.1298\n",
      "Epoch 102/200, Train Loss: 8.6687, Val Loss: 8.1898\n",
      "Epoch 103/200, Train Loss: 8.7301, Val Loss: 8.2295\n",
      "Epoch 104/200, Train Loss: 8.5481, Val Loss: 8.2137\n",
      "Epoch 105/200, Train Loss: 8.1558, Val Loss: 7.8410\n",
      "Epoch 106/200, Train Loss: 8.0875, Val Loss: 7.9108\n",
      "Epoch 107/200, Train Loss: 8.2440, Val Loss: 8.2683\n",
      "Epoch 108/200, Train Loss: 8.5144, Val Loss: 8.2382\n",
      "Epoch 109/200, Train Loss: 8.3749, Val Loss: 7.8117\n",
      "  New best validation loss: 7.8117\n",
      "Epoch 110/200, Train Loss: 8.1959, Val Loss: 7.7420\n",
      "  New best validation loss: 7.7420\n",
      "Epoch 111/200, Train Loss: 8.1002, Val Loss: 8.2166\n",
      "Epoch 112/200, Train Loss: 7.9767, Val Loss: 8.0469\n",
      "Epoch 113/200, Train Loss: 8.0037, Val Loss: 7.8631\n",
      "Epoch 114/200, Train Loss: 7.9721, Val Loss: 7.7534\n",
      "Epoch 115/200, Train Loss: 7.8991, Val Loss: 7.8206\n",
      "Epoch 116/200, Train Loss: 7.7615, Val Loss: 7.8424\n",
      "Epoch 117/200, Train Loss: 7.8221, Val Loss: 7.8121\n",
      "Epoch 118/200, Train Loss: 7.8798, Val Loss: 8.1357\n",
      "Epoch 119/200, Train Loss: 7.7805, Val Loss: 7.6584\n",
      "  New best validation loss: 7.6584\n",
      "Epoch 120/200, Train Loss: 7.6692, Val Loss: 8.4156\n",
      "Epoch 121/200, Train Loss: 7.6624, Val Loss: 7.7189\n",
      "Epoch 122/200, Train Loss: 7.5190, Val Loss: 8.0623\n",
      "Epoch 123/200, Train Loss: 7.5096, Val Loss: 7.9886\n",
      "Epoch 124/200, Train Loss: 7.6907, Val Loss: 7.8978\n",
      "Epoch 125/200, Train Loss: 7.7120, Val Loss: 8.3844\n",
      "Epoch 126/200, Train Loss: 7.6883, Val Loss: 7.7756\n",
      "Epoch 127/200, Train Loss: 7.5862, Val Loss: 7.9986\n",
      "Epoch 128/200, Train Loss: 7.5609, Val Loss: 7.8133\n",
      "Epoch 129/200, Train Loss: 7.4095, Val Loss: 7.9967\n",
      "Epoch 130/200, Train Loss: 7.5036, Val Loss: 7.9803\n",
      "Epoch 131/200, Train Loss: 7.4157, Val Loss: 7.9135\n",
      "Epoch 132/200, Train Loss: 7.4106, Val Loss: 7.8376\n",
      "Epoch 133/200, Train Loss: 7.4251, Val Loss: 7.8872\n",
      "Epoch 134/200, Train Loss: 7.4503, Val Loss: 7.7584\n",
      "Epoch 135/200, Train Loss: 7.3559, Val Loss: 7.6020\n",
      "  New best validation loss: 7.6020\n",
      "Epoch 136/200, Train Loss: 7.5638, Val Loss: 7.7749\n",
      "Epoch 137/200, Train Loss: 7.4668, Val Loss: 7.7457\n",
      "Epoch 138/200, Train Loss: 7.4752, Val Loss: 7.8670\n",
      "Epoch 139/200, Train Loss: 7.4764, Val Loss: 7.8224\n",
      "Epoch 140/200, Train Loss: 7.5344, Val Loss: 8.0605\n",
      "Epoch 141/200, Train Loss: 7.6123, Val Loss: 7.8832\n",
      "Epoch 142/200, Train Loss: 7.3562, Val Loss: 7.6540\n",
      "Epoch 143/200, Train Loss: 7.2734, Val Loss: 7.9164\n",
      "Epoch 144/200, Train Loss: 7.1895, Val Loss: 7.8124\n",
      "Epoch 145/200, Train Loss: 7.2639, Val Loss: 7.6683\n",
      "Epoch 146/200, Train Loss: 7.2012, Val Loss: 7.8537\n",
      "Epoch 147/200, Train Loss: 7.0929, Val Loss: 8.1088\n",
      "Epoch 148/200, Train Loss: 7.2753, Val Loss: 7.7981\n",
      "Epoch 149/200, Train Loss: 7.1747, Val Loss: 7.6860\n",
      "Epoch 150/200, Train Loss: 7.1718, Val Loss: 7.7806\n",
      "Epoch 151/200, Train Loss: 7.1170, Val Loss: 7.9415\n",
      "Epoch 152/200, Train Loss: 7.0810, Val Loss: 7.7341\n",
      "Epoch 153/200, Train Loss: 6.9586, Val Loss: 7.7863\n",
      "Epoch 154/200, Train Loss: 6.9883, Val Loss: 7.7136\n",
      "Epoch 155/200, Train Loss: 7.1046, Val Loss: 7.7201\n",
      "Epoch 156/200, Train Loss: 7.1317, Val Loss: 7.8406\n",
      "Epoch 157/200, Train Loss: 7.0470, Val Loss: 7.8227\n",
      "Epoch 158/200, Train Loss: 6.9241, Val Loss: 7.8897\n",
      "Epoch 159/200, Train Loss: 7.1431, Val Loss: 7.7435\n",
      "Epoch 160/200, Train Loss: 6.9569, Val Loss: 7.7202\n",
      "Epoch 161/200, Train Loss: 6.9053, Val Loss: 7.8364\n",
      "Epoch 162/200, Train Loss: 6.8479, Val Loss: 7.7479\n",
      "Epoch 163/200, Train Loss: 6.8373, Val Loss: 7.7997\n",
      "Epoch 164/200, Train Loss: 6.6944, Val Loss: 7.7897\n",
      "Epoch 165/200, Train Loss: 6.7020, Val Loss: 7.7869\n",
      "Epoch 166/200, Train Loss: 6.7411, Val Loss: 8.2516\n",
      "Epoch 167/200, Train Loss: 6.8637, Val Loss: 7.7576\n",
      "Epoch 168/200, Train Loss: 7.0247, Val Loss: 7.6972\n",
      "Epoch 169/200, Train Loss: 6.8508, Val Loss: 8.3421\n",
      "Epoch 170/200, Train Loss: 6.9448, Val Loss: 8.0150\n",
      "Epoch 171/200, Train Loss: 6.7488, Val Loss: 7.8250\n",
      "Epoch 172/200, Train Loss: 6.5857, Val Loss: 7.7692\n",
      "Epoch 173/200, Train Loss: 6.5183, Val Loss: 8.0034\n",
      "Epoch 174/200, Train Loss: 6.5638, Val Loss: 8.0202\n",
      "Epoch 175/200, Train Loss: 6.7267, Val Loss: 8.0485\n",
      "Epoch 176/200, Train Loss: 6.6283, Val Loss: 7.8895\n",
      "Epoch 177/200, Train Loss: 6.4680, Val Loss: 8.0940\n",
      "Epoch 178/200, Train Loss: 6.5075, Val Loss: 7.9155\n",
      "Epoch 179/200, Train Loss: 6.5702, Val Loss: 8.0402\n",
      "Epoch 180/200, Train Loss: 6.4805, Val Loss: 7.8378\n",
      "Epoch 181/200, Train Loss: 6.4084, Val Loss: 7.9877\n",
      "Epoch 182/200, Train Loss: 6.4842, Val Loss: 8.2113\n",
      "Epoch 183/200, Train Loss: 6.7197, Val Loss: 7.8844\n",
      "Epoch 184/200, Train Loss: 6.5406, Val Loss: 7.8511\n",
      "Epoch 185/200, Train Loss: 6.3364, Val Loss: 7.7809\n",
      "Epoch 186/200, Train Loss: 6.2600, Val Loss: 7.9763\n",
      "Epoch 187/200, Train Loss: 6.2290, Val Loss: 7.8744\n",
      "Epoch 188/200, Train Loss: 6.1252, Val Loss: 8.1049\n",
      "Epoch 189/200, Train Loss: 6.1539, Val Loss: 8.0403\n",
      "Epoch 190/200, Train Loss: 6.1405, Val Loss: 7.9058\n",
      "Epoch 191/200, Train Loss: 6.1293, Val Loss: 7.8271\n",
      "Epoch 192/200, Train Loss: 6.1178, Val Loss: 8.0769\n",
      "Epoch 193/200, Train Loss: 6.0986, Val Loss: 7.9235\n",
      "Epoch 194/200, Train Loss: 5.9763, Val Loss: 8.0707\n",
      "Epoch 195/200, Train Loss: 6.0165, Val Loss: 8.0119\n",
      "Epoch 196/200, Train Loss: 6.0906, Val Loss: 7.8431\n",
      "Epoch 197/200, Train Loss: 6.2554, Val Loss: 8.6053\n",
      "Epoch 198/200, Train Loss: 6.3329, Val Loss: 8.0610\n",
      "Epoch 199/200, Train Loss: 6.2418, Val Loss: 8.0156\n",
      "Epoch 200/200, Train Loss: 6.1169, Val Loss: 7.7581\n",
      "\n",
      "Loaded best model (Val Loss: 7.6020) for final hidden state extraction.\n",
      "Saved best model for RNN_GRU to results/20250508_090642/RNN_GRU_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for RNN_GRU ---\n",
      "  Analyzing decodability for RNN_GRU...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 10])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for RNN_GRU - Test MSE: 1.2774, Test R2: 0.3344 (best alpha: 23.3572)\n",
      "Decodability (R2 score) for RNN_GRU: 0.3344\n",
      "\n",
      "--- Training Transformer ---\n",
      "Number of parameters: 281345\n",
      "Epoch 1/200, Train Loss: 12.9897, Val Loss: 10.9119\n",
      "  New best validation loss: 10.9119\n",
      "Epoch 2/200, Train Loss: 11.9179, Val Loss: 10.6967\n",
      "  New best validation loss: 10.6967\n",
      "Epoch 3/200, Train Loss: 11.6769, Val Loss: 10.5601\n",
      "  New best validation loss: 10.5601\n",
      "Epoch 4/200, Train Loss: 11.6080, Val Loss: 10.5436\n",
      "  New best validation loss: 10.5436\n",
      "Epoch 5/200, Train Loss: 11.6193, Val Loss: 10.5224\n",
      "  New best validation loss: 10.5224\n",
      "Epoch 6/200, Train Loss: 11.6058, Val Loss: 10.5285\n",
      "Epoch 7/200, Train Loss: 11.6242, Val Loss: 10.5176\n",
      "  New best validation loss: 10.5176\n",
      "Epoch 8/200, Train Loss: 11.6093, Val Loss: 10.5414\n",
      "Epoch 9/200, Train Loss: 11.6356, Val Loss: 10.6021\n",
      "Epoch 10/200, Train Loss: 11.6419, Val Loss: 10.5319\n",
      "Epoch 11/200, Train Loss: 11.5674, Val Loss: 10.5056\n",
      "  New best validation loss: 10.5056\n",
      "Epoch 12/200, Train Loss: 11.5701, Val Loss: 10.4966\n",
      "  New best validation loss: 10.4966\n",
      "Epoch 13/200, Train Loss: 11.5711, Val Loss: 10.5735\n",
      "Epoch 14/200, Train Loss: 11.5966, Val Loss: 10.5655\n",
      "Epoch 15/200, Train Loss: 11.6165, Val Loss: 10.5171\n",
      "Epoch 16/200, Train Loss: 11.5949, Val Loss: 10.5408\n",
      "Epoch 17/200, Train Loss: 11.5743, Val Loss: 10.5289\n",
      "Epoch 18/200, Train Loss: 11.5519, Val Loss: 10.4847\n",
      "  New best validation loss: 10.4847\n",
      "Epoch 19/200, Train Loss: 11.5123, Val Loss: 10.4786\n",
      "  New best validation loss: 10.4786\n",
      "Epoch 20/200, Train Loss: 11.5350, Val Loss: 10.4715\n",
      "  New best validation loss: 10.4715\n",
      "Epoch 21/200, Train Loss: 11.5151, Val Loss: 10.5068\n",
      "Epoch 22/200, Train Loss: 11.5277, Val Loss: 10.6253\n",
      "Epoch 23/200, Train Loss: 11.5251, Val Loss: 10.4705\n",
      "  New best validation loss: 10.4705\n",
      "Epoch 24/200, Train Loss: 11.5065, Val Loss: 10.4869\n",
      "Epoch 25/200, Train Loss: 11.5860, Val Loss: 10.4638\n",
      "  New best validation loss: 10.4638\n",
      "Epoch 26/200, Train Loss: 11.5264, Val Loss: 10.4513\n",
      "  New best validation loss: 10.4513\n",
      "Epoch 27/200, Train Loss: 11.4935, Val Loss: 10.4674\n",
      "Epoch 28/200, Train Loss: 11.4949, Val Loss: 10.4958\n",
      "Epoch 29/200, Train Loss: 11.5123, Val Loss: 10.4869\n",
      "Epoch 30/200, Train Loss: 11.5165, Val Loss: 10.5327\n",
      "Epoch 31/200, Train Loss: 11.4977, Val Loss: 10.5664\n",
      "Epoch 32/200, Train Loss: 11.5520, Val Loss: 10.5332\n",
      "Epoch 33/200, Train Loss: 11.5172, Val Loss: 10.4912\n",
      "Epoch 34/200, Train Loss: 11.4658, Val Loss: 10.4897\n",
      "Epoch 35/200, Train Loss: 11.4851, Val Loss: 10.4618\n",
      "Epoch 36/200, Train Loss: 11.4453, Val Loss: 10.4999\n",
      "Epoch 37/200, Train Loss: 11.5078, Val Loss: 10.4454\n",
      "  New best validation loss: 10.4454\n",
      "Epoch 38/200, Train Loss: 11.4804, Val Loss: 10.4823\n",
      "Epoch 39/200, Train Loss: 11.4428, Val Loss: 10.4985\n",
      "Epoch 40/200, Train Loss: 11.4442, Val Loss: 10.4830\n",
      "Epoch 41/200, Train Loss: 11.4614, Val Loss: 10.6337\n",
      "Epoch 42/200, Train Loss: 11.5310, Val Loss: 10.5411\n",
      "Epoch 43/200, Train Loss: 11.4872, Val Loss: 10.4687\n",
      "Epoch 44/200, Train Loss: 11.4489, Val Loss: 10.5060\n",
      "Epoch 45/200, Train Loss: 11.5094, Val Loss: 10.5461\n",
      "Epoch 46/200, Train Loss: 11.4379, Val Loss: 10.5288\n",
      "Epoch 47/200, Train Loss: 11.4622, Val Loss: 10.4999\n",
      "Epoch 48/200, Train Loss: 11.4188, Val Loss: 10.4652\n",
      "Epoch 49/200, Train Loss: 11.4483, Val Loss: 10.4340\n",
      "  New best validation loss: 10.4340\n",
      "Epoch 50/200, Train Loss: 11.4404, Val Loss: 10.4714\n",
      "Epoch 51/200, Train Loss: 11.4195, Val Loss: 10.4993\n",
      "Epoch 52/200, Train Loss: 11.4101, Val Loss: 10.4608\n",
      "Epoch 53/200, Train Loss: 11.4062, Val Loss: 10.5087\n",
      "Epoch 54/200, Train Loss: 11.4478, Val Loss: 10.4138\n",
      "  New best validation loss: 10.4138\n",
      "Epoch 55/200, Train Loss: 11.4350, Val Loss: 10.4595\n",
      "Epoch 56/200, Train Loss: 11.4287, Val Loss: 10.5538\n",
      "Epoch 57/200, Train Loss: 11.4121, Val Loss: 10.4708\n",
      "Epoch 58/200, Train Loss: 11.4371, Val Loss: 10.4816\n",
      "Epoch 59/200, Train Loss: 11.3802, Val Loss: 10.4880\n",
      "Epoch 60/200, Train Loss: 11.4316, Val Loss: 10.4497\n",
      "Epoch 61/200, Train Loss: 11.4170, Val Loss: 10.5349\n",
      "Epoch 62/200, Train Loss: 11.4198, Val Loss: 10.4738\n",
      "Epoch 63/200, Train Loss: 11.4225, Val Loss: 10.4150\n",
      "Epoch 64/200, Train Loss: 11.3910, Val Loss: 10.4579\n",
      "Epoch 65/200, Train Loss: 11.4073, Val Loss: 10.3879\n",
      "  New best validation loss: 10.3879\n",
      "Epoch 66/200, Train Loss: 11.4212, Val Loss: 10.4929\n",
      "Epoch 67/200, Train Loss: 11.3890, Val Loss: 10.4571\n",
      "Epoch 68/200, Train Loss: 11.3679, Val Loss: 10.3703\n",
      "  New best validation loss: 10.3703\n",
      "Epoch 69/200, Train Loss: 11.3364, Val Loss: 10.4405\n",
      "Epoch 70/200, Train Loss: 11.3476, Val Loss: 10.4951\n",
      "Epoch 71/200, Train Loss: 11.3669, Val Loss: 10.4657\n",
      "Epoch 72/200, Train Loss: 11.3824, Val Loss: 10.4526\n",
      "Epoch 73/200, Train Loss: 11.3269, Val Loss: 10.3568\n",
      "  New best validation loss: 10.3568\n",
      "Epoch 74/200, Train Loss: 11.3349, Val Loss: 10.3435\n",
      "  New best validation loss: 10.3435\n",
      "Epoch 75/200, Train Loss: 11.3089, Val Loss: 10.4434\n",
      "Epoch 76/200, Train Loss: 11.3557, Val Loss: 10.4008\n",
      "Epoch 77/200, Train Loss: 11.3035, Val Loss: 10.3583\n",
      "Epoch 78/200, Train Loss: 11.3078, Val Loss: 10.2953\n",
      "  New best validation loss: 10.2953\n",
      "Epoch 79/200, Train Loss: 11.3415, Val Loss: 10.4441\n",
      "Epoch 80/200, Train Loss: 11.3398, Val Loss: 10.3637\n",
      "Epoch 81/200, Train Loss: 11.3526, Val Loss: 10.4276\n",
      "Epoch 82/200, Train Loss: 11.2775, Val Loss: 10.4870\n",
      "Epoch 83/200, Train Loss: 11.3012, Val Loss: 10.2047\n",
      "  New best validation loss: 10.2047\n",
      "Epoch 84/200, Train Loss: 11.1927, Val Loss: 10.2734\n",
      "Epoch 85/200, Train Loss: 11.1680, Val Loss: 10.2071\n",
      "Epoch 86/200, Train Loss: 11.1628, Val Loss: 10.3674\n",
      "Epoch 87/200, Train Loss: 11.1412, Val Loss: 10.1616\n",
      "  New best validation loss: 10.1616\n",
      "Epoch 88/200, Train Loss: 11.1138, Val Loss: 10.1164\n",
      "  New best validation loss: 10.1164\n",
      "Epoch 89/200, Train Loss: 11.1176, Val Loss: 10.2565\n",
      "Epoch 90/200, Train Loss: 10.9776, Val Loss: 9.9015\n",
      "  New best validation loss: 9.9015\n",
      "Epoch 91/200, Train Loss: 10.8394, Val Loss: 9.8015\n",
      "  New best validation loss: 9.8015\n",
      "Epoch 92/200, Train Loss: 10.7135, Val Loss: 9.7550\n",
      "  New best validation loss: 9.7550\n",
      "Epoch 93/200, Train Loss: 10.7223, Val Loss: 9.7375\n",
      "  New best validation loss: 9.7375\n",
      "Epoch 94/200, Train Loss: 10.9442, Val Loss: 9.6472\n",
      "  New best validation loss: 9.6472\n",
      "Epoch 95/200, Train Loss: 10.6520, Val Loss: 9.4878\n",
      "  New best validation loss: 9.4878\n",
      "Epoch 96/200, Train Loss: 10.4875, Val Loss: 9.5844\n",
      "Epoch 97/200, Train Loss: 10.4201, Val Loss: 9.5507\n",
      "Epoch 98/200, Train Loss: 10.5396, Val Loss: 9.6368\n",
      "Epoch 99/200, Train Loss: 10.1516, Val Loss: 9.0461\n",
      "  New best validation loss: 9.0461\n",
      "Epoch 100/200, Train Loss: 9.8617, Val Loss: 8.8284\n",
      "  New best validation loss: 8.8284\n",
      "Epoch 101/200, Train Loss: 9.6801, Val Loss: 8.7069\n",
      "  New best validation loss: 8.7069\n",
      "Epoch 102/200, Train Loss: 9.5244, Val Loss: 8.6311\n",
      "  New best validation loss: 8.6311\n",
      "Epoch 103/200, Train Loss: 9.3025, Val Loss: 8.2974\n",
      "  New best validation loss: 8.2974\n",
      "Epoch 104/200, Train Loss: 9.0051, Val Loss: 7.7577\n",
      "  New best validation loss: 7.7577\n",
      "Epoch 105/200, Train Loss: 8.7114, Val Loss: 7.6350\n",
      "  New best validation loss: 7.6350\n",
      "Epoch 106/200, Train Loss: 8.1331, Val Loss: 7.0149\n",
      "  New best validation loss: 7.0149\n",
      "Epoch 107/200, Train Loss: 7.8415, Val Loss: 6.7927\n",
      "  New best validation loss: 6.7927\n",
      "Epoch 108/200, Train Loss: 7.7213, Val Loss: 6.6726\n",
      "  New best validation loss: 6.6726\n",
      "Epoch 109/200, Train Loss: 7.4090, Val Loss: 6.4156\n",
      "  New best validation loss: 6.4156\n",
      "Epoch 110/200, Train Loss: 7.0588, Val Loss: 6.0818\n",
      "  New best validation loss: 6.0818\n",
      "Epoch 111/200, Train Loss: 6.8590, Val Loss: 5.8402\n",
      "  New best validation loss: 5.8402\n",
      "Epoch 112/200, Train Loss: 6.7425, Val Loss: 5.7138\n",
      "  New best validation loss: 5.7138\n",
      "Epoch 113/200, Train Loss: 6.5292, Val Loss: 5.4626\n",
      "  New best validation loss: 5.4626\n",
      "Epoch 114/200, Train Loss: 6.2346, Val Loss: 5.3915\n",
      "  New best validation loss: 5.3915\n",
      "Epoch 115/200, Train Loss: 6.1412, Val Loss: 5.5154\n",
      "Epoch 116/200, Train Loss: 6.1811, Val Loss: 5.2106\n",
      "  New best validation loss: 5.2106\n",
      "Epoch 117/200, Train Loss: 6.0988, Val Loss: 5.5137\n",
      "Epoch 118/200, Train Loss: 5.8159, Val Loss: 5.3481\n",
      "Epoch 119/200, Train Loss: 5.5462, Val Loss: 4.8707\n",
      "  New best validation loss: 4.8707\n",
      "Epoch 120/200, Train Loss: 5.4077, Val Loss: 4.6926\n",
      "  New best validation loss: 4.6926\n",
      "Epoch 121/200, Train Loss: 5.3609, Val Loss: 4.4186\n",
      "  New best validation loss: 4.4186\n",
      "Epoch 122/200, Train Loss: 5.1555, Val Loss: 4.6415\n",
      "Epoch 123/200, Train Loss: 5.2441, Val Loss: 4.4810\n",
      "Epoch 124/200, Train Loss: 5.3304, Val Loss: 4.6226\n",
      "Epoch 125/200, Train Loss: 5.1285, Val Loss: 4.8304\n",
      "Epoch 126/200, Train Loss: 5.1163, Val Loss: 4.3173\n",
      "  New best validation loss: 4.3173\n",
      "Epoch 127/200, Train Loss: 4.8782, Val Loss: 4.2840\n",
      "  New best validation loss: 4.2840\n",
      "Epoch 128/200, Train Loss: 4.6714, Val Loss: 4.0258\n",
      "  New best validation loss: 4.0258\n",
      "Epoch 129/200, Train Loss: 4.6085, Val Loss: 3.9843\n",
      "  New best validation loss: 3.9843\n",
      "Epoch 130/200, Train Loss: 4.7143, Val Loss: 4.1039\n",
      "Epoch 131/200, Train Loss: 4.6046, Val Loss: 4.0401\n",
      "Epoch 132/200, Train Loss: 4.5023, Val Loss: 4.2255\n",
      "Epoch 133/200, Train Loss: 4.5901, Val Loss: 4.0762\n",
      "Epoch 134/200, Train Loss: 4.4924, Val Loss: 4.1669\n",
      "Epoch 135/200, Train Loss: 4.4608, Val Loss: 4.3092\n",
      "Epoch 136/200, Train Loss: 4.3487, Val Loss: 3.8650\n",
      "  New best validation loss: 3.8650\n",
      "Epoch 137/200, Train Loss: 4.3556, Val Loss: 4.2073\n",
      "Epoch 138/200, Train Loss: 4.1933, Val Loss: 3.7827\n",
      "  New best validation loss: 3.7827\n",
      "Epoch 139/200, Train Loss: 4.0970, Val Loss: 3.7558\n",
      "  New best validation loss: 3.7558\n",
      "Epoch 140/200, Train Loss: 4.1529, Val Loss: 3.8108\n",
      "Epoch 141/200, Train Loss: 4.1327, Val Loss: 4.1305\n",
      "Epoch 142/200, Train Loss: 4.1298, Val Loss: 3.8637\n",
      "Epoch 143/200, Train Loss: 4.0720, Val Loss: 3.6954\n",
      "  New best validation loss: 3.6954\n",
      "Epoch 144/200, Train Loss: 3.9736, Val Loss: 3.5959\n",
      "  New best validation loss: 3.5959\n",
      "Epoch 145/200, Train Loss: 4.0968, Val Loss: 3.6431\n",
      "Epoch 146/200, Train Loss: 4.0095, Val Loss: 3.8237\n",
      "Epoch 147/200, Train Loss: 4.0090, Val Loss: 3.7126\n",
      "Epoch 148/200, Train Loss: 3.8910, Val Loss: 3.5848\n",
      "  New best validation loss: 3.5848\n",
      "Epoch 149/200, Train Loss: 3.9700, Val Loss: 3.4727\n",
      "  New best validation loss: 3.4727\n",
      "Epoch 150/200, Train Loss: 3.8144, Val Loss: 3.5839\n",
      "Epoch 151/200, Train Loss: 3.8456, Val Loss: 3.8539\n",
      "Epoch 152/200, Train Loss: 3.7607, Val Loss: 3.6673\n",
      "Epoch 153/200, Train Loss: 4.0826, Val Loss: 3.6841\n",
      "Epoch 154/200, Train Loss: 3.8400, Val Loss: 4.2407\n",
      "Epoch 155/200, Train Loss: 4.0134, Val Loss: 4.1884\n",
      "Epoch 156/200, Train Loss: 3.9743, Val Loss: 3.8279\n",
      "Epoch 157/200, Train Loss: 3.8116, Val Loss: 3.6230\n",
      "Epoch 158/200, Train Loss: 3.7503, Val Loss: 3.4717\n",
      "  New best validation loss: 3.4717\n",
      "Epoch 159/200, Train Loss: 3.5369, Val Loss: 3.7385\n",
      "Epoch 160/200, Train Loss: 3.6356, Val Loss: 3.6799\n",
      "Epoch 161/200, Train Loss: 3.6143, Val Loss: 3.4547\n",
      "  New best validation loss: 3.4547\n",
      "Epoch 162/200, Train Loss: 3.5374, Val Loss: 3.4398\n",
      "  New best validation loss: 3.4398\n",
      "Epoch 163/200, Train Loss: 3.4725, Val Loss: 3.3885\n",
      "  New best validation loss: 3.3885\n",
      "Epoch 164/200, Train Loss: 3.4722, Val Loss: 3.4851\n",
      "Epoch 165/200, Train Loss: 3.3808, Val Loss: 3.4767\n",
      "Epoch 166/200, Train Loss: 3.4269, Val Loss: 3.4075\n",
      "Epoch 167/200, Train Loss: 3.4771, Val Loss: 3.4651\n",
      "Epoch 168/200, Train Loss: 3.4505, Val Loss: 3.3940\n",
      "Epoch 169/200, Train Loss: 3.5611, Val Loss: 3.9306\n",
      "Epoch 170/200, Train Loss: 3.4452, Val Loss: 3.5698\n",
      "Epoch 171/200, Train Loss: 3.3499, Val Loss: 3.5417\n",
      "Epoch 172/200, Train Loss: 3.4009, Val Loss: 3.4198\n",
      "Epoch 173/200, Train Loss: 3.2831, Val Loss: 3.3574\n",
      "  New best validation loss: 3.3574\n",
      "Epoch 174/200, Train Loss: 3.3065, Val Loss: 3.4539\n",
      "Epoch 175/200, Train Loss: 3.3191, Val Loss: 3.3295\n",
      "  New best validation loss: 3.3295\n",
      "Epoch 176/200, Train Loss: 3.3709, Val Loss: 3.3301\n",
      "Epoch 177/200, Train Loss: 3.3124, Val Loss: 3.2867\n",
      "  New best validation loss: 3.2867\n",
      "Epoch 178/200, Train Loss: 3.1906, Val Loss: 3.4081\n",
      "Epoch 179/200, Train Loss: 3.2122, Val Loss: 3.6001\n",
      "Epoch 180/200, Train Loss: 3.2825, Val Loss: 3.3242\n",
      "Epoch 181/200, Train Loss: 3.2467, Val Loss: 3.3831\n",
      "Epoch 182/200, Train Loss: 3.2828, Val Loss: 3.2990\n",
      "Epoch 183/200, Train Loss: 3.1102, Val Loss: 3.3635\n",
      "Epoch 184/200, Train Loss: 3.1200, Val Loss: 3.2831\n",
      "  New best validation loss: 3.2831\n",
      "Epoch 185/200, Train Loss: 3.1911, Val Loss: 3.5384\n",
      "Epoch 186/200, Train Loss: 3.3835, Val Loss: 3.4060\n",
      "Epoch 187/200, Train Loss: 3.2442, Val Loss: 3.4597\n",
      "Epoch 188/200, Train Loss: 3.2273, Val Loss: 3.6654\n",
      "Epoch 189/200, Train Loss: 3.1237, Val Loss: 3.3806\n",
      "Epoch 190/200, Train Loss: 3.1253, Val Loss: 3.5023\n",
      "Epoch 191/200, Train Loss: 3.0774, Val Loss: 3.5170\n",
      "Epoch 192/200, Train Loss: 3.1094, Val Loss: 3.4005\n",
      "Epoch 193/200, Train Loss: 3.0379, Val Loss: 3.3308\n",
      "Epoch 194/200, Train Loss: 3.0800, Val Loss: 3.4828\n",
      "Epoch 195/200, Train Loss: 3.0644, Val Loss: 3.6653\n",
      "Epoch 196/200, Train Loss: 3.0524, Val Loss: 3.3664\n",
      "Epoch 197/200, Train Loss: 3.0348, Val Loss: 3.4629\n",
      "Epoch 198/200, Train Loss: 2.9353, Val Loss: 3.5471\n",
      "Epoch 199/200, Train Loss: 2.9061, Val Loss: 3.5914\n",
      "Epoch 200/200, Train Loss: 2.9595, Val Loss: 3.2990\n",
      "\n",
      "Loaded best model (Val Loss: 3.2831) for final hidden state extraction.\n",
      "Saved best model for Transformer to results/20250508_090642/Transformer_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for Transformer ---\n",
      "  Analyzing decodability for Transformer...\n",
      "  Hidden states shape: torch.Size([160, 200, 64])\n",
      "  Coefficients shape: torch.Size([160, 10])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for Transformer - Test MSE: 0.8049, Test R2: 0.5688 (best alpha: 0.6158)\n",
      "Decodability (R2 score) for Transformer: 0.5688\n",
      "\n",
      "--- Training HIPPORNN_LegT ---\n",
      "Number of parameters: 99716\n",
      "Epoch 1/200, Train Loss: 11.8282, Val Loss: 10.8220\n",
      "  New best validation loss: 10.8220\n",
      "Epoch 2/200, Train Loss: 11.8270, Val Loss: 10.8210\n",
      "  New best validation loss: 10.8210\n",
      "Epoch 3/200, Train Loss: 11.8265, Val Loss: 10.8188\n",
      "  New best validation loss: 10.8188\n",
      "Epoch 4/200, Train Loss: 11.8222, Val Loss: 10.8091\n",
      "  New best validation loss: 10.8091\n",
      "Epoch 5/200, Train Loss: 11.7960, Val Loss: 10.7627\n",
      "  New best validation loss: 10.7627\n",
      "Epoch 6/200, Train Loss: 11.6202, Val Loss: 10.5815\n",
      "  New best validation loss: 10.5815\n",
      "Epoch 7/200, Train Loss: 11.3578, Val Loss: 10.1934\n",
      "  New best validation loss: 10.1934\n",
      "Epoch 8/200, Train Loss: 11.1580, Val Loss: 10.2206\n",
      "Epoch 9/200, Train Loss: 11.1265, Val Loss: 10.0510\n",
      "  New best validation loss: 10.0510\n",
      "Epoch 10/200, Train Loss: 11.0246, Val Loss: 9.9883\n",
      "  New best validation loss: 9.9883\n",
      "Epoch 11/200, Train Loss: 11.0009, Val Loss: 9.9556\n",
      "  New best validation loss: 9.9556\n",
      "Epoch 12/200, Train Loss: 10.9154, Val Loss: 9.8454\n",
      "  New best validation loss: 9.8454\n",
      "Epoch 13/200, Train Loss: 10.7663, Val Loss: 9.7280\n",
      "  New best validation loss: 9.7280\n",
      "Epoch 14/200, Train Loss: 10.6464, Val Loss: 9.4362\n",
      "  New best validation loss: 9.4362\n",
      "Epoch 15/200, Train Loss: 10.5426, Val Loss: 9.5226\n",
      "Epoch 16/200, Train Loss: 10.2208, Val Loss: 9.0964\n",
      "  New best validation loss: 9.0964\n",
      "Epoch 17/200, Train Loss: 9.9104, Val Loss: 8.7952\n",
      "  New best validation loss: 8.7952\n",
      "Epoch 18/200, Train Loss: 9.6131, Val Loss: 8.5587\n",
      "  New best validation loss: 8.5587\n",
      "Epoch 19/200, Train Loss: 9.2394, Val Loss: 8.2705\n",
      "  New best validation loss: 8.2705\n",
      "Epoch 20/200, Train Loss: 9.0692, Val Loss: 8.0929\n",
      "  New best validation loss: 8.0929\n",
      "Epoch 21/200, Train Loss: 8.7429, Val Loss: 7.7460\n",
      "  New best validation loss: 7.7460\n",
      "Epoch 22/200, Train Loss: 8.6904, Val Loss: 7.7335\n",
      "  New best validation loss: 7.7335\n",
      "Epoch 23/200, Train Loss: 8.4243, Val Loss: 7.3094\n",
      "  New best validation loss: 7.3094\n",
      "Epoch 24/200, Train Loss: 8.1652, Val Loss: 7.5139\n",
      "Epoch 25/200, Train Loss: 8.2039, Val Loss: 7.1841\n",
      "  New best validation loss: 7.1841\n",
      "Epoch 26/200, Train Loss: 7.8764, Val Loss: 6.9554\n",
      "  New best validation loss: 6.9554\n",
      "Epoch 27/200, Train Loss: 7.9464, Val Loss: 7.0723\n",
      "Epoch 28/200, Train Loss: 7.7853, Val Loss: 6.7107\n",
      "  New best validation loss: 6.7107\n",
      "Epoch 29/200, Train Loss: 7.6334, Val Loss: 6.6524\n",
      "  New best validation loss: 6.6524\n",
      "Epoch 30/200, Train Loss: 7.5618, Val Loss: 6.8663\n",
      "Epoch 31/200, Train Loss: 7.6058, Val Loss: 6.4092\n",
      "  New best validation loss: 6.4092\n",
      "Epoch 32/200, Train Loss: 7.3200, Val Loss: 6.4453\n",
      "Epoch 33/200, Train Loss: 7.2236, Val Loss: 6.2586\n",
      "  New best validation loss: 6.2586\n",
      "Epoch 34/200, Train Loss: 7.2141, Val Loss: 6.4269\n",
      "Epoch 35/200, Train Loss: 6.9733, Val Loss: 6.1270\n",
      "  New best validation loss: 6.1270\n",
      "Epoch 36/200, Train Loss: 6.9647, Val Loss: 6.3218\n",
      "Epoch 37/200, Train Loss: 6.9286, Val Loss: 6.2255\n",
      "Epoch 38/200, Train Loss: 6.8150, Val Loss: 6.2814\n",
      "Epoch 39/200, Train Loss: 6.8643, Val Loss: 6.3833\n",
      "Epoch 40/200, Train Loss: 6.9247, Val Loss: 6.1435\n",
      "Epoch 41/200, Train Loss: 6.6960, Val Loss: 5.8880\n",
      "  New best validation loss: 5.8880\n",
      "Epoch 42/200, Train Loss: 6.5878, Val Loss: 5.9821\n",
      "Epoch 43/200, Train Loss: 6.4741, Val Loss: 5.9185\n",
      "Epoch 44/200, Train Loss: 6.5308, Val Loss: 5.7560\n",
      "  New best validation loss: 5.7560\n",
      "Epoch 45/200, Train Loss: 6.4394, Val Loss: 6.2627\n",
      "Epoch 46/200, Train Loss: 6.7445, Val Loss: 6.2372\n",
      "Epoch 47/200, Train Loss: 6.5580, Val Loss: 5.9982\n",
      "Epoch 48/200, Train Loss: 6.3749, Val Loss: 5.7319\n",
      "  New best validation loss: 5.7319\n",
      "Epoch 49/200, Train Loss: 6.3380, Val Loss: 5.8435\n",
      "Epoch 50/200, Train Loss: 6.3613, Val Loss: 5.7118\n",
      "  New best validation loss: 5.7118\n",
      "Epoch 51/200, Train Loss: 6.3322, Val Loss: 5.5693\n",
      "  New best validation loss: 5.5693\n",
      "Epoch 52/200, Train Loss: 6.2715, Val Loss: 5.6127\n",
      "Epoch 53/200, Train Loss: 6.1067, Val Loss: 5.5283\n",
      "  New best validation loss: 5.5283\n",
      "Epoch 54/200, Train Loss: 6.1211, Val Loss: 5.7662\n",
      "Epoch 55/200, Train Loss: 6.1572, Val Loss: 5.5109\n",
      "  New best validation loss: 5.5109\n",
      "Epoch 56/200, Train Loss: 6.1405, Val Loss: 5.9215\n",
      "Epoch 57/200, Train Loss: 6.1481, Val Loss: 5.4872\n",
      "  New best validation loss: 5.4872\n",
      "Epoch 58/200, Train Loss: 5.9902, Val Loss: 5.5790\n",
      "Epoch 59/200, Train Loss: 6.0658, Val Loss: 5.6467\n",
      "Epoch 60/200, Train Loss: 6.0929, Val Loss: 5.7736\n",
      "Epoch 61/200, Train Loss: 6.0919, Val Loss: 5.5501\n",
      "Epoch 62/200, Train Loss: 5.9572, Val Loss: 5.4459\n",
      "  New best validation loss: 5.4459\n",
      "Epoch 63/200, Train Loss: 5.9212, Val Loss: 5.6740\n",
      "Epoch 64/200, Train Loss: 5.9298, Val Loss: 5.4949\n",
      "Epoch 65/200, Train Loss: 5.9059, Val Loss: 5.5488\n",
      "Epoch 66/200, Train Loss: 5.8782, Val Loss: 5.3571\n",
      "  New best validation loss: 5.3571\n",
      "Epoch 67/200, Train Loss: 5.7721, Val Loss: 5.6965\n",
      "Epoch 68/200, Train Loss: 5.9946, Val Loss: 5.4816\n",
      "Epoch 69/200, Train Loss: 5.9088, Val Loss: 5.5596\n",
      "Epoch 70/200, Train Loss: 5.8385, Val Loss: 5.3873\n",
      "Epoch 71/200, Train Loss: 5.7523, Val Loss: 5.3016\n",
      "  New best validation loss: 5.3016\n",
      "Epoch 72/200, Train Loss: 5.6087, Val Loss: 5.3122\n",
      "Epoch 73/200, Train Loss: 5.6018, Val Loss: 5.2262\n",
      "  New best validation loss: 5.2262\n",
      "Epoch 74/200, Train Loss: 5.5997, Val Loss: 5.2091\n",
      "  New best validation loss: 5.2091\n",
      "Epoch 75/200, Train Loss: 5.5742, Val Loss: 5.2880\n",
      "Epoch 76/200, Train Loss: 5.8183, Val Loss: 5.3310\n",
      "Epoch 77/200, Train Loss: 5.7328, Val Loss: 5.3155\n",
      "Epoch 78/200, Train Loss: 5.7552, Val Loss: 5.6839\n",
      "Epoch 79/200, Train Loss: 5.9039, Val Loss: 5.5249\n",
      "Epoch 80/200, Train Loss: 5.7709, Val Loss: 5.3286\n",
      "Epoch 81/200, Train Loss: 5.6863, Val Loss: 5.3833\n",
      "Epoch 82/200, Train Loss: 5.7672, Val Loss: 5.4063\n",
      "Epoch 83/200, Train Loss: 5.5925, Val Loss: 5.3021\n",
      "Epoch 84/200, Train Loss: 5.5244, Val Loss: 5.3544\n",
      "Epoch 85/200, Train Loss: 5.4666, Val Loss: 5.1689\n",
      "  New best validation loss: 5.1689\n",
      "Epoch 86/200, Train Loss: 5.4541, Val Loss: 5.1900\n",
      "Epoch 87/200, Train Loss: 5.5019, Val Loss: 5.5224\n",
      "Epoch 88/200, Train Loss: 5.5805, Val Loss: 5.3166\n",
      "Epoch 89/200, Train Loss: 5.4799, Val Loss: 5.2533\n",
      "Epoch 90/200, Train Loss: 5.4975, Val Loss: 5.1934\n",
      "Epoch 91/200, Train Loss: 5.4945, Val Loss: 5.3844\n",
      "Epoch 92/200, Train Loss: 5.6632, Val Loss: 5.4274\n",
      "Epoch 93/200, Train Loss: 5.6658, Val Loss: 5.2424\n",
      "Epoch 94/200, Train Loss: 5.4993, Val Loss: 5.2089\n",
      "Epoch 95/200, Train Loss: 5.3605, Val Loss: 5.1465\n",
      "  New best validation loss: 5.1465\n",
      "Epoch 96/200, Train Loss: 5.2956, Val Loss: 5.1151\n",
      "  New best validation loss: 5.1151\n",
      "Epoch 97/200, Train Loss: 5.2784, Val Loss: 5.2113\n",
      "Epoch 98/200, Train Loss: 5.3000, Val Loss: 5.1099\n",
      "  New best validation loss: 5.1099\n",
      "Epoch 99/200, Train Loss: 5.3104, Val Loss: 5.1037\n",
      "  New best validation loss: 5.1037\n",
      "Epoch 100/200, Train Loss: 5.3142, Val Loss: 5.1244\n",
      "Epoch 101/200, Train Loss: 5.3839, Val Loss: 5.5914\n",
      "Epoch 102/200, Train Loss: 5.4513, Val Loss: 5.2592\n",
      "Epoch 103/200, Train Loss: 5.2771, Val Loss: 5.3014\n",
      "Epoch 104/200, Train Loss: 5.2604, Val Loss: 5.1446\n",
      "Epoch 105/200, Train Loss: 5.2350, Val Loss: 5.2829\n",
      "Epoch 106/200, Train Loss: 5.3307, Val Loss: 5.1874\n",
      "Epoch 107/200, Train Loss: 5.2264, Val Loss: 5.0989\n",
      "  New best validation loss: 5.0989\n",
      "Epoch 108/200, Train Loss: 5.2396, Val Loss: 5.1877\n",
      "Epoch 109/200, Train Loss: 5.2343, Val Loss: 5.1274\n",
      "Epoch 110/200, Train Loss: 5.1714, Val Loss: 5.1478\n",
      "Epoch 111/200, Train Loss: 5.1637, Val Loss: 5.3558\n",
      "Epoch 112/200, Train Loss: 5.2155, Val Loss: 5.0335\n",
      "  New best validation loss: 5.0335\n",
      "Epoch 113/200, Train Loss: 5.1237, Val Loss: 5.1696\n",
      "Epoch 114/200, Train Loss: 5.0405, Val Loss: 5.1316\n",
      "Epoch 115/200, Train Loss: 5.0865, Val Loss: 5.1617\n",
      "Epoch 116/200, Train Loss: 5.1037, Val Loss: 5.1233\n",
      "Epoch 117/200, Train Loss: 5.0769, Val Loss: 5.1244\n",
      "Epoch 118/200, Train Loss: 5.0129, Val Loss: 5.0533\n",
      "Epoch 119/200, Train Loss: 4.9324, Val Loss: 5.0571\n",
      "Epoch 120/200, Train Loss: 4.9470, Val Loss: 5.0787\n",
      "Epoch 121/200, Train Loss: 4.8827, Val Loss: 5.0496\n",
      "Epoch 122/200, Train Loss: 4.9346, Val Loss: 5.0228\n",
      "  New best validation loss: 5.0228\n",
      "Epoch 123/200, Train Loss: 4.9944, Val Loss: 5.1582\n",
      "Epoch 124/200, Train Loss: 5.0414, Val Loss: 5.1462\n",
      "Epoch 125/200, Train Loss: 4.9667, Val Loss: 5.1632\n",
      "Epoch 126/200, Train Loss: 5.0203, Val Loss: 5.1588\n",
      "Epoch 127/200, Train Loss: 4.9691, Val Loss: 5.0284\n",
      "Epoch 128/200, Train Loss: 5.0786, Val Loss: 5.2677\n",
      "Epoch 129/200, Train Loss: 4.9478, Val Loss: 5.0775\n",
      "Epoch 130/200, Train Loss: 4.8790, Val Loss: 5.0803\n",
      "Epoch 131/200, Train Loss: 4.9607, Val Loss: 5.1005\n",
      "Epoch 132/200, Train Loss: 4.8779, Val Loss: 5.0606\n",
      "Epoch 133/200, Train Loss: 4.8347, Val Loss: 5.0872\n",
      "Epoch 134/200, Train Loss: 4.7634, Val Loss: 5.0266\n",
      "Epoch 135/200, Train Loss: 4.7434, Val Loss: 5.0940\n",
      "Epoch 136/200, Train Loss: 4.7240, Val Loss: 5.0746\n",
      "Epoch 137/200, Train Loss: 4.7061, Val Loss: 5.0222\n",
      "  New best validation loss: 5.0222\n",
      "Epoch 138/200, Train Loss: 4.6949, Val Loss: 5.0842\n",
      "Epoch 139/200, Train Loss: 4.7705, Val Loss: 5.1301\n",
      "Epoch 140/200, Train Loss: 4.7275, Val Loss: 5.0428\n",
      "Epoch 141/200, Train Loss: 4.6760, Val Loss: 5.0264\n",
      "Epoch 142/200, Train Loss: 4.6457, Val Loss: 5.2159\n",
      "Epoch 143/200, Train Loss: 4.7250, Val Loss: 5.0490\n",
      "Epoch 144/200, Train Loss: 4.6555, Val Loss: 5.2609\n",
      "Epoch 145/200, Train Loss: 4.8132, Val Loss: 5.1956\n",
      "Epoch 146/200, Train Loss: 4.8851, Val Loss: 5.2823\n",
      "Epoch 147/200, Train Loss: 4.7250, Val Loss: 5.2401\n",
      "Epoch 148/200, Train Loss: 4.6932, Val Loss: 5.0652\n",
      "Epoch 149/200, Train Loss: 4.6900, Val Loss: 5.0243\n",
      "Epoch 150/200, Train Loss: 4.5833, Val Loss: 5.1047\n",
      "Epoch 151/200, Train Loss: 4.5432, Val Loss: 5.0910\n",
      "Epoch 152/200, Train Loss: 4.5207, Val Loss: 5.0162\n",
      "  New best validation loss: 5.0162\n",
      "Epoch 153/200, Train Loss: 4.6954, Val Loss: 5.6895\n",
      "Epoch 154/200, Train Loss: 4.9067, Val Loss: 5.2847\n",
      "Epoch 155/200, Train Loss: 4.6373, Val Loss: 5.0425\n",
      "Epoch 156/200, Train Loss: 4.5511, Val Loss: 5.0925\n",
      "Epoch 157/200, Train Loss: 4.6280, Val Loss: 5.1551\n",
      "Epoch 158/200, Train Loss: 4.5349, Val Loss: 5.2093\n",
      "Epoch 159/200, Train Loss: 4.5490, Val Loss: 5.0255\n",
      "Epoch 160/200, Train Loss: 4.7458, Val Loss: 5.3213\n",
      "Epoch 161/200, Train Loss: 4.7263, Val Loss: 5.3851\n",
      "Epoch 162/200, Train Loss: 4.6587, Val Loss: 5.1277\n",
      "Epoch 163/200, Train Loss: 4.5572, Val Loss: 5.2234\n",
      "Epoch 164/200, Train Loss: 4.4288, Val Loss: 5.0925\n",
      "Epoch 165/200, Train Loss: 4.5705, Val Loss: 5.4911\n",
      "Epoch 166/200, Train Loss: 4.5705, Val Loss: 5.0932\n",
      "Epoch 167/200, Train Loss: 4.3991, Val Loss: 5.2014\n",
      "Epoch 168/200, Train Loss: 4.3961, Val Loss: 5.2739\n",
      "Epoch 169/200, Train Loss: 4.4193, Val Loss: 5.1011\n",
      "Epoch 170/200, Train Loss: 4.4257, Val Loss: 5.2047\n",
      "Epoch 171/200, Train Loss: 4.4488, Val Loss: 5.3227\n",
      "Epoch 172/200, Train Loss: 4.4092, Val Loss: 5.0039\n",
      "  New best validation loss: 5.0039\n",
      "Epoch 173/200, Train Loss: 4.3766, Val Loss: 5.2701\n",
      "Epoch 174/200, Train Loss: 4.3948, Val Loss: 5.1819\n",
      "Epoch 175/200, Train Loss: 4.3527, Val Loss: 5.1838\n",
      "Epoch 176/200, Train Loss: 4.3188, Val Loss: 5.2336\n",
      "Epoch 177/200, Train Loss: 4.3522, Val Loss: 5.1580\n",
      "Epoch 178/200, Train Loss: 4.4441, Val Loss: 5.1824\n",
      "Epoch 179/200, Train Loss: 4.3025, Val Loss: 5.0339\n",
      "Epoch 180/200, Train Loss: 4.2239, Val Loss: 5.2400\n",
      "Epoch 181/200, Train Loss: 4.2561, Val Loss: 5.1894\n",
      "Epoch 182/200, Train Loss: 4.2690, Val Loss: 5.1556\n",
      "Epoch 183/200, Train Loss: 4.2214, Val Loss: 5.2918\n",
      "Epoch 184/200, Train Loss: 4.4146, Val Loss: 5.0681\n",
      "Epoch 185/200, Train Loss: 4.3736, Val Loss: 5.3280\n",
      "Epoch 186/200, Train Loss: 4.3590, Val Loss: 5.0546\n",
      "Epoch 187/200, Train Loss: 4.2667, Val Loss: 5.2974\n",
      "Epoch 188/200, Train Loss: 4.2929, Val Loss: 5.1810\n",
      "Epoch 189/200, Train Loss: 4.2702, Val Loss: 5.0585\n",
      "Epoch 190/200, Train Loss: 4.2663, Val Loss: 5.1800\n",
      "Epoch 191/200, Train Loss: 4.1665, Val Loss: 5.2081\n",
      "Epoch 192/200, Train Loss: 4.1296, Val Loss: 5.4707\n",
      "Epoch 193/200, Train Loss: 4.2089, Val Loss: 5.2656\n",
      "Epoch 194/200, Train Loss: 4.2404, Val Loss: 5.3708\n",
      "Epoch 195/200, Train Loss: 4.1930, Val Loss: 5.3673\n",
      "Epoch 196/200, Train Loss: 4.1870, Val Loss: 5.2741\n",
      "Epoch 197/200, Train Loss: 4.1569, Val Loss: 5.2406\n",
      "Epoch 198/200, Train Loss: 4.1677, Val Loss: 5.3678\n",
      "Epoch 199/200, Train Loss: 4.0575, Val Loss: 5.1349\n",
      "Epoch 200/200, Train Loss: 4.1170, Val Loss: 5.3734\n",
      "\n",
      "Loaded best model (Val Loss: 5.0039) for final hidden state extraction.\n",
      "Saved best model for HIPPORNN_LegT to results/20250508_090642/HIPPORNN_LegT_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for HIPPORNN_LegT ---\n",
      "  Analyzing decodability for HIPPORNN_LegT...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 10])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n",
      "  RidgeCV Decoder for HIPPORNN_LegT - Test MSE: 0.8385, Test R2: 0.5609 (best alpha: 23.3572)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decodability (R2 score) for HIPPORNN_LegT: 0.5609\n",
      "\n",
      "--- Training NMRNN_Spatial_ModReadout ---\n",
      "Number of parameters: 66832\n",
      "Epoch 1/200, Train Loss: 11.7282, Val Loss: 10.6048\n",
      "  New best validation loss: 10.6048\n",
      "Epoch 2/200, Train Loss: 11.6301, Val Loss: 10.5059\n",
      "  New best validation loss: 10.5059\n",
      "Epoch 3/200, Train Loss: 11.6210, Val Loss: 10.5351\n",
      "Epoch 4/200, Train Loss: 11.6003, Val Loss: 10.5323\n",
      "Epoch 5/200, Train Loss: 11.5896, Val Loss: 10.4591\n",
      "  New best validation loss: 10.4591\n",
      "Epoch 6/200, Train Loss: 11.5634, Val Loss: 10.4581\n",
      "  New best validation loss: 10.4581\n",
      "Epoch 7/200, Train Loss: 11.5574, Val Loss: 10.4392\n",
      "  New best validation loss: 10.4392\n",
      "Epoch 8/200, Train Loss: 11.4995, Val Loss: 10.3434\n",
      "  New best validation loss: 10.3434\n",
      "Epoch 9/200, Train Loss: 11.4644, Val Loss: 10.4236\n",
      "Epoch 10/200, Train Loss: 11.3868, Val Loss: 10.1851\n",
      "  New best validation loss: 10.1851\n",
      "Epoch 11/200, Train Loss: 11.2707, Val Loss: 10.0963\n",
      "  New best validation loss: 10.0963\n",
      "Epoch 12/200, Train Loss: 11.1562, Val Loss: 10.1713\n",
      "Epoch 13/200, Train Loss: 11.1633, Val Loss: 10.1282\n",
      "Epoch 14/200, Train Loss: 10.9867, Val Loss: 9.8339\n",
      "  New best validation loss: 9.8339\n",
      "Epoch 15/200, Train Loss: 10.8362, Val Loss: 9.8469\n",
      "Epoch 16/200, Train Loss: 10.7348, Val Loss: 9.7080\n",
      "  New best validation loss: 9.7080\n",
      "Epoch 17/200, Train Loss: 10.6411, Val Loss: 9.6316\n",
      "  New best validation loss: 9.6316\n",
      "Epoch 18/200, Train Loss: 10.6048, Val Loss: 9.5770\n",
      "  New best validation loss: 9.5770\n",
      "Epoch 19/200, Train Loss: 10.5105, Val Loss: 9.6468\n",
      "Epoch 20/200, Train Loss: 10.4552, Val Loss: 9.7962\n",
      "Epoch 21/200, Train Loss: 10.4219, Val Loss: 9.4266\n",
      "  New best validation loss: 9.4266\n",
      "Epoch 22/200, Train Loss: 10.3318, Val Loss: 9.4834\n",
      "Epoch 23/200, Train Loss: 10.2704, Val Loss: 9.2961\n",
      "  New best validation loss: 9.2961\n",
      "Epoch 24/200, Train Loss: 10.1899, Val Loss: 9.3213\n",
      "Epoch 25/200, Train Loss: 10.1155, Val Loss: 9.4374\n",
      "Epoch 26/200, Train Loss: 10.1245, Val Loss: 9.4452\n",
      "Epoch 27/200, Train Loss: 10.0353, Val Loss: 9.1303\n",
      "  New best validation loss: 9.1303\n",
      "Epoch 28/200, Train Loss: 9.9275, Val Loss: 9.2210\n",
      "Epoch 29/200, Train Loss: 9.8531, Val Loss: 9.1603\n",
      "Epoch 30/200, Train Loss: 9.8010, Val Loss: 9.0234\n",
      "  New best validation loss: 9.0234\n",
      "Epoch 31/200, Train Loss: 9.7807, Val Loss: 9.0240\n",
      "Epoch 32/200, Train Loss: 9.7294, Val Loss: 9.5040\n",
      "Epoch 33/200, Train Loss: 9.7583, Val Loss: 8.9172\n",
      "  New best validation loss: 8.9172\n",
      "Epoch 34/200, Train Loss: 9.7124, Val Loss: 9.2362\n",
      "Epoch 35/200, Train Loss: 9.8711, Val Loss: 9.0428\n",
      "Epoch 36/200, Train Loss: 9.6982, Val Loss: 9.0739\n",
      "Epoch 37/200, Train Loss: 9.5870, Val Loss: 8.9932\n",
      "Epoch 38/200, Train Loss: 9.4659, Val Loss: 9.0693\n",
      "Epoch 39/200, Train Loss: 9.4898, Val Loss: 8.8487\n",
      "  New best validation loss: 8.8487\n",
      "Epoch 40/200, Train Loss: 9.3249, Val Loss: 8.9272\n",
      "Epoch 41/200, Train Loss: 9.2557, Val Loss: 8.8875\n",
      "Epoch 42/200, Train Loss: 9.2163, Val Loss: 8.9717\n",
      "Epoch 43/200, Train Loss: 9.1347, Val Loss: 8.8636\n",
      "Epoch 44/200, Train Loss: 9.1458, Val Loss: 8.7764\n",
      "  New best validation loss: 8.7764\n",
      "Epoch 45/200, Train Loss: 9.0559, Val Loss: 8.7285\n",
      "  New best validation loss: 8.7285\n",
      "Epoch 46/200, Train Loss: 9.0964, Val Loss: 8.8536\n",
      "Epoch 47/200, Train Loss: 9.2006, Val Loss: 9.0328\n",
      "Epoch 48/200, Train Loss: 9.1556, Val Loss: 8.8450\n",
      "Epoch 49/200, Train Loss: 8.9739, Val Loss: 8.7797\n",
      "Epoch 50/200, Train Loss: 8.8676, Val Loss: 8.5894\n",
      "  New best validation loss: 8.5894\n",
      "Epoch 51/200, Train Loss: 8.8764, Val Loss: 9.0744\n",
      "Epoch 52/200, Train Loss: 8.9584, Val Loss: 8.7529\n",
      "Epoch 53/200, Train Loss: 8.9684, Val Loss: 8.7195\n",
      "Epoch 54/200, Train Loss: 8.9126, Val Loss: 8.8133\n",
      "Epoch 55/200, Train Loss: 8.7472, Val Loss: 8.4252\n",
      "  New best validation loss: 8.4252\n",
      "Epoch 56/200, Train Loss: 8.6586, Val Loss: 8.8301\n",
      "Epoch 57/200, Train Loss: 8.6262, Val Loss: 8.7787\n",
      "Epoch 58/200, Train Loss: 8.4617, Val Loss: 8.5226\n",
      "Epoch 59/200, Train Loss: 8.6415, Val Loss: 8.6014\n",
      "Epoch 60/200, Train Loss: 8.5295, Val Loss: 8.3938\n",
      "  New best validation loss: 8.3938\n",
      "Epoch 61/200, Train Loss: 8.4500, Val Loss: 8.5613\n",
      "Epoch 62/200, Train Loss: 8.3631, Val Loss: 8.5279\n",
      "Epoch 63/200, Train Loss: 8.3760, Val Loss: 8.4054\n",
      "Epoch 64/200, Train Loss: 8.2579, Val Loss: 8.6445\n",
      "Epoch 65/200, Train Loss: 8.2122, Val Loss: 8.4746\n",
      "Epoch 66/200, Train Loss: 8.1640, Val Loss: 8.4891\n",
      "Epoch 67/200, Train Loss: 8.1957, Val Loss: 8.3266\n",
      "  New best validation loss: 8.3266\n",
      "Epoch 68/200, Train Loss: 8.0836, Val Loss: 8.2293\n",
      "  New best validation loss: 8.2293\n",
      "Epoch 69/200, Train Loss: 7.9469, Val Loss: 8.3874\n",
      "Epoch 70/200, Train Loss: 7.8790, Val Loss: 8.4644\n",
      "Epoch 71/200, Train Loss: 7.9448, Val Loss: 8.2322\n",
      "Epoch 72/200, Train Loss: 7.8659, Val Loss: 8.1119\n",
      "  New best validation loss: 8.1119\n",
      "Epoch 73/200, Train Loss: 7.8443, Val Loss: 8.5247\n",
      "Epoch 74/200, Train Loss: 7.7877, Val Loss: 8.3430\n",
      "Epoch 75/200, Train Loss: 7.8520, Val Loss: 8.3203\n",
      "Epoch 76/200, Train Loss: 7.7909, Val Loss: 8.5095\n",
      "Epoch 77/200, Train Loss: 7.6746, Val Loss: 8.1917\n",
      "Epoch 78/200, Train Loss: 7.5519, Val Loss: 8.3199\n",
      "Epoch 79/200, Train Loss: 7.5975, Val Loss: 8.1071\n",
      "  New best validation loss: 8.1071\n",
      "Epoch 80/200, Train Loss: 7.5595, Val Loss: 8.3022\n",
      "Epoch 81/200, Train Loss: 7.5295, Val Loss: 8.1027\n",
      "  New best validation loss: 8.1027\n",
      "Epoch 82/200, Train Loss: 7.4839, Val Loss: 8.3170\n",
      "Epoch 83/200, Train Loss: 7.3744, Val Loss: 8.2248\n",
      "Epoch 84/200, Train Loss: 7.4478, Val Loss: 8.0862\n",
      "  New best validation loss: 8.0862\n",
      "Epoch 85/200, Train Loss: 7.3395, Val Loss: 8.1361\n",
      "Epoch 86/200, Train Loss: 7.3016, Val Loss: 8.0356\n",
      "  New best validation loss: 8.0356\n",
      "Epoch 87/200, Train Loss: 7.2188, Val Loss: 8.0066\n",
      "  New best validation loss: 8.0066\n",
      "Epoch 88/200, Train Loss: 7.1230, Val Loss: 8.0612\n",
      "Epoch 89/200, Train Loss: 7.3073, Val Loss: 8.1433\n",
      "Epoch 90/200, Train Loss: 7.1953, Val Loss: 7.9905\n",
      "  New best validation loss: 7.9905\n",
      "Epoch 91/200, Train Loss: 7.0519, Val Loss: 7.9449\n",
      "  New best validation loss: 7.9449\n",
      "Epoch 92/200, Train Loss: 6.9367, Val Loss: 8.0035\n",
      "Epoch 93/200, Train Loss: 6.9511, Val Loss: 7.9716\n",
      "Epoch 94/200, Train Loss: 6.9262, Val Loss: 7.9095\n",
      "  New best validation loss: 7.9095\n",
      "Epoch 95/200, Train Loss: 6.9529, Val Loss: 8.4873\n",
      "Epoch 96/200, Train Loss: 7.0488, Val Loss: 8.0749\n",
      "Epoch 97/200, Train Loss: 6.8666, Val Loss: 8.1242\n",
      "Epoch 98/200, Train Loss: 6.8682, Val Loss: 8.1457\n",
      "Epoch 99/200, Train Loss: 6.8298, Val Loss: 8.2781\n",
      "Epoch 100/200, Train Loss: 6.7438, Val Loss: 7.9490\n",
      "Epoch 101/200, Train Loss: 6.7212, Val Loss: 8.0009\n",
      "Epoch 102/200, Train Loss: 6.8989, Val Loss: 7.8822\n",
      "  New best validation loss: 7.8822\n",
      "Epoch 103/200, Train Loss: 6.6318, Val Loss: 7.8762\n",
      "  New best validation loss: 7.8762\n",
      "Epoch 104/200, Train Loss: 6.6427, Val Loss: 7.7842\n",
      "  New best validation loss: 7.7842\n",
      "Epoch 105/200, Train Loss: 6.7383, Val Loss: 7.9120\n",
      "Epoch 106/200, Train Loss: 6.5882, Val Loss: 8.0514\n",
      "Epoch 107/200, Train Loss: 6.5410, Val Loss: 8.0016\n",
      "Epoch 108/200, Train Loss: 6.4717, Val Loss: 8.0967\n",
      "Epoch 109/200, Train Loss: 6.5365, Val Loss: 7.9561\n",
      "Epoch 110/200, Train Loss: 6.6151, Val Loss: 8.6614\n",
      "Epoch 111/200, Train Loss: 6.7833, Val Loss: 8.6457\n",
      "Epoch 112/200, Train Loss: 7.0960, Val Loss: 8.0434\n",
      "Epoch 113/200, Train Loss: 6.4072, Val Loss: 7.9100\n",
      "Epoch 114/200, Train Loss: 6.2626, Val Loss: 8.2257\n",
      "Epoch 115/200, Train Loss: 6.2833, Val Loss: 7.9718\n",
      "Epoch 116/200, Train Loss: 6.2793, Val Loss: 8.1761\n",
      "Epoch 117/200, Train Loss: 6.3010, Val Loss: 8.2439\n",
      "Epoch 118/200, Train Loss: 6.5203, Val Loss: 8.4636\n",
      "Epoch 119/200, Train Loss: 6.3765, Val Loss: 8.1957\n",
      "Epoch 120/200, Train Loss: 6.3550, Val Loss: 7.8519\n",
      "Epoch 121/200, Train Loss: 6.1744, Val Loss: 8.0506\n",
      "Epoch 122/200, Train Loss: 6.1680, Val Loss: 7.9083\n",
      "Epoch 123/200, Train Loss: 6.1430, Val Loss: 7.9903\n",
      "Epoch 124/200, Train Loss: 6.1029, Val Loss: 8.1263\n",
      "Epoch 125/200, Train Loss: 6.0065, Val Loss: 7.9819\n",
      "Epoch 126/200, Train Loss: 6.0523, Val Loss: 8.0068\n",
      "Epoch 127/200, Train Loss: 6.0072, Val Loss: 8.1900\n",
      "Epoch 128/200, Train Loss: 5.9933, Val Loss: 8.1144\n",
      "Epoch 129/200, Train Loss: 6.0737, Val Loss: 8.1967\n",
      "Epoch 130/200, Train Loss: 6.0511, Val Loss: 7.8781\n",
      "Epoch 131/200, Train Loss: 5.9563, Val Loss: 7.9818\n",
      "Epoch 132/200, Train Loss: 5.9464, Val Loss: 7.9452\n",
      "Epoch 133/200, Train Loss: 5.9044, Val Loss: 8.0399\n",
      "Epoch 134/200, Train Loss: 5.9022, Val Loss: 8.2678\n",
      "Epoch 135/200, Train Loss: 5.8861, Val Loss: 8.0200\n",
      "Epoch 136/200, Train Loss: 5.8001, Val Loss: 8.0274\n",
      "Epoch 137/200, Train Loss: 5.7155, Val Loss: 8.2906\n",
      "Epoch 138/200, Train Loss: 5.7749, Val Loss: 8.1737\n",
      "Epoch 139/200, Train Loss: 5.8453, Val Loss: 8.0557\n",
      "Epoch 140/200, Train Loss: 5.8608, Val Loss: 8.0335\n",
      "Epoch 141/200, Train Loss: 5.7089, Val Loss: 8.0027\n",
      "Epoch 142/200, Train Loss: 5.7371, Val Loss: 8.0572\n",
      "Epoch 143/200, Train Loss: 5.7197, Val Loss: 7.9576\n",
      "Epoch 144/200, Train Loss: 5.6827, Val Loss: 8.1559\n",
      "Epoch 145/200, Train Loss: 5.7081, Val Loss: 8.1393\n",
      "Epoch 146/200, Train Loss: 5.6144, Val Loss: 8.0690\n",
      "Epoch 147/200, Train Loss: 5.6143, Val Loss: 7.9049\n",
      "Epoch 148/200, Train Loss: 5.6600, Val Loss: 8.2860\n",
      "Epoch 149/200, Train Loss: 5.6167, Val Loss: 8.1547\n",
      "Epoch 150/200, Train Loss: 5.6872, Val Loss: 7.9408\n",
      "Epoch 151/200, Train Loss: 5.7954, Val Loss: 8.2725\n",
      "Epoch 152/200, Train Loss: 5.8818, Val Loss: 8.0737\n",
      "Epoch 153/200, Train Loss: 5.6563, Val Loss: 8.0224\n",
      "Epoch 154/200, Train Loss: 5.7334, Val Loss: 8.1258\n",
      "Epoch 155/200, Train Loss: 5.6516, Val Loss: 8.2651\n",
      "Epoch 156/200, Train Loss: 5.5582, Val Loss: 8.0684\n",
      "Epoch 157/200, Train Loss: 5.6853, Val Loss: 8.2673\n",
      "Epoch 158/200, Train Loss: 5.7540, Val Loss: 8.2712\n",
      "Epoch 159/200, Train Loss: 5.5565, Val Loss: 8.2786\n",
      "Epoch 160/200, Train Loss: 5.4268, Val Loss: 7.9921\n",
      "Epoch 161/200, Train Loss: 5.4382, Val Loss: 8.3056\n",
      "Epoch 162/200, Train Loss: 5.4064, Val Loss: 7.8202\n",
      "Epoch 163/200, Train Loss: 5.4612, Val Loss: 8.1609\n",
      "Epoch 164/200, Train Loss: 5.4103, Val Loss: 8.1711\n",
      "Epoch 165/200, Train Loss: 5.3101, Val Loss: 8.0885\n",
      "Epoch 166/200, Train Loss: 5.3302, Val Loss: 8.1600\n",
      "Epoch 167/200, Train Loss: 5.2801, Val Loss: 8.0978\n",
      "Epoch 168/200, Train Loss: 5.2601, Val Loss: 8.0647\n",
      "Epoch 169/200, Train Loss: 5.3054, Val Loss: 8.0512\n",
      "Epoch 170/200, Train Loss: 5.3244, Val Loss: 8.1169\n",
      "Epoch 171/200, Train Loss: 5.3098, Val Loss: 8.1914\n",
      "Epoch 172/200, Train Loss: 5.2739, Val Loss: 8.0847\n",
      "Epoch 173/200, Train Loss: 5.2369, Val Loss: 8.6538\n",
      "Epoch 174/200, Train Loss: 5.2211, Val Loss: 7.9976\n",
      "Epoch 175/200, Train Loss: 5.1644, Val Loss: 8.2995\n",
      "Epoch 176/200, Train Loss: 5.1681, Val Loss: 7.9856\n",
      "Epoch 177/200, Train Loss: 5.1980, Val Loss: 8.1033\n",
      "Epoch 178/200, Train Loss: 5.1522, Val Loss: 8.0631\n",
      "Epoch 179/200, Train Loss: 5.1610, Val Loss: 7.9919\n",
      "Epoch 180/200, Train Loss: 5.1240, Val Loss: 8.1284\n",
      "Epoch 181/200, Train Loss: 5.1799, Val Loss: 8.0211\n",
      "Epoch 182/200, Train Loss: 5.1182, Val Loss: 8.0659\n",
      "Epoch 183/200, Train Loss: 5.1214, Val Loss: 8.1041\n",
      "Epoch 184/200, Train Loss: 5.2106, Val Loss: 8.0511\n",
      "Epoch 185/200, Train Loss: 5.1122, Val Loss: 8.0057\n",
      "Epoch 186/200, Train Loss: 4.9890, Val Loss: 8.1177\n",
      "Epoch 187/200, Train Loss: 5.0380, Val Loss: 8.0885\n",
      "Epoch 188/200, Train Loss: 5.0428, Val Loss: 8.2727\n",
      "Epoch 189/200, Train Loss: 4.9165, Val Loss: 8.3139\n",
      "Epoch 190/200, Train Loss: 4.9771, Val Loss: 8.4029\n",
      "Epoch 191/200, Train Loss: 5.0240, Val Loss: 8.1760\n",
      "Epoch 192/200, Train Loss: 5.0151, Val Loss: 8.1760\n",
      "Epoch 193/200, Train Loss: 5.0021, Val Loss: 7.9510\n",
      "Epoch 194/200, Train Loss: 4.8984, Val Loss: 8.2146\n",
      "Epoch 195/200, Train Loss: 4.8462, Val Loss: 8.3286\n",
      "Epoch 196/200, Train Loss: 4.8881, Val Loss: 8.1152\n",
      "Epoch 197/200, Train Loss: 4.9079, Val Loss: 8.0945\n",
      "Epoch 198/200, Train Loss: 4.8990, Val Loss: 8.0185\n",
      "Epoch 199/200, Train Loss: 4.9033, Val Loss: 8.0995\n",
      "Epoch 200/200, Train Loss: 4.8419, Val Loss: 8.2043\n",
      "\n",
      "Loaded best model (Val Loss: 7.7842) for final hidden state extraction.\n",
      "Saved best model for NMRNN_Spatial_ModReadout to results/20250508_090642/NMRNN_Spatial_ModReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_Spatial_ModReadout ---\n",
      "  Analyzing decodability for NMRNN_Spatial_ModReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 10])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n",
      "  RidgeCV Decoder for NMRNN_Spatial_ModReadout - Test MSE: 1.2177, Test R2: 0.3681 (best alpha: 48.3293)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decodability (R2 score) for NMRNN_Spatial_ModReadout: 0.3681\n",
      "\n",
      "--- Training NMRNN_NoSpatial_ModReadout ---\n",
      "Number of parameters: 66832\n",
      "Epoch 1/200, Train Loss: 11.7239, Val Loss: 10.6394\n",
      "  New best validation loss: 10.6394\n",
      "Epoch 2/200, Train Loss: 11.6091, Val Loss: 10.5036\n",
      "  New best validation loss: 10.5036\n",
      "Epoch 3/200, Train Loss: 11.6039, Val Loss: 10.4902\n",
      "  New best validation loss: 10.4902\n",
      "Epoch 4/200, Train Loss: 11.5820, Val Loss: 10.5107\n",
      "Epoch 5/200, Train Loss: 11.5717, Val Loss: 10.4247\n",
      "  New best validation loss: 10.4247\n",
      "Epoch 6/200, Train Loss: 11.5141, Val Loss: 10.4303\n",
      "Epoch 7/200, Train Loss: 11.4809, Val Loss: 10.4041\n",
      "  New best validation loss: 10.4041\n",
      "Epoch 8/200, Train Loss: 11.4040, Val Loss: 10.2663\n",
      "  New best validation loss: 10.2663\n",
      "Epoch 9/200, Train Loss: 11.3374, Val Loss: 10.1932\n",
      "  New best validation loss: 10.1932\n",
      "Epoch 10/200, Train Loss: 11.2723, Val Loss: 10.1545\n",
      "  New best validation loss: 10.1545\n",
      "Epoch 11/200, Train Loss: 11.1561, Val Loss: 10.0262\n",
      "  New best validation loss: 10.0262\n",
      "Epoch 12/200, Train Loss: 11.0692, Val Loss: 9.9540\n",
      "  New best validation loss: 9.9540\n",
      "Epoch 13/200, Train Loss: 10.9705, Val Loss: 9.9270\n",
      "  New best validation loss: 9.9270\n",
      "Epoch 14/200, Train Loss: 10.8997, Val Loss: 9.9047\n",
      "  New best validation loss: 9.9047\n",
      "Epoch 15/200, Train Loss: 10.7864, Val Loss: 9.8505\n",
      "  New best validation loss: 9.8505\n",
      "Epoch 16/200, Train Loss: 10.7569, Val Loss: 9.7531\n",
      "  New best validation loss: 9.7531\n",
      "Epoch 17/200, Train Loss: 10.6379, Val Loss: 9.6075\n",
      "  New best validation loss: 9.6075\n",
      "Epoch 18/200, Train Loss: 10.5105, Val Loss: 9.7146\n",
      "Epoch 19/200, Train Loss: 10.4827, Val Loss: 9.7638\n",
      "Epoch 20/200, Train Loss: 10.4584, Val Loss: 9.4287\n",
      "  New best validation loss: 9.4287\n",
      "Epoch 21/200, Train Loss: 10.3745, Val Loss: 9.6566\n",
      "Epoch 22/200, Train Loss: 10.4699, Val Loss: 9.4094\n",
      "  New best validation loss: 9.4094\n",
      "Epoch 23/200, Train Loss: 10.3445, Val Loss: 9.5096\n",
      "Epoch 24/200, Train Loss: 10.2121, Val Loss: 9.3935\n",
      "  New best validation loss: 9.3935\n",
      "Epoch 25/200, Train Loss: 10.1972, Val Loss: 9.4460\n",
      "Epoch 26/200, Train Loss: 10.1757, Val Loss: 9.2135\n",
      "  New best validation loss: 9.2135\n",
      "Epoch 27/200, Train Loss: 10.0465, Val Loss: 9.2772\n",
      "Epoch 28/200, Train Loss: 9.9603, Val Loss: 9.1997\n",
      "  New best validation loss: 9.1997\n",
      "Epoch 29/200, Train Loss: 9.9562, Val Loss: 9.1635\n",
      "  New best validation loss: 9.1635\n",
      "Epoch 30/200, Train Loss: 9.8601, Val Loss: 9.1923\n",
      "Epoch 31/200, Train Loss: 9.8098, Val Loss: 9.2344\n",
      "Epoch 32/200, Train Loss: 9.8014, Val Loss: 9.0958\n",
      "  New best validation loss: 9.0958\n",
      "Epoch 33/200, Train Loss: 9.7189, Val Loss: 9.1175\n",
      "Epoch 34/200, Train Loss: 9.7426, Val Loss: 8.9974\n",
      "  New best validation loss: 8.9974\n",
      "Epoch 35/200, Train Loss: 9.6163, Val Loss: 9.0989\n",
      "Epoch 36/200, Train Loss: 9.5540, Val Loss: 8.9994\n",
      "Epoch 37/200, Train Loss: 9.5394, Val Loss: 9.0620\n",
      "Epoch 38/200, Train Loss: 9.4937, Val Loss: 8.8733\n",
      "  New best validation loss: 8.8733\n",
      "Epoch 39/200, Train Loss: 9.4165, Val Loss: 8.8775\n",
      "Epoch 40/200, Train Loss: 9.4018, Val Loss: 8.9239\n",
      "Epoch 41/200, Train Loss: 9.3511, Val Loss: 8.8516\n",
      "  New best validation loss: 8.8516\n",
      "Epoch 42/200, Train Loss: 9.3362, Val Loss: 8.8554\n",
      "Epoch 43/200, Train Loss: 9.2108, Val Loss: 8.9598\n",
      "Epoch 44/200, Train Loss: 9.1953, Val Loss: 8.8380\n",
      "  New best validation loss: 8.8380\n",
      "Epoch 45/200, Train Loss: 9.2922, Val Loss: 8.9788\n",
      "Epoch 46/200, Train Loss: 9.4241, Val Loss: 8.8194\n",
      "  New best validation loss: 8.8194\n",
      "Epoch 47/200, Train Loss: 9.3601, Val Loss: 8.7468\n",
      "  New best validation loss: 8.7468\n",
      "Epoch 48/200, Train Loss: 9.2272, Val Loss: 8.8327\n",
      "Epoch 49/200, Train Loss: 9.1841, Val Loss: 8.7615\n",
      "Epoch 50/200, Train Loss: 9.1614, Val Loss: 8.6833\n",
      "  New best validation loss: 8.6833\n",
      "Epoch 51/200, Train Loss: 9.0190, Val Loss: 8.7839\n",
      "Epoch 52/200, Train Loss: 8.9628, Val Loss: 8.7087\n",
      "Epoch 53/200, Train Loss: 8.9310, Val Loss: 8.8412\n",
      "Epoch 54/200, Train Loss: 8.8541, Val Loss: 8.6952\n",
      "Epoch 55/200, Train Loss: 8.8371, Val Loss: 8.6176\n",
      "  New best validation loss: 8.6176\n",
      "Epoch 56/200, Train Loss: 8.6587, Val Loss: 8.5165\n",
      "  New best validation loss: 8.5165\n",
      "Epoch 57/200, Train Loss: 8.5890, Val Loss: 8.5096\n",
      "  New best validation loss: 8.5096\n",
      "Epoch 58/200, Train Loss: 8.5420, Val Loss: 8.5479\n",
      "Epoch 59/200, Train Loss: 8.5409, Val Loss: 8.5382\n",
      "Epoch 60/200, Train Loss: 8.5602, Val Loss: 8.4554\n",
      "  New best validation loss: 8.4554\n",
      "Epoch 61/200, Train Loss: 8.4634, Val Loss: 8.5540\n",
      "Epoch 62/200, Train Loss: 8.4257, Val Loss: 8.7559\n",
      "Epoch 63/200, Train Loss: 8.3844, Val Loss: 8.3761\n",
      "  New best validation loss: 8.3761\n",
      "Epoch 64/200, Train Loss: 8.3739, Val Loss: 8.3812\n",
      "Epoch 65/200, Train Loss: 8.2072, Val Loss: 8.6639\n",
      "Epoch 66/200, Train Loss: 8.2112, Val Loss: 8.8053\n",
      "Epoch 67/200, Train Loss: 8.3791, Val Loss: 8.3325\n",
      "  New best validation loss: 8.3325\n",
      "Epoch 68/200, Train Loss: 8.0799, Val Loss: 8.4148\n",
      "Epoch 69/200, Train Loss: 8.1020, Val Loss: 8.2943\n",
      "  New best validation loss: 8.2943\n",
      "Epoch 70/200, Train Loss: 8.1302, Val Loss: 8.3545\n",
      "Epoch 71/200, Train Loss: 7.9367, Val Loss: 8.2609\n",
      "  New best validation loss: 8.2609\n",
      "Epoch 72/200, Train Loss: 7.8140, Val Loss: 8.3623\n",
      "Epoch 73/200, Train Loss: 7.8007, Val Loss: 8.5910\n",
      "Epoch 74/200, Train Loss: 7.8489, Val Loss: 8.3897\n",
      "Epoch 75/200, Train Loss: 7.9376, Val Loss: 9.2308\n",
      "Epoch 76/200, Train Loss: 7.8764, Val Loss: 9.0638\n",
      "Epoch 77/200, Train Loss: 7.7961, Val Loss: 8.7410\n",
      "Epoch 78/200, Train Loss: 7.6535, Val Loss: 8.1246\n",
      "  New best validation loss: 8.1246\n",
      "Epoch 79/200, Train Loss: 7.6200, Val Loss: 8.5549\n",
      "Epoch 80/200, Train Loss: 7.4948, Val Loss: 8.2586\n",
      "Epoch 81/200, Train Loss: 7.4564, Val Loss: 8.3104\n",
      "Epoch 82/200, Train Loss: 7.4625, Val Loss: 8.5875\n",
      "Epoch 83/200, Train Loss: 7.7116, Val Loss: 8.1475\n",
      "Epoch 84/200, Train Loss: 7.4005, Val Loss: 8.4212\n",
      "Epoch 85/200, Train Loss: 7.5425, Val Loss: 8.7072\n",
      "Epoch 86/200, Train Loss: 7.3912, Val Loss: 8.4180\n",
      "Epoch 87/200, Train Loss: 7.1969, Val Loss: 8.4973\n",
      "Epoch 88/200, Train Loss: 7.1522, Val Loss: 8.2751\n",
      "Epoch 89/200, Train Loss: 7.3016, Val Loss: 8.2084\n",
      "Epoch 90/200, Train Loss: 7.2675, Val Loss: 8.4925\n",
      "Epoch 91/200, Train Loss: 7.1545, Val Loss: 8.3100\n",
      "Epoch 92/200, Train Loss: 7.1318, Val Loss: 8.2126\n",
      "Epoch 93/200, Train Loss: 7.0636, Val Loss: 8.2900\n",
      "Epoch 94/200, Train Loss: 7.0569, Val Loss: 8.1922\n",
      "Epoch 95/200, Train Loss: 7.1138, Val Loss: 8.1807\n",
      "Epoch 96/200, Train Loss: 6.9712, Val Loss: 8.2866\n",
      "Epoch 97/200, Train Loss: 6.8634, Val Loss: 8.3611\n",
      "Epoch 98/200, Train Loss: 6.8502, Val Loss: 8.2257\n",
      "Epoch 99/200, Train Loss: 7.0490, Val Loss: 8.4433\n",
      "Epoch 100/200, Train Loss: 6.9637, Val Loss: 8.3822\n",
      "Epoch 101/200, Train Loss: 6.7737, Val Loss: 8.4738\n",
      "Epoch 102/200, Train Loss: 6.8048, Val Loss: 8.0677\n",
      "  New best validation loss: 8.0677\n",
      "Epoch 103/200, Train Loss: 6.7725, Val Loss: 8.5559\n",
      "Epoch 104/200, Train Loss: 6.7118, Val Loss: 9.1982\n",
      "Epoch 105/200, Train Loss: 6.8423, Val Loss: 8.6417\n",
      "Epoch 106/200, Train Loss: 6.6968, Val Loss: 8.5482\n",
      "Epoch 107/200, Train Loss: 6.7891, Val Loss: 8.2030\n",
      "Epoch 108/200, Train Loss: 6.6010, Val Loss: 8.3322\n",
      "Epoch 109/200, Train Loss: 6.4670, Val Loss: 8.6739\n",
      "Epoch 110/200, Train Loss: 6.5157, Val Loss: 8.4822\n",
      "Epoch 111/200, Train Loss: 6.6705, Val Loss: 8.2900\n",
      "Epoch 112/200, Train Loss: 6.4581, Val Loss: 8.2455\n",
      "Epoch 113/200, Train Loss: 6.6517, Val Loss: 8.2046\n",
      "Epoch 114/200, Train Loss: 6.5427, Val Loss: 8.2295\n",
      "Epoch 115/200, Train Loss: 6.4705, Val Loss: 8.2882\n",
      "Epoch 116/200, Train Loss: 6.2715, Val Loss: 7.9598\n",
      "  New best validation loss: 7.9598\n",
      "Epoch 117/200, Train Loss: 6.2819, Val Loss: 8.7906\n",
      "Epoch 118/200, Train Loss: 6.2944, Val Loss: 8.4670\n",
      "Epoch 119/200, Train Loss: 6.2186, Val Loss: 8.1263\n",
      "Epoch 120/200, Train Loss: 6.1720, Val Loss: 7.8323\n",
      "  New best validation loss: 7.8323\n",
      "Epoch 121/200, Train Loss: 6.1743, Val Loss: 8.0179\n",
      "Epoch 122/200, Train Loss: 6.1761, Val Loss: 8.3629\n",
      "Epoch 123/200, Train Loss: 6.1803, Val Loss: 8.3274\n",
      "Epoch 124/200, Train Loss: 6.0923, Val Loss: 8.5105\n",
      "Epoch 125/200, Train Loss: 6.1634, Val Loss: 7.9708\n",
      "Epoch 126/200, Train Loss: 6.0182, Val Loss: 8.1416\n",
      "Epoch 127/200, Train Loss: 6.0390, Val Loss: 8.1127\n",
      "Epoch 128/200, Train Loss: 6.0055, Val Loss: 8.2219\n",
      "Epoch 129/200, Train Loss: 6.0439, Val Loss: 8.3119\n",
      "Epoch 130/200, Train Loss: 6.0586, Val Loss: 8.5813\n",
      "Epoch 131/200, Train Loss: 6.0919, Val Loss: 9.1974\n",
      "Epoch 132/200, Train Loss: 6.1686, Val Loss: 8.9081\n",
      "Epoch 133/200, Train Loss: 6.0228, Val Loss: 8.4213\n",
      "Epoch 134/200, Train Loss: 5.9853, Val Loss: 8.3486\n",
      "Epoch 135/200, Train Loss: 6.0236, Val Loss: 8.1644\n",
      "Epoch 136/200, Train Loss: 6.0081, Val Loss: 8.1110\n",
      "Epoch 137/200, Train Loss: 5.9608, Val Loss: 8.5526\n",
      "Epoch 138/200, Train Loss: 5.9531, Val Loss: 8.0102\n",
      "Epoch 139/200, Train Loss: 6.0155, Val Loss: 8.1149\n",
      "Epoch 140/200, Train Loss: 5.8702, Val Loss: 8.4306\n",
      "Epoch 141/200, Train Loss: 5.8861, Val Loss: 7.9873\n",
      "Epoch 142/200, Train Loss: 5.7837, Val Loss: 8.2229\n",
      "Epoch 143/200, Train Loss: 5.6962, Val Loss: 8.2575\n",
      "Epoch 144/200, Train Loss: 5.6948, Val Loss: 8.2697\n",
      "Epoch 145/200, Train Loss: 5.7654, Val Loss: 8.2301\n",
      "Epoch 146/200, Train Loss: 5.9243, Val Loss: 8.7769\n",
      "Epoch 147/200, Train Loss: 5.8664, Val Loss: 8.1569\n",
      "Epoch 148/200, Train Loss: 5.6892, Val Loss: 8.3665\n",
      "Epoch 149/200, Train Loss: 5.6835, Val Loss: 8.7775\n",
      "Epoch 150/200, Train Loss: 5.7818, Val Loss: 8.5644\n",
      "Epoch 151/200, Train Loss: 5.6121, Val Loss: 8.3017\n",
      "Epoch 152/200, Train Loss: 5.5410, Val Loss: 8.1632\n",
      "Epoch 153/200, Train Loss: 5.5232, Val Loss: 8.4331\n",
      "Epoch 154/200, Train Loss: 5.6142, Val Loss: 8.4351\n",
      "Epoch 155/200, Train Loss: 5.4882, Val Loss: 8.2034\n",
      "Epoch 156/200, Train Loss: 5.4738, Val Loss: 7.9557\n",
      "Epoch 157/200, Train Loss: 5.5685, Val Loss: 8.4382\n",
      "Epoch 158/200, Train Loss: 5.4737, Val Loss: 8.5200\n",
      "Epoch 159/200, Train Loss: 5.6273, Val Loss: 8.4197\n",
      "Epoch 160/200, Train Loss: 5.5270, Val Loss: 8.3373\n",
      "Epoch 161/200, Train Loss: 5.3755, Val Loss: 8.6369\n",
      "Epoch 162/200, Train Loss: 5.3796, Val Loss: 8.3546\n",
      "Epoch 163/200, Train Loss: 5.3357, Val Loss: 8.2081\n",
      "Epoch 164/200, Train Loss: 5.3633, Val Loss: 8.1132\n",
      "Epoch 165/200, Train Loss: 5.4832, Val Loss: 8.4565\n",
      "Epoch 166/200, Train Loss: 5.4207, Val Loss: 8.5744\n",
      "Epoch 167/200, Train Loss: 5.4235, Val Loss: 8.0724\n",
      "Epoch 168/200, Train Loss: 5.3387, Val Loss: 8.1494\n",
      "Epoch 169/200, Train Loss: 5.5595, Val Loss: 8.2812\n",
      "Epoch 170/200, Train Loss: 5.3619, Val Loss: 8.2734\n",
      "Epoch 171/200, Train Loss: 5.3356, Val Loss: 8.0945\n",
      "Epoch 172/200, Train Loss: 5.3855, Val Loss: 8.2582\n",
      "Epoch 173/200, Train Loss: 5.3539, Val Loss: 8.0290\n",
      "Epoch 174/200, Train Loss: 5.3058, Val Loss: 8.1548\n",
      "Epoch 175/200, Train Loss: 5.2882, Val Loss: 8.3367\n",
      "Epoch 176/200, Train Loss: 5.2182, Val Loss: 8.4675\n",
      "Epoch 177/200, Train Loss: 5.2500, Val Loss: 8.4594\n",
      "Epoch 178/200, Train Loss: 5.2513, Val Loss: 8.5199\n",
      "Epoch 179/200, Train Loss: 5.1706, Val Loss: 8.5087\n",
      "Epoch 180/200, Train Loss: 5.1374, Val Loss: 8.3589\n",
      "Epoch 181/200, Train Loss: 5.1636, Val Loss: 8.5159\n",
      "Epoch 182/200, Train Loss: 5.1108, Val Loss: 8.2559\n",
      "Epoch 183/200, Train Loss: 5.1473, Val Loss: 8.4351\n",
      "Epoch 184/200, Train Loss: 5.2232, Val Loss: 8.5460\n",
      "Epoch 185/200, Train Loss: 5.1380, Val Loss: 8.4681\n",
      "Epoch 186/200, Train Loss: 5.1113, Val Loss: 8.4164\n",
      "Epoch 187/200, Train Loss: 5.1133, Val Loss: 8.5056\n",
      "Epoch 188/200, Train Loss: 4.9947, Val Loss: 8.3077\n",
      "Epoch 189/200, Train Loss: 5.1503, Val Loss: 8.3668\n",
      "Epoch 190/200, Train Loss: 5.1190, Val Loss: 8.3788\n",
      "Epoch 191/200, Train Loss: 5.1173, Val Loss: 8.5488\n",
      "Epoch 192/200, Train Loss: 5.0401, Val Loss: 8.3207\n",
      "Epoch 193/200, Train Loss: 5.0160, Val Loss: 8.5778\n",
      "Epoch 194/200, Train Loss: 5.1553, Val Loss: 8.5034\n",
      "Epoch 195/200, Train Loss: 5.0132, Val Loss: 8.5069\n",
      "Epoch 196/200, Train Loss: 5.0072, Val Loss: 8.4609\n",
      "Epoch 197/200, Train Loss: 5.0322, Val Loss: 8.2675\n",
      "Epoch 198/200, Train Loss: 4.9964, Val Loss: 8.2572\n",
      "Epoch 199/200, Train Loss: 4.9658, Val Loss: 8.5492\n",
      "Epoch 200/200, Train Loss: 5.0513, Val Loss: 8.5994\n",
      "\n",
      "Loaded best model (Val Loss: 7.8323) for final hidden state extraction.\n",
      "Saved best model for NMRNN_NoSpatial_ModReadout to results/20250508_090642/NMRNN_NoSpatial_ModReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_NoSpatial_ModReadout ---\n",
      "  Analyzing decodability for NMRNN_NoSpatial_ModReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 10])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_NoSpatial_ModReadout - Test MSE: 1.1489, Test R2: 0.4031 (best alpha: 48.3293)\n",
      "Decodability (R2 score) for NMRNN_NoSpatial_ModReadout: 0.4031\n",
      "\n",
      "--- Training NMRNN_Spatial_FixedReadout ---\n",
      "Number of parameters: 66449\n",
      "Epoch 1/200, Train Loss: 11.8307, Val Loss: 10.6832\n",
      "  New best validation loss: 10.6832\n",
      "Epoch 2/200, Train Loss: 11.6829, Val Loss: 10.6117\n",
      "  New best validation loss: 10.6117\n",
      "Epoch 3/200, Train Loss: 11.6694, Val Loss: 10.6058\n",
      "  New best validation loss: 10.6058\n",
      "Epoch 4/200, Train Loss: 11.6348, Val Loss: 10.5904\n",
      "  New best validation loss: 10.5904\n",
      "Epoch 5/200, Train Loss: 11.5767, Val Loss: 10.5303\n",
      "  New best validation loss: 10.5303\n",
      "Epoch 6/200, Train Loss: 11.4937, Val Loss: 10.4616\n",
      "  New best validation loss: 10.4616\n",
      "Epoch 7/200, Train Loss: 11.3958, Val Loss: 10.4591\n",
      "  New best validation loss: 10.4591\n",
      "Epoch 8/200, Train Loss: 11.3424, Val Loss: 10.4205\n",
      "  New best validation loss: 10.4205\n",
      "Epoch 9/200, Train Loss: 11.2679, Val Loss: 10.3899\n",
      "  New best validation loss: 10.3899\n",
      "Epoch 10/200, Train Loss: 11.2215, Val Loss: 10.3538\n",
      "  New best validation loss: 10.3538\n",
      "Epoch 11/200, Train Loss: 11.1738, Val Loss: 10.3619\n",
      "Epoch 12/200, Train Loss: 11.0798, Val Loss: 10.2756\n",
      "  New best validation loss: 10.2756\n",
      "Epoch 13/200, Train Loss: 11.0207, Val Loss: 10.2568\n",
      "  New best validation loss: 10.2568\n",
      "Epoch 14/200, Train Loss: 10.9807, Val Loss: 10.2806\n",
      "Epoch 15/200, Train Loss: 10.9203, Val Loss: 10.1549\n",
      "  New best validation loss: 10.1549\n",
      "Epoch 16/200, Train Loss: 10.8106, Val Loss: 10.1367\n",
      "  New best validation loss: 10.1367\n",
      "Epoch 17/200, Train Loss: 10.7829, Val Loss: 10.1723\n",
      "Epoch 18/200, Train Loss: 10.7044, Val Loss: 10.0313\n",
      "  New best validation loss: 10.0313\n",
      "Epoch 19/200, Train Loss: 10.6825, Val Loss: 10.0210\n",
      "  New best validation loss: 10.0210\n",
      "Epoch 20/200, Train Loss: 10.5719, Val Loss: 10.0130\n",
      "  New best validation loss: 10.0130\n",
      "Epoch 21/200, Train Loss: 10.5225, Val Loss: 10.0451\n",
      "Epoch 22/200, Train Loss: 10.4412, Val Loss: 10.0620\n",
      "Epoch 23/200, Train Loss: 10.3962, Val Loss: 9.9171\n",
      "  New best validation loss: 9.9171\n",
      "Epoch 24/200, Train Loss: 10.3878, Val Loss: 9.8399\n",
      "  New best validation loss: 9.8399\n",
      "Epoch 25/200, Train Loss: 10.3070, Val Loss: 9.8659\n",
      "Epoch 26/200, Train Loss: 10.3117, Val Loss: 9.8865\n",
      "Epoch 27/200, Train Loss: 10.2748, Val Loss: 9.7829\n",
      "  New best validation loss: 9.7829\n",
      "Epoch 28/200, Train Loss: 10.1632, Val Loss: 9.7745\n",
      "  New best validation loss: 9.7745\n",
      "Epoch 29/200, Train Loss: 10.1238, Val Loss: 9.7755\n",
      "Epoch 30/200, Train Loss: 10.0731, Val Loss: 9.7433\n",
      "  New best validation loss: 9.7433\n",
      "Epoch 31/200, Train Loss: 10.0217, Val Loss: 9.7281\n",
      "  New best validation loss: 9.7281\n",
      "Epoch 32/200, Train Loss: 10.0353, Val Loss: 9.7257\n",
      "  New best validation loss: 9.7257\n",
      "Epoch 33/200, Train Loss: 9.9884, Val Loss: 9.6723\n",
      "  New best validation loss: 9.6723\n",
      "Epoch 34/200, Train Loss: 9.9016, Val Loss: 9.6817\n",
      "Epoch 35/200, Train Loss: 9.8185, Val Loss: 9.6077\n",
      "  New best validation loss: 9.6077\n",
      "Epoch 36/200, Train Loss: 9.8837, Val Loss: 9.6077\n",
      "  New best validation loss: 9.6077\n",
      "Epoch 37/200, Train Loss: 9.8015, Val Loss: 9.6157\n",
      "Epoch 38/200, Train Loss: 9.7710, Val Loss: 9.7809\n",
      "Epoch 39/200, Train Loss: 9.7093, Val Loss: 9.5426\n",
      "  New best validation loss: 9.5426\n",
      "Epoch 40/200, Train Loss: 9.6265, Val Loss: 9.6201\n",
      "Epoch 41/200, Train Loss: 9.5326, Val Loss: 9.5986\n",
      "Epoch 42/200, Train Loss: 9.5243, Val Loss: 9.7075\n",
      "Epoch 43/200, Train Loss: 9.5133, Val Loss: 9.6353\n",
      "Epoch 44/200, Train Loss: 9.4731, Val Loss: 9.6094\n",
      "Epoch 45/200, Train Loss: 9.3624, Val Loss: 9.5755\n",
      "Epoch 46/200, Train Loss: 9.3228, Val Loss: 9.6328\n",
      "Epoch 47/200, Train Loss: 9.2932, Val Loss: 9.5236\n",
      "  New best validation loss: 9.5236\n",
      "Epoch 48/200, Train Loss: 9.2335, Val Loss: 9.7473\n",
      "Epoch 49/200, Train Loss: 9.1929, Val Loss: 9.5870\n",
      "Epoch 50/200, Train Loss: 9.1226, Val Loss: 9.6394\n",
      "Epoch 51/200, Train Loss: 9.2555, Val Loss: 9.5740\n",
      "Epoch 52/200, Train Loss: 9.1219, Val Loss: 9.7262\n",
      "Epoch 53/200, Train Loss: 9.1509, Val Loss: 9.5381\n",
      "Epoch 54/200, Train Loss: 9.0706, Val Loss: 9.6950\n",
      "Epoch 55/200, Train Loss: 9.1112, Val Loss: 9.5857\n",
      "Epoch 56/200, Train Loss: 9.0499, Val Loss: 9.5224\n",
      "  New best validation loss: 9.5224\n",
      "Epoch 57/200, Train Loss: 8.9592, Val Loss: 9.6967\n",
      "Epoch 58/200, Train Loss: 8.8838, Val Loss: 9.7113\n",
      "Epoch 59/200, Train Loss: 8.9294, Val Loss: 9.6737\n",
      "Epoch 60/200, Train Loss: 8.8398, Val Loss: 9.6912\n",
      "Epoch 61/200, Train Loss: 8.7567, Val Loss: 9.6824\n",
      "Epoch 62/200, Train Loss: 8.6950, Val Loss: 9.8065\n",
      "Epoch 63/200, Train Loss: 8.7245, Val Loss: 9.7040\n",
      "Epoch 64/200, Train Loss: 8.5600, Val Loss: 9.7116\n",
      "Epoch 65/200, Train Loss: 8.5340, Val Loss: 9.6192\n",
      "Epoch 66/200, Train Loss: 8.4454, Val Loss: 9.7230\n",
      "Epoch 67/200, Train Loss: 8.4847, Val Loss: 9.7064\n",
      "Epoch 68/200, Train Loss: 8.3971, Val Loss: 9.8033\n",
      "Epoch 69/200, Train Loss: 8.2911, Val Loss: 9.9124\n",
      "Epoch 70/200, Train Loss: 8.2470, Val Loss: 9.7351\n",
      "Epoch 71/200, Train Loss: 8.2520, Val Loss: 9.7002\n",
      "Epoch 72/200, Train Loss: 8.2179, Val Loss: 9.8737\n",
      "Epoch 73/200, Train Loss: 8.1985, Val Loss: 9.6887\n",
      "Epoch 74/200, Train Loss: 8.1392, Val Loss: 9.9711\n",
      "Epoch 75/200, Train Loss: 8.0664, Val Loss: 9.9122\n",
      "Epoch 76/200, Train Loss: 7.9853, Val Loss: 10.1054\n",
      "Epoch 77/200, Train Loss: 7.9256, Val Loss: 9.9540\n",
      "Epoch 78/200, Train Loss: 8.0422, Val Loss: 10.0414\n",
      "Epoch 79/200, Train Loss: 7.8804, Val Loss: 9.9451\n",
      "Epoch 80/200, Train Loss: 7.7768, Val Loss: 9.9956\n",
      "Epoch 81/200, Train Loss: 7.7599, Val Loss: 9.8936\n",
      "Epoch 82/200, Train Loss: 7.8531, Val Loss: 10.2820\n",
      "Epoch 83/200, Train Loss: 7.8213, Val Loss: 10.1094\n",
      "Epoch 84/200, Train Loss: 7.7609, Val Loss: 10.0794\n",
      "Epoch 85/200, Train Loss: 7.8638, Val Loss: 9.9194\n",
      "Epoch 86/200, Train Loss: 7.6330, Val Loss: 10.1045\n",
      "Epoch 87/200, Train Loss: 7.6544, Val Loss: 10.0449\n",
      "Epoch 88/200, Train Loss: 7.6096, Val Loss: 9.9773\n",
      "Epoch 89/200, Train Loss: 7.8308, Val Loss: 10.2489\n",
      "Epoch 90/200, Train Loss: 7.7156, Val Loss: 9.9458\n",
      "Epoch 91/200, Train Loss: 7.4917, Val Loss: 10.2206\n",
      "Epoch 92/200, Train Loss: 7.4184, Val Loss: 10.1277\n",
      "Epoch 93/200, Train Loss: 7.3792, Val Loss: 10.0747\n",
      "Epoch 94/200, Train Loss: 7.5131, Val Loss: 10.5242\n",
      "Epoch 95/200, Train Loss: 7.4875, Val Loss: 10.2379\n",
      "Epoch 96/200, Train Loss: 7.4258, Val Loss: 10.1840\n",
      "Epoch 97/200, Train Loss: 7.2322, Val Loss: 10.3887\n",
      "Epoch 98/200, Train Loss: 7.1745, Val Loss: 10.1781\n",
      "Epoch 99/200, Train Loss: 7.1784, Val Loss: 10.2729\n",
      "Epoch 100/200, Train Loss: 7.1208, Val Loss: 10.3005\n",
      "Epoch 101/200, Train Loss: 7.1011, Val Loss: 10.3696\n",
      "Epoch 102/200, Train Loss: 7.1925, Val Loss: 10.4010\n",
      "Epoch 103/200, Train Loss: 7.0759, Val Loss: 10.5036\n",
      "Epoch 104/200, Train Loss: 7.0624, Val Loss: 10.3281\n",
      "Epoch 105/200, Train Loss: 7.0942, Val Loss: 10.3296\n",
      "Epoch 106/200, Train Loss: 7.0271, Val Loss: 10.2540\n",
      "Epoch 107/200, Train Loss: 7.0434, Val Loss: 10.4240\n",
      "Epoch 108/200, Train Loss: 7.1220, Val Loss: 10.5684\n",
      "Epoch 109/200, Train Loss: 7.0951, Val Loss: 10.4039\n",
      "Epoch 110/200, Train Loss: 7.0780, Val Loss: 10.3749\n",
      "Epoch 111/200, Train Loss: 6.9506, Val Loss: 10.4968\n",
      "Epoch 112/200, Train Loss: 6.8909, Val Loss: 10.6115\n",
      "Epoch 113/200, Train Loss: 6.8168, Val Loss: 10.6355\n",
      "Epoch 114/200, Train Loss: 6.8192, Val Loss: 10.5380\n",
      "Epoch 115/200, Train Loss: 6.7904, Val Loss: 10.4749\n",
      "Epoch 116/200, Train Loss: 6.6930, Val Loss: 10.3618\n",
      "Epoch 117/200, Train Loss: 6.6332, Val Loss: 10.8405\n",
      "Epoch 118/200, Train Loss: 6.8538, Val Loss: 10.5413\n",
      "Epoch 119/200, Train Loss: 7.0768, Val Loss: 10.4619\n",
      "Epoch 120/200, Train Loss: 6.7812, Val Loss: 10.5123\n",
      "Epoch 121/200, Train Loss: 6.6815, Val Loss: 10.3988\n",
      "Epoch 122/200, Train Loss: 6.6052, Val Loss: 10.6770\n",
      "Epoch 123/200, Train Loss: 6.5537, Val Loss: 10.5620\n",
      "Epoch 124/200, Train Loss: 6.5370, Val Loss: 10.7270\n",
      "Epoch 125/200, Train Loss: 6.5711, Val Loss: 10.6935\n",
      "Epoch 126/200, Train Loss: 6.4974, Val Loss: 10.3648\n",
      "Epoch 127/200, Train Loss: 6.5247, Val Loss: 10.7576\n",
      "Epoch 128/200, Train Loss: 6.4261, Val Loss: 10.3838\n",
      "Epoch 129/200, Train Loss: 6.3706, Val Loss: 10.7783\n",
      "Epoch 130/200, Train Loss: 6.4042, Val Loss: 10.7333\n",
      "Epoch 131/200, Train Loss: 6.4824, Val Loss: 10.9185\n",
      "Epoch 132/200, Train Loss: 6.3356, Val Loss: 10.6777\n",
      "Epoch 133/200, Train Loss: 6.2398, Val Loss: 10.6548\n",
      "Epoch 134/200, Train Loss: 6.1979, Val Loss: 10.7454\n",
      "Epoch 135/200, Train Loss: 6.2479, Val Loss: 10.8268\n",
      "Epoch 136/200, Train Loss: 6.2917, Val Loss: 10.9193\n",
      "Epoch 137/200, Train Loss: 6.3469, Val Loss: 10.8609\n",
      "Epoch 138/200, Train Loss: 6.5808, Val Loss: 10.7704\n",
      "Epoch 139/200, Train Loss: 6.4208, Val Loss: 10.6702\n",
      "Epoch 140/200, Train Loss: 6.2619, Val Loss: 10.5807\n",
      "Epoch 141/200, Train Loss: 6.0835, Val Loss: 10.6363\n",
      "Epoch 142/200, Train Loss: 6.0640, Val Loss: 11.0102\n",
      "Epoch 143/200, Train Loss: 6.1719, Val Loss: 10.4532\n",
      "Epoch 144/200, Train Loss: 6.0688, Val Loss: 10.9402\n",
      "Epoch 145/200, Train Loss: 6.0233, Val Loss: 10.7420\n",
      "Epoch 146/200, Train Loss: 6.0872, Val Loss: 10.6896\n",
      "Epoch 147/200, Train Loss: 6.0327, Val Loss: 10.8183\n",
      "Epoch 148/200, Train Loss: 6.0297, Val Loss: 10.6351\n",
      "Epoch 149/200, Train Loss: 5.9034, Val Loss: 10.5966\n",
      "Epoch 150/200, Train Loss: 5.9091, Val Loss: 11.1514\n",
      "Epoch 151/200, Train Loss: 5.9657, Val Loss: 11.1268\n",
      "Epoch 152/200, Train Loss: 6.0309, Val Loss: 10.9048\n",
      "Epoch 153/200, Train Loss: 5.9761, Val Loss: 11.4167\n",
      "Epoch 154/200, Train Loss: 5.9012, Val Loss: 10.7783\n",
      "Epoch 155/200, Train Loss: 5.8630, Val Loss: 10.8256\n",
      "Epoch 156/200, Train Loss: 5.8402, Val Loss: 11.0032\n",
      "Epoch 157/200, Train Loss: 5.7724, Val Loss: 10.9178\n",
      "Epoch 158/200, Train Loss: 5.9526, Val Loss: 10.6834\n",
      "Epoch 159/200, Train Loss: 5.8603, Val Loss: 11.0328\n",
      "Epoch 160/200, Train Loss: 5.8690, Val Loss: 10.9419\n",
      "Epoch 161/200, Train Loss: 5.9644, Val Loss: 10.8571\n",
      "Epoch 162/200, Train Loss: 5.9961, Val Loss: 10.9665\n",
      "Epoch 163/200, Train Loss: 5.8093, Val Loss: 10.7171\n",
      "Epoch 164/200, Train Loss: 5.9390, Val Loss: 11.2809\n",
      "Epoch 165/200, Train Loss: 5.7800, Val Loss: 10.9001\n",
      "Epoch 166/200, Train Loss: 5.7628, Val Loss: 11.0077\n",
      "Epoch 167/200, Train Loss: 5.6558, Val Loss: 10.9332\n",
      "Epoch 168/200, Train Loss: 5.9081, Val Loss: 11.2248\n",
      "Epoch 169/200, Train Loss: 5.7577, Val Loss: 10.8419\n",
      "Epoch 170/200, Train Loss: 5.8593, Val Loss: 10.7733\n",
      "Epoch 171/200, Train Loss: 5.7108, Val Loss: 11.3077\n",
      "Epoch 172/200, Train Loss: 5.7207, Val Loss: 10.9895\n",
      "Epoch 173/200, Train Loss: 5.6422, Val Loss: 10.9621\n",
      "Epoch 174/200, Train Loss: 5.5184, Val Loss: 11.0451\n",
      "Epoch 175/200, Train Loss: 5.6431, Val Loss: 10.9736\n",
      "Epoch 176/200, Train Loss: 5.6019, Val Loss: 11.0286\n",
      "Epoch 177/200, Train Loss: 5.4254, Val Loss: 10.8615\n",
      "Epoch 178/200, Train Loss: 5.6277, Val Loss: 11.3358\n",
      "Epoch 179/200, Train Loss: 5.5034, Val Loss: 10.9975\n",
      "Epoch 180/200, Train Loss: 5.5335, Val Loss: 11.1163\n",
      "Epoch 181/200, Train Loss: 5.4895, Val Loss: 10.8990\n",
      "Epoch 182/200, Train Loss: 5.3792, Val Loss: 11.0232\n",
      "Epoch 183/200, Train Loss: 5.3810, Val Loss: 11.1100\n",
      "Epoch 184/200, Train Loss: 5.3465, Val Loss: 11.2914\n",
      "Epoch 185/200, Train Loss: 5.4191, Val Loss: 10.7239\n",
      "Epoch 186/200, Train Loss: 5.3558, Val Loss: 11.3953\n",
      "Epoch 187/200, Train Loss: 5.3114, Val Loss: 11.1408\n",
      "Epoch 188/200, Train Loss: 5.3304, Val Loss: 11.7404\n",
      "Epoch 189/200, Train Loss: 5.2879, Val Loss: 11.3938\n",
      "Epoch 190/200, Train Loss: 5.2899, Val Loss: 11.1578\n",
      "Epoch 191/200, Train Loss: 5.2910, Val Loss: 10.9924\n",
      "Epoch 192/200, Train Loss: 5.2285, Val Loss: 10.9679\n",
      "Epoch 193/200, Train Loss: 5.2407, Val Loss: 11.3101\n",
      "Epoch 194/200, Train Loss: 5.1950, Val Loss: 11.3743\n",
      "Epoch 195/200, Train Loss: 5.2541, Val Loss: 11.3540\n",
      "Epoch 196/200, Train Loss: 5.2318, Val Loss: 11.5403\n",
      "Epoch 197/200, Train Loss: 5.2169, Val Loss: 11.4283\n",
      "Epoch 198/200, Train Loss: 5.2610, Val Loss: 11.7627\n",
      "Epoch 199/200, Train Loss: 5.2692, Val Loss: 11.1892\n",
      "Epoch 200/200, Train Loss: 5.1159, Val Loss: 11.4931\n",
      "\n",
      "Loaded best model (Val Loss: 9.5224) for final hidden state extraction.\n",
      "Saved best model for NMRNN_Spatial_FixedReadout to results/20250508_090642/NMRNN_Spatial_FixedReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_Spatial_FixedReadout ---\n",
      "  Analyzing decodability for NMRNN_Spatial_FixedReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 10])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_Spatial_FixedReadout - Test MSE: 1.5930, Test R2: 0.1639 (best alpha: 100.0000)\n",
      "Decodability (R2 score) for NMRNN_Spatial_FixedReadout: 0.1639\n",
      "Learning curves saved to results/20250508_090642/learning_curves.png\n",
      "\n",
      "Learning curves plotted to results/20250508_090642/learning_curves.png\n",
      "\n",
      "--- Decodability Results (R2 Score) ---\n",
      "ComplexOscillatorNet: 0.4856\n",
      "RNN_GRU: 0.3344\n",
      "Transformer: 0.5688\n",
      "HIPPORNN_LegT: 0.5609\n",
      "NMRNN_Spatial_ModReadout: 0.3681\n",
      "NMRNN_NoSpatial_ModReadout: 0.4031\n",
      "NMRNN_Spatial_FixedReadout: 0.1639\n",
      "\n",
      "Experiment finished. All results in results/20250508_090642\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Project specific imports\n",
    "from dataset import CompositionalDataset, create_dataloaders\n",
    "from model import NonlinearOscillatorNet, RNNModel, TransformerModel, HippoRNNModel, NMRNN_Spatial_ModulatedReadout, NMRNN_NoSpatial_ModulatedReadout, NMRNN_Spatial_FixedReadout\n",
    "from training import train_model_comparative\n",
    "from analysis import plot_learning_curves, perform_decodability_analysis\n",
    "from utils import set_seed, get_device, count_parameters\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"seed\": 0,\n",
    "    \"num_task_coefficients\": 10, \n",
    "    \"seq_length\": 200,          \n",
    "    \"train_samples\": 32 * 10,  # Reduced for quicker testing, increase for real runs\n",
    "    \"test_samples\": 32 * 5,   # Reduced for quicker testing\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200, # Reduced for quick test, increase for real runs (e.g., 50-200)\n",
    "    \"lr\": 1e-3,\n",
    "    \n",
    "    # Model-specific hidden sizes / main dimension\n",
    "    \"hidden_size_oscillator\": 64, \n",
    "    \"hidden_size_rnn\": 128,       # For standard GRU/LSTM\n",
    "    \"d_model_transformer\": 64,   \n",
    "    \"hidden_size_hippo\": 128,     # N for HIPPO\n",
    "    \"hidden_size_nm_rnn\": 128,    # n_rnn for nmRNN variants\n",
    "\n",
    "    # Transformer specific\n",
    "    \"nhead_transformer\": 1,\n",
    "    \"num_layers_transformer\": 1,\n",
    "    \n",
    "    # HIPPORNN specific\n",
    "    \"hippo_method\": 'legt', # 'legs' or 'legt'\n",
    "    \"hippo_theta\": 1.0,     # Required for 'legt'\n",
    "    \"hippo_dt\": 1.0 / 200,  # Discretization step for HIPPO (e.g., 1.0 / seq_length)\n",
    "    \"hippo_inv_eps\": 1e-6, # Epsilon for LegS matrix inversion regularization\n",
    "    \"hippo_clip_val\": 50.0, # Clipping for HIPPO state c_t\n",
    "\n",
    "    # nmRNN specific (shared for variants where applicable)\n",
    "    \"nm_N_NM\": 4,               # Number of neuromodulators\n",
    "    \"nm_activation\": 'tanh',    # 'relu', 'tanh' (original code had 'relu-tanh', simplified here)\n",
    "    \"nm_decay\": 0.05, # dt_sec / tau_rnn, e.g., (20ms/step) / (100ms tau) -> exp(-0.2)\n",
    "                                     # Original: math.exp(-20/100) - assuming 20ms step, 100ms tau\n",
    "    \"nm_bias\": True,\n",
    "    \"nm_keepW0_spatial\": False, # For the version with spatial connections\n",
    "    \"nm_keepW0_no_spatial\": False,\n",
    "    \"nm_grad_clip\": 1.0,\n",
    "    \"nm_spatial_ell\": 0.1,      # For SpatialWeight\n",
    "    \"nm_spatial_scale\": 1.0,    # For SpatialWeight\n",
    "\n",
    "    # General task params\n",
    "    \"output_dim\": 1, \n",
    "    \"input_dim\": 1,  \n",
    "    \"noise_level_data\": 0.01,\n",
    "    \"run_timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"results_dir\": \"results\"\n",
    "}\n",
    "\n",
    "def run_experiment():\n",
    "    \"\"\"\n",
    "    Runs the full comparative analysis experiment.\n",
    "    \"\"\"\n",
    "    set_seed(CONFIG[\"seed\"])\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    os.makedirs(CONFIG[\"results_dir\"], exist_ok=True)\n",
    "    run_results_dir = os.path.join(CONFIG[\"results_dir\"], CONFIG[\"run_timestamp\"])\n",
    "    os.makedirs(run_results_dir, exist_ok=True)\n",
    "    print(f\"Results will be saved in: {run_results_dir}\")\n",
    "\n",
    "    # --- 1. Dataset ---\n",
    "    print(\"Loading dataset...\")\n",
    "    train_loader, val_loader, test_loader, (input_basis, output_basis) = create_dataloaders(\n",
    "        num_train_samples=CONFIG[\"train_samples\"],\n",
    "        num_val_samples=CONFIG[\"test_samples\"], \n",
    "        num_test_samples=CONFIG[\"test_samples\"],\n",
    "        num_basis=CONFIG[\"num_task_coefficients\"],\n",
    "        seq_length=CONFIG[\"seq_length\"],\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        noise=CONFIG[\"noise_level_data\"]\n",
    "    )\n",
    "    print(\"Dataset loaded.\")\n",
    "\n",
    "    # --- 2. Models ---\n",
    "    models_to_test = {\n",
    "        \"ComplexOscillatorNet\": NonlinearOscillatorNet(\n",
    "            N_oscillators=CONFIG[\"hidden_size_oscillator\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            seq_length=CONFIG[\"seq_length\"], \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"RNN_GRU\": RNNModel(\n",
    "            hidden_size=CONFIG[\"hidden_size_rnn\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            num_layers=1, \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"Transformer\": TransformerModel(\n",
    "            d_model=CONFIG[\"d_model_transformer\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            num_heads=CONFIG[\"nhead_transformer\"],\n",
    "            num_layers=CONFIG[\"num_layers_transformer\"],\n",
    "            seq_length=CONFIG[\"seq_length\"], \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"HIPPORNN_LegT\": HippoRNNModel( # Using LegT by default as per config\n",
    "            hidden_size=CONFIG[\"hidden_size_hippo\"],\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            method=CONFIG[\"hippo_method\"], \n",
    "            theta=CONFIG[\"hippo_theta\"],\n",
    "            dt=CONFIG[\"hippo_dt\"],\n",
    "            inv_eps=CONFIG[\"hippo_inv_eps\"],\n",
    "            clip_val=CONFIG[\"hippo_clip_val\"],\n",
    "            device=device, # Pass device\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_Spatial_ModReadout\": NMRNN_Spatial_ModulatedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"],\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_spatial\"],\n",
    "            spatial_ell=CONFIG[\"nm_spatial_ell\"],\n",
    "            spatial_scale=CONFIG[\"nm_spatial_scale\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_NoSpatial_ModReadout\": NMRNN_NoSpatial_ModulatedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"],\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_no_spatial\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_Spatial_FixedReadout\": NMRNN_Spatial_FixedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"], # N_nm still needed for the core recurrence, just not readout\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_spatial\"],\n",
    "            spatial_ell=CONFIG[\"nm_spatial_ell\"],\n",
    "            spatial_scale=CONFIG[\"nm_spatial_scale\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    all_val_losses = {}\n",
    "    decodability_results = {}\n",
    "    trained_models_paths = {}\n",
    "\n",
    "    # --- 3. Training & Evaluation Loop ---\n",
    "    for model_name, model in models_to_test.items():\n",
    "        print(f\"\\n--- Training {model_name} ---\")\n",
    "        model.to(device)\n",
    "        print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "\n",
    "        try:\n",
    "            val_losses, best_model_state, hidden_states_test, coeffs_test = train_model_comparative(\n",
    "                model,\n",
    "                model_name,\n",
    "                train_loader,\n",
    "                val_loader, \n",
    "                test_loader, \n",
    "                CONFIG[\"epochs\"],\n",
    "                CONFIG[\"lr\"],\n",
    "                device,\n",
    "                CONFIG[\"num_task_coefficients\"], \n",
    "                run_results_dir,\n",
    "                plot_intermediate_results=(len(models_to_test) == 1) \n",
    "            )\n",
    "            all_val_losses[model_name] = val_losses\n",
    "            \n",
    "            if best_model_state:\n",
    "                model_path = os.path.join(run_results_dir, f\"{model_name}_best.pt\")\n",
    "                torch.save(best_model_state, model_path)\n",
    "                trained_models_paths[model_name] = model_path\n",
    "                print(f\"Saved best model for {model_name} to {model_path}\")\n",
    "            else:\n",
    "                print(f\"No best model state saved for {model_name} (possibly due to training issues).\")\n",
    "\n",
    "            # --- 4. Decodability Analysis ---\n",
    "            if hidden_states_test is not None and coeffs_test is not None:\n",
    "                print(f\"\\n--- Performing Decodability Analysis for {model_name} ---\")\n",
    "                decodability_score = perform_decodability_analysis(\n",
    "                    model_name=model_name, \n",
    "                    hidden_states=hidden_states_test, \n",
    "                    coefficients=coeffs_test,       \n",
    "                    decoder_type='ridge', # Using RidgeCV as a robust default\n",
    "                    decoding_metric='r2', # R-squared is often more interpretable than MSE here\n",
    "                    results_dir=run_results_dir,\n",
    "                    device=device,\n",
    "                )\n",
    "                decodability_results[model_name] = decodability_score\n",
    "                print(f\"Decodability (R2 score) for {model_name}: {decodability_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"Skipping decodability for {model_name} due to missing hidden states or coefficients.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"!!!!!! ERROR during training or analysis for {model_name}: {e} !!!!!!\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            all_val_losses[model_name] = [float('nan')] * CONFIG[\"epochs\"] # Log error for this model\n",
    "            decodability_results[model_name] = float('nan')\n",
    "\n",
    "\n",
    "    # --- 5. Plot Learning Curves ---\n",
    "    if any(all_val_losses.values()): # Check if there's anything to plot\n",
    "        plot_learning_curves(all_val_losses, title=\"Validation Learning Curves\", save_path=os.path.join(run_results_dir, \"learning_curves.png\"))\n",
    "        print(f\"\\nLearning curves plotted to {os.path.join(run_results_dir, 'learning_curves.png')}\")\n",
    "\n",
    "    # --- 6. Report Decodability ---\n",
    "    print(\"\\n--- Decodability Results (R2 Score) ---\")\n",
    "    if decodability_results:\n",
    "        for model_name, score in decodability_results.items():\n",
    "            print(f\"{model_name}: {score:.4f}\")\n",
    "        with open(os.path.join(run_results_dir, \"decodability_summary.txt\"), \"w\") as f:\n",
    "            f.write(\"Model,R2_Score\\n\")\n",
    "            for model_name, score in decodability_results.items():\n",
    "                f.write(f\"{model_name},{score:.4f}\\n\")\n",
    "    else:\n",
    "        print(\"No decodability results to report.\")\n",
    "        \n",
    "    print(f\"\\nExperiment finished. All results in {run_results_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ec43da-6ade-4862-81be-ed723ffe257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951229424500714"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-1.0 / 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba850af2-8fe7-4ac3-a31b-87c844aef3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
