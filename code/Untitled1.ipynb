{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cfca335-29cc-423c-a5a1-16b167672c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Results will be saved in: results/20250508_043138\n",
      "Loading dataset...\n",
      "Dataset loaded.\n",
      "\n",
      "--- Training ComplexOscillatorNet ---\n",
      "Number of parameters: 4480\n",
      "Epoch 1/200, Train Loss: 4.9201, Val Loss: 4.7620\n",
      "  New best validation loss: 4.7620\n",
      "Epoch 2/200, Train Loss: 4.5789, Val Loss: 4.4091\n",
      "  New best validation loss: 4.4091\n",
      "Epoch 3/200, Train Loss: 4.3282, Val Loss: 4.1753\n",
      "  New best validation loss: 4.1753\n",
      "Epoch 4/200, Train Loss: 4.0661, Val Loss: 3.9659\n",
      "  New best validation loss: 3.9659\n",
      "Epoch 5/200, Train Loss: 3.8658, Val Loss: 3.8017\n",
      "  New best validation loss: 3.8017\n",
      "Epoch 6/200, Train Loss: 3.7076, Val Loss: 3.6618\n",
      "  New best validation loss: 3.6618\n",
      "Epoch 7/200, Train Loss: 3.5907, Val Loss: 3.5625\n",
      "  New best validation loss: 3.5625\n",
      "Epoch 8/200, Train Loss: 3.5033, Val Loss: 3.4988\n",
      "  New best validation loss: 3.4988\n",
      "Epoch 9/200, Train Loss: 3.4445, Val Loss: 3.4559\n",
      "  New best validation loss: 3.4559\n",
      "Epoch 10/200, Train Loss: 3.3918, Val Loss: 3.4240\n",
      "  New best validation loss: 3.4240\n",
      "Epoch 11/200, Train Loss: 3.3591, Val Loss: 3.3929\n",
      "  New best validation loss: 3.3929\n",
      "Epoch 12/200, Train Loss: 3.3200, Val Loss: 3.3670\n",
      "  New best validation loss: 3.3670\n",
      "Epoch 13/200, Train Loss: 3.2859, Val Loss: 3.3369\n",
      "  New best validation loss: 3.3369\n",
      "Epoch 14/200, Train Loss: 3.2548, Val Loss: 3.3249\n",
      "  New best validation loss: 3.3249\n",
      "Epoch 15/200, Train Loss: 3.2252, Val Loss: 3.3029\n",
      "  New best validation loss: 3.3029\n",
      "Epoch 16/200, Train Loss: 3.2038, Val Loss: 3.2829\n",
      "  New best validation loss: 3.2829\n",
      "Epoch 17/200, Train Loss: 3.1829, Val Loss: 3.2637\n",
      "  New best validation loss: 3.2637\n",
      "Epoch 18/200, Train Loss: 3.1683, Val Loss: 3.2503\n",
      "  New best validation loss: 3.2503\n",
      "Epoch 19/200, Train Loss: 3.1526, Val Loss: 3.2527\n",
      "Epoch 20/200, Train Loss: 3.1344, Val Loss: 3.2244\n",
      "  New best validation loss: 3.2244\n",
      "Epoch 21/200, Train Loss: 3.1222, Val Loss: 3.2162\n",
      "  New best validation loss: 3.2162\n",
      "Epoch 22/200, Train Loss: 3.1127, Val Loss: 3.2097\n",
      "  New best validation loss: 3.2097\n",
      "Epoch 23/200, Train Loss: 3.0990, Val Loss: 3.1974\n",
      "  New best validation loss: 3.1974\n",
      "Epoch 24/200, Train Loss: 3.0880, Val Loss: 3.1916\n",
      "  New best validation loss: 3.1916\n",
      "Epoch 25/200, Train Loss: 3.0796, Val Loss: 3.1830\n",
      "  New best validation loss: 3.1830\n",
      "Epoch 26/200, Train Loss: 3.0722, Val Loss: 3.1798\n",
      "  New best validation loss: 3.1798\n",
      "Epoch 27/200, Train Loss: 3.0609, Val Loss: 3.1648\n",
      "  New best validation loss: 3.1648\n",
      "Epoch 28/200, Train Loss: 3.0537, Val Loss: 3.1605\n",
      "  New best validation loss: 3.1605\n",
      "Epoch 29/200, Train Loss: 3.0473, Val Loss: 3.1547\n",
      "  New best validation loss: 3.1547\n",
      "Epoch 30/200, Train Loss: 3.0411, Val Loss: 3.1548\n",
      "Epoch 31/200, Train Loss: 3.0285, Val Loss: 3.1424\n",
      "  New best validation loss: 3.1424\n",
      "Epoch 32/200, Train Loss: 3.0221, Val Loss: 3.1414\n",
      "  New best validation loss: 3.1414\n",
      "Epoch 33/200, Train Loss: 3.0143, Val Loss: 3.1280\n",
      "  New best validation loss: 3.1280\n",
      "Epoch 34/200, Train Loss: 3.0051, Val Loss: 3.1212\n",
      "  New best validation loss: 3.1212\n",
      "Epoch 35/200, Train Loss: 2.9973, Val Loss: 3.1180\n",
      "  New best validation loss: 3.1180\n",
      "Epoch 36/200, Train Loss: 2.9925, Val Loss: 3.1161\n",
      "  New best validation loss: 3.1161\n",
      "Epoch 37/200, Train Loss: 2.9853, Val Loss: 3.1096\n",
      "  New best validation loss: 3.1096\n",
      "Epoch 38/200, Train Loss: 2.9811, Val Loss: 3.1058\n",
      "  New best validation loss: 3.1058\n",
      "Epoch 39/200, Train Loss: 2.9749, Val Loss: 3.0975\n",
      "  New best validation loss: 3.0975\n",
      "Epoch 40/200, Train Loss: 2.9718, Val Loss: 3.0915\n",
      "  New best validation loss: 3.0915\n",
      "Epoch 41/200, Train Loss: 2.9652, Val Loss: 3.0912\n",
      "  New best validation loss: 3.0912\n",
      "Epoch 42/200, Train Loss: 2.9636, Val Loss: 3.0828\n",
      "  New best validation loss: 3.0828\n",
      "Epoch 43/200, Train Loss: 2.9553, Val Loss: 3.0812\n",
      "  New best validation loss: 3.0812\n",
      "Epoch 44/200, Train Loss: 2.9524, Val Loss: 3.0761\n",
      "  New best validation loss: 3.0761\n",
      "Epoch 45/200, Train Loss: 2.9533, Val Loss: 3.0762\n",
      "Epoch 46/200, Train Loss: 2.9443, Val Loss: 3.0717\n",
      "  New best validation loss: 3.0717\n",
      "Epoch 47/200, Train Loss: 2.9444, Val Loss: 3.0649\n",
      "  New best validation loss: 3.0649\n",
      "Epoch 48/200, Train Loss: 2.9384, Val Loss: 3.0687\n",
      "Epoch 49/200, Train Loss: 2.9332, Val Loss: 3.0638\n",
      "  New best validation loss: 3.0638\n",
      "Epoch 50/200, Train Loss: 2.9295, Val Loss: 3.0577\n",
      "  New best validation loss: 3.0577\n",
      "Epoch 51/200, Train Loss: 2.9243, Val Loss: 3.0577\n",
      "  New best validation loss: 3.0577\n",
      "Epoch 52/200, Train Loss: 2.9237, Val Loss: 3.0549\n",
      "  New best validation loss: 3.0549\n",
      "Epoch 53/200, Train Loss: 2.9202, Val Loss: 3.0481\n",
      "  New best validation loss: 3.0481\n",
      "Epoch 54/200, Train Loss: 2.9152, Val Loss: 3.0540\n",
      "Epoch 55/200, Train Loss: 2.9139, Val Loss: 3.0547\n",
      "Epoch 56/200, Train Loss: 2.9112, Val Loss: 3.0484\n",
      "Epoch 57/200, Train Loss: 2.9075, Val Loss: 3.0553\n",
      "Epoch 58/200, Train Loss: 2.9074, Val Loss: 3.0453\n",
      "  New best validation loss: 3.0453\n",
      "Epoch 59/200, Train Loss: 2.9017, Val Loss: 3.0495\n",
      "Epoch 60/200, Train Loss: 2.9004, Val Loss: 3.0392\n",
      "  New best validation loss: 3.0392\n",
      "Epoch 61/200, Train Loss: 2.8962, Val Loss: 3.0464\n",
      "Epoch 62/200, Train Loss: 2.8972, Val Loss: 3.0347\n",
      "  New best validation loss: 3.0347\n",
      "Epoch 63/200, Train Loss: 2.8954, Val Loss: 3.0349\n",
      "Epoch 64/200, Train Loss: 2.8937, Val Loss: 3.0320\n",
      "  New best validation loss: 3.0320\n",
      "Epoch 65/200, Train Loss: 2.8872, Val Loss: 3.0309\n",
      "  New best validation loss: 3.0309\n",
      "Epoch 66/200, Train Loss: 2.8823, Val Loss: 3.0261\n",
      "  New best validation loss: 3.0261\n",
      "Epoch 67/200, Train Loss: 2.8784, Val Loss: 3.0196\n",
      "  New best validation loss: 3.0196\n",
      "Epoch 68/200, Train Loss: 2.8799, Val Loss: 3.0272\n",
      "Epoch 69/200, Train Loss: 2.8812, Val Loss: 3.0308\n",
      "Epoch 70/200, Train Loss: 2.8794, Val Loss: 3.0282\n",
      "Epoch 71/200, Train Loss: 2.8757, Val Loss: 3.0209\n",
      "Epoch 72/200, Train Loss: 2.8712, Val Loss: 3.0196\n",
      "Epoch 73/200, Train Loss: 2.8724, Val Loss: 3.0093\n",
      "  New best validation loss: 3.0093\n",
      "Epoch 74/200, Train Loss: 2.8722, Val Loss: 3.0315\n",
      "Epoch 75/200, Train Loss: 2.8765, Val Loss: 3.0196\n",
      "Epoch 76/200, Train Loss: 2.8665, Val Loss: 3.0165\n",
      "Epoch 77/200, Train Loss: 2.8677, Val Loss: 3.0153\n",
      "Epoch 78/200, Train Loss: 2.8640, Val Loss: 3.0086\n",
      "  New best validation loss: 3.0086\n",
      "Epoch 79/200, Train Loss: 2.8599, Val Loss: 3.0116\n",
      "Epoch 80/200, Train Loss: 2.8584, Val Loss: 3.0149\n",
      "Epoch 81/200, Train Loss: 2.8570, Val Loss: 3.0188\n",
      "Epoch 82/200, Train Loss: 2.8550, Val Loss: 3.0106\n",
      "Epoch 83/200, Train Loss: 2.8512, Val Loss: 3.0065\n",
      "  New best validation loss: 3.0065\n",
      "Epoch 84/200, Train Loss: 2.8502, Val Loss: 3.0114\n",
      "Epoch 85/200, Train Loss: 2.8513, Val Loss: 3.0002\n",
      "  New best validation loss: 3.0002\n",
      "Epoch 86/200, Train Loss: 2.8513, Val Loss: 3.0111\n",
      "Epoch 87/200, Train Loss: 2.8464, Val Loss: 2.9976\n",
      "  New best validation loss: 2.9976\n",
      "Epoch 88/200, Train Loss: 2.8463, Val Loss: 3.0061\n",
      "Epoch 89/200, Train Loss: 2.8474, Val Loss: 3.0015\n",
      "Epoch 90/200, Train Loss: 2.8456, Val Loss: 2.9980\n",
      "Epoch 91/200, Train Loss: 2.8397, Val Loss: 3.0026\n",
      "Epoch 92/200, Train Loss: 2.8391, Val Loss: 2.9956\n",
      "  New best validation loss: 2.9956\n",
      "Epoch 93/200, Train Loss: 2.8369, Val Loss: 3.0068\n",
      "Epoch 94/200, Train Loss: 2.8355, Val Loss: 2.9937\n",
      "  New best validation loss: 2.9937\n",
      "Epoch 95/200, Train Loss: 2.8337, Val Loss: 2.9892\n",
      "  New best validation loss: 2.9892\n",
      "Epoch 96/200, Train Loss: 2.8335, Val Loss: 2.9934\n",
      "Epoch 97/200, Train Loss: 2.8385, Val Loss: 2.9988\n",
      "Epoch 98/200, Train Loss: 2.8354, Val Loss: 2.9921\n",
      "Epoch 99/200, Train Loss: 2.8335, Val Loss: 3.0028\n",
      "Epoch 100/200, Train Loss: 2.8307, Val Loss: 2.9957\n",
      "Epoch 101/200, Train Loss: 2.8301, Val Loss: 2.9938\n",
      "Epoch 102/200, Train Loss: 2.8304, Val Loss: 3.0002\n",
      "Epoch 103/200, Train Loss: 2.8301, Val Loss: 2.9992\n",
      "Epoch 104/200, Train Loss: 2.8272, Val Loss: 2.9937\n",
      "Epoch 105/200, Train Loss: 2.8275, Val Loss: 2.9979\n",
      "Epoch 106/200, Train Loss: 2.8289, Val Loss: 2.9941\n",
      "Epoch 107/200, Train Loss: 2.8314, Val Loss: 3.0030\n",
      "Epoch 108/200, Train Loss: 2.8391, Val Loss: 3.0062\n",
      "Epoch 109/200, Train Loss: 2.8396, Val Loss: 3.0040\n",
      "Epoch 110/200, Train Loss: 2.8313, Val Loss: 2.9871\n",
      "  New best validation loss: 2.9871\n",
      "Epoch 111/200, Train Loss: 2.8255, Val Loss: 2.9834\n",
      "  New best validation loss: 2.9834\n",
      "Epoch 112/200, Train Loss: 2.8215, Val Loss: 2.9841\n",
      "Epoch 113/200, Train Loss: 2.8203, Val Loss: 2.9895\n",
      "Epoch 114/200, Train Loss: 2.8208, Val Loss: 2.9813\n",
      "  New best validation loss: 2.9813\n",
      "Epoch 115/200, Train Loss: 2.8141, Val Loss: 2.9874\n",
      "Epoch 116/200, Train Loss: 2.8167, Val Loss: 2.9860\n",
      "Epoch 117/200, Train Loss: 2.8132, Val Loss: 2.9898\n",
      "Epoch 118/200, Train Loss: 2.8155, Val Loss: 2.9857\n",
      "Epoch 119/200, Train Loss: 2.8147, Val Loss: 2.9938\n",
      "Epoch 120/200, Train Loss: 2.8192, Val Loss: 2.9811\n",
      "  New best validation loss: 2.9811\n",
      "Epoch 121/200, Train Loss: 2.8132, Val Loss: 2.9873\n",
      "Epoch 122/200, Train Loss: 2.8076, Val Loss: 2.9948\n",
      "Epoch 123/200, Train Loss: 2.8103, Val Loss: 2.9860\n",
      "Epoch 124/200, Train Loss: 2.8099, Val Loss: 2.9786\n",
      "  New best validation loss: 2.9786\n",
      "Epoch 125/200, Train Loss: 2.8097, Val Loss: 2.9942\n",
      "Epoch 126/200, Train Loss: 2.8155, Val Loss: 2.9965\n",
      "Epoch 127/200, Train Loss: 2.8139, Val Loss: 2.9894\n",
      "Epoch 128/200, Train Loss: 2.8107, Val Loss: 2.9833\n",
      "Epoch 129/200, Train Loss: 2.8128, Val Loss: 2.9869\n",
      "Epoch 130/200, Train Loss: 2.8098, Val Loss: 2.9752\n",
      "  New best validation loss: 2.9752\n",
      "Epoch 131/200, Train Loss: 2.8054, Val Loss: 2.9841\n",
      "Epoch 132/200, Train Loss: 2.8065, Val Loss: 2.9824\n",
      "Epoch 133/200, Train Loss: 2.8035, Val Loss: 2.9831\n",
      "Epoch 134/200, Train Loss: 2.8053, Val Loss: 2.9779\n",
      "Epoch 135/200, Train Loss: 2.7984, Val Loss: 2.9985\n",
      "Epoch 136/200, Train Loss: 2.8007, Val Loss: 2.9838\n",
      "Epoch 137/200, Train Loss: 2.7962, Val Loss: 2.9778\n",
      "Epoch 138/200, Train Loss: 2.7926, Val Loss: 2.9761\n",
      "Epoch 139/200, Train Loss: 2.7960, Val Loss: 2.9833\n",
      "Epoch 140/200, Train Loss: 2.7947, Val Loss: 2.9793\n",
      "Epoch 141/200, Train Loss: 2.7934, Val Loss: 2.9804\n",
      "Epoch 142/200, Train Loss: 2.7979, Val Loss: 2.9935\n",
      "Epoch 143/200, Train Loss: 2.8013, Val Loss: 2.9843\n",
      "Epoch 144/200, Train Loss: 2.8007, Val Loss: 2.9783\n",
      "Epoch 145/200, Train Loss: 2.7987, Val Loss: 2.9769\n",
      "Epoch 146/200, Train Loss: 2.7971, Val Loss: 2.9783\n",
      "Epoch 147/200, Train Loss: 2.7976, Val Loss: 2.9757\n",
      "Epoch 148/200, Train Loss: 2.7970, Val Loss: 2.9900\n",
      "Epoch 149/200, Train Loss: 2.7954, Val Loss: 2.9882\n",
      "Epoch 150/200, Train Loss: 2.7979, Val Loss: 2.9724\n",
      "  New best validation loss: 2.9724\n",
      "Epoch 151/200, Train Loss: 2.7977, Val Loss: 2.9792\n",
      "Epoch 152/200, Train Loss: 2.8010, Val Loss: 2.9852\n",
      "Epoch 153/200, Train Loss: 2.8111, Val Loss: 2.9819\n",
      "Epoch 154/200, Train Loss: 2.7982, Val Loss: 2.9832\n",
      "Epoch 155/200, Train Loss: 2.7930, Val Loss: 2.9705\n",
      "  New best validation loss: 2.9705\n",
      "Epoch 156/200, Train Loss: 2.7951, Val Loss: 2.9743\n",
      "Epoch 157/200, Train Loss: 2.7948, Val Loss: 2.9881\n",
      "Epoch 158/200, Train Loss: 2.7964, Val Loss: 2.9828\n",
      "Epoch 159/200, Train Loss: 2.7886, Val Loss: 2.9781\n",
      "Epoch 160/200, Train Loss: 2.7877, Val Loss: 2.9758\n",
      "Epoch 161/200, Train Loss: 2.7883, Val Loss: 2.9798\n",
      "Epoch 162/200, Train Loss: 2.7884, Val Loss: 2.9756\n",
      "Epoch 163/200, Train Loss: 2.7845, Val Loss: 2.9728\n",
      "Epoch 164/200, Train Loss: 2.7799, Val Loss: 2.9743\n",
      "Epoch 165/200, Train Loss: 2.7798, Val Loss: 2.9681\n",
      "  New best validation loss: 2.9681\n",
      "Epoch 166/200, Train Loss: 2.7783, Val Loss: 2.9735\n",
      "Epoch 167/200, Train Loss: 2.7803, Val Loss: 2.9782\n",
      "Epoch 168/200, Train Loss: 2.7803, Val Loss: 2.9728\n",
      "Epoch 169/200, Train Loss: 2.7861, Val Loss: 2.9854\n",
      "Epoch 170/200, Train Loss: 2.7844, Val Loss: 2.9783\n",
      "Epoch 171/200, Train Loss: 2.7818, Val Loss: 2.9710\n",
      "Epoch 172/200, Train Loss: 2.7777, Val Loss: 2.9709\n",
      "Epoch 173/200, Train Loss: 2.7797, Val Loss: 2.9814\n",
      "Epoch 174/200, Train Loss: 2.7837, Val Loss: 2.9744\n",
      "Epoch 175/200, Train Loss: 2.7801, Val Loss: 2.9741\n",
      "Epoch 176/200, Train Loss: 2.7783, Val Loss: 2.9629\n",
      "  New best validation loss: 2.9629\n",
      "Epoch 177/200, Train Loss: 2.7747, Val Loss: 2.9659\n",
      "Epoch 178/200, Train Loss: 2.7737, Val Loss: 2.9703\n",
      "Epoch 179/200, Train Loss: 2.7745, Val Loss: 2.9711\n",
      "Epoch 180/200, Train Loss: 2.7764, Val Loss: 2.9827\n",
      "Epoch 181/200, Train Loss: 2.7778, Val Loss: 2.9791\n",
      "Epoch 182/200, Train Loss: 2.7779, Val Loss: 2.9782\n",
      "Epoch 183/200, Train Loss: 2.7825, Val Loss: 2.9727\n",
      "Epoch 184/200, Train Loss: 2.7763, Val Loss: 2.9672\n",
      "Epoch 185/200, Train Loss: 2.7796, Val Loss: 2.9699\n",
      "Epoch 186/200, Train Loss: 2.7738, Val Loss: 2.9782\n",
      "Epoch 187/200, Train Loss: 2.7799, Val Loss: 2.9763\n",
      "Epoch 188/200, Train Loss: 2.7760, Val Loss: 2.9696\n",
      "Epoch 189/200, Train Loss: 2.7727, Val Loss: 2.9631\n",
      "Epoch 190/200, Train Loss: 2.7764, Val Loss: 2.9674\n",
      "Epoch 191/200, Train Loss: 2.7745, Val Loss: 2.9659\n",
      "Epoch 192/200, Train Loss: 2.7769, Val Loss: 2.9622\n",
      "  New best validation loss: 2.9622\n",
      "Epoch 193/200, Train Loss: 2.7734, Val Loss: 2.9649\n",
      "Epoch 194/200, Train Loss: 2.7733, Val Loss: 2.9770\n",
      "Epoch 195/200, Train Loss: 2.7739, Val Loss: 2.9706\n",
      "Epoch 196/200, Train Loss: 2.7711, Val Loss: 2.9697\n",
      "Epoch 197/200, Train Loss: 2.7710, Val Loss: 2.9783\n",
      "Epoch 198/200, Train Loss: 2.7754, Val Loss: 2.9681\n",
      "Epoch 199/200, Train Loss: 2.7702, Val Loss: 2.9644\n",
      "Epoch 200/200, Train Loss: 2.7716, Val Loss: 2.9645\n",
      "\n",
      "Loaded best model (Val Loss: 2.9622) for final hidden state extraction.\n",
      "Saved best model for ComplexOscillatorNet to results/20250508_043138/ComplexOscillatorNet_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for ComplexOscillatorNet ---\n",
      "  Analyzing decodability for ComplexOscillatorNet...\n",
      "  Hidden states shape: torch.Size([160, 200, 64])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for ComplexOscillatorNet - Test MSE: 0.0605, Test R2: 0.9709 (best alpha: 1.2743)\n",
      "Decodability (R2 score) for ComplexOscillatorNet: 0.9709\n",
      "\n",
      "--- Training RNN_GRU ---\n",
      "Number of parameters: 50561\n",
      "Epoch 1/200, Train Loss: 4.8265, Val Loss: 5.2117\n",
      "  New best validation loss: 5.2117\n",
      "Epoch 2/200, Train Loss: 4.7924, Val Loss: 5.0764\n",
      "  New best validation loss: 5.0764\n",
      "Epoch 3/200, Train Loss: 4.7707, Val Loss: 5.1396\n",
      "Epoch 4/200, Train Loss: 4.7394, Val Loss: 5.0988\n",
      "Epoch 5/200, Train Loss: 4.7277, Val Loss: 5.1501\n",
      "Epoch 6/200, Train Loss: 4.7182, Val Loss: 5.0424\n",
      "  New best validation loss: 5.0424\n",
      "Epoch 7/200, Train Loss: 4.7303, Val Loss: 5.1388\n",
      "Epoch 8/200, Train Loss: 4.7113, Val Loss: 5.0679\n",
      "Epoch 9/200, Train Loss: 4.7093, Val Loss: 5.0527\n",
      "Epoch 10/200, Train Loss: 4.6897, Val Loss: 5.0944\n",
      "Epoch 11/200, Train Loss: 4.6845, Val Loss: 5.0749\n",
      "Epoch 12/200, Train Loss: 4.6761, Val Loss: 5.0849\n",
      "Epoch 13/200, Train Loss: 4.6684, Val Loss: 5.0234\n",
      "  New best validation loss: 5.0234\n",
      "Epoch 14/200, Train Loss: 4.6521, Val Loss: 5.0573\n",
      "Epoch 15/200, Train Loss: 4.6441, Val Loss: 5.0201\n",
      "  New best validation loss: 5.0201\n",
      "Epoch 16/200, Train Loss: 4.6142, Val Loss: 5.0198\n",
      "  New best validation loss: 5.0198\n",
      "Epoch 17/200, Train Loss: 4.6353, Val Loss: 4.9464\n",
      "  New best validation loss: 4.9464\n",
      "Epoch 18/200, Train Loss: 4.5297, Val Loss: 4.7721\n",
      "  New best validation loss: 4.7721\n",
      "Epoch 19/200, Train Loss: 4.2350, Val Loss: 4.6043\n",
      "  New best validation loss: 4.6043\n",
      "Epoch 20/200, Train Loss: 4.0221, Val Loss: 4.3158\n",
      "  New best validation loss: 4.3158\n",
      "Epoch 21/200, Train Loss: 3.8879, Val Loss: 4.3527\n",
      "Epoch 22/200, Train Loss: 3.8762, Val Loss: 4.1611\n",
      "  New best validation loss: 4.1611\n",
      "Epoch 23/200, Train Loss: 3.7864, Val Loss: 4.0002\n",
      "  New best validation loss: 4.0002\n",
      "Epoch 24/200, Train Loss: 3.7110, Val Loss: 3.9996\n",
      "  New best validation loss: 3.9996\n",
      "Epoch 25/200, Train Loss: 3.6700, Val Loss: 3.9559\n",
      "  New best validation loss: 3.9559\n",
      "Epoch 26/200, Train Loss: 3.6367, Val Loss: 3.8993\n",
      "  New best validation loss: 3.8993\n",
      "Epoch 27/200, Train Loss: 3.6100, Val Loss: 3.9718\n",
      "Epoch 28/200, Train Loss: 3.5717, Val Loss: 3.8616\n",
      "  New best validation loss: 3.8616\n",
      "Epoch 29/200, Train Loss: 3.5076, Val Loss: 3.7183\n",
      "  New best validation loss: 3.7183\n",
      "Epoch 30/200, Train Loss: 3.4259, Val Loss: 3.8250\n",
      "Epoch 31/200, Train Loss: 3.3407, Val Loss: 3.7150\n",
      "  New best validation loss: 3.7150\n",
      "Epoch 32/200, Train Loss: 3.2029, Val Loss: 3.5306\n",
      "  New best validation loss: 3.5306\n",
      "Epoch 33/200, Train Loss: 3.2129, Val Loss: 3.6834\n",
      "Epoch 34/200, Train Loss: 3.2131, Val Loss: 3.5791\n",
      "Epoch 35/200, Train Loss: 3.0419, Val Loss: 3.3802\n",
      "  New best validation loss: 3.3802\n",
      "Epoch 36/200, Train Loss: 2.9639, Val Loss: 3.2087\n",
      "  New best validation loss: 3.2087\n",
      "Epoch 37/200, Train Loss: 2.8188, Val Loss: 3.2107\n",
      "Epoch 38/200, Train Loss: 2.7033, Val Loss: 2.8835\n",
      "  New best validation loss: 2.8835\n",
      "Epoch 39/200, Train Loss: 2.6397, Val Loss: 2.8881\n",
      "Epoch 40/200, Train Loss: 2.5716, Val Loss: 2.7611\n",
      "  New best validation loss: 2.7611\n",
      "Epoch 41/200, Train Loss: 2.4875, Val Loss: 2.7465\n",
      "  New best validation loss: 2.7465\n",
      "Epoch 42/200, Train Loss: 2.4313, Val Loss: 2.7671\n",
      "Epoch 43/200, Train Loss: 2.3899, Val Loss: 2.3981\n",
      "  New best validation loss: 2.3981\n",
      "Epoch 44/200, Train Loss: 2.2914, Val Loss: 2.8026\n",
      "Epoch 45/200, Train Loss: 2.1411, Val Loss: 2.2558\n",
      "  New best validation loss: 2.2558\n",
      "Epoch 46/200, Train Loss: 2.0057, Val Loss: 2.3667\n",
      "Epoch 47/200, Train Loss: 1.9921, Val Loss: 2.1346\n",
      "  New best validation loss: 2.1346\n",
      "Epoch 48/200, Train Loss: 1.9283, Val Loss: 2.2170\n",
      "Epoch 49/200, Train Loss: 1.9242, Val Loss: 2.1341\n",
      "  New best validation loss: 2.1341\n",
      "Epoch 50/200, Train Loss: 1.7978, Val Loss: 2.0001\n",
      "  New best validation loss: 2.0001\n",
      "Epoch 51/200, Train Loss: 1.7662, Val Loss: 2.0080\n",
      "Epoch 52/200, Train Loss: 1.8101, Val Loss: 1.7647\n",
      "  New best validation loss: 1.7647\n",
      "Epoch 53/200, Train Loss: 1.7172, Val Loss: 1.9012\n",
      "Epoch 54/200, Train Loss: 1.7041, Val Loss: 1.7692\n",
      "Epoch 55/200, Train Loss: 1.5998, Val Loss: 1.7658\n",
      "Epoch 56/200, Train Loss: 1.6262, Val Loss: 1.6791\n",
      "  New best validation loss: 1.6791\n",
      "Epoch 57/200, Train Loss: 1.5168, Val Loss: 1.7050\n",
      "Epoch 58/200, Train Loss: 1.4586, Val Loss: 1.6818\n",
      "Epoch 59/200, Train Loss: 1.4286, Val Loss: 1.5502\n",
      "  New best validation loss: 1.5502\n",
      "Epoch 60/200, Train Loss: 1.3855, Val Loss: 1.4945\n",
      "  New best validation loss: 1.4945\n",
      "Epoch 61/200, Train Loss: 1.3051, Val Loss: 1.3722\n",
      "  New best validation loss: 1.3722\n",
      "Epoch 62/200, Train Loss: 1.3128, Val Loss: 1.5600\n",
      "Epoch 63/200, Train Loss: 1.4269, Val Loss: 1.6335\n",
      "Epoch 64/200, Train Loss: 1.3769, Val Loss: 1.3611\n",
      "  New best validation loss: 1.3611\n",
      "Epoch 65/200, Train Loss: 1.3025, Val Loss: 1.4723\n",
      "Epoch 66/200, Train Loss: 1.2574, Val Loss: 1.3940\n",
      "Epoch 67/200, Train Loss: 1.1896, Val Loss: 1.2475\n",
      "  New best validation loss: 1.2475\n",
      "Epoch 68/200, Train Loss: 1.1586, Val Loss: 1.2656\n",
      "Epoch 69/200, Train Loss: 1.1581, Val Loss: 1.3047\n",
      "Epoch 70/200, Train Loss: 1.1382, Val Loss: 1.1991\n",
      "  New best validation loss: 1.1991\n",
      "Epoch 71/200, Train Loss: 1.0917, Val Loss: 1.1702\n",
      "  New best validation loss: 1.1702\n",
      "Epoch 72/200, Train Loss: 1.1130, Val Loss: 1.2809\n",
      "Epoch 73/200, Train Loss: 1.1004, Val Loss: 1.1909\n",
      "Epoch 74/200, Train Loss: 1.0841, Val Loss: 1.2496\n",
      "Epoch 75/200, Train Loss: 1.0407, Val Loss: 1.1111\n",
      "  New best validation loss: 1.1111\n",
      "Epoch 76/200, Train Loss: 0.9958, Val Loss: 1.0101\n",
      "  New best validation loss: 1.0101\n",
      "Epoch 77/200, Train Loss: 1.0270, Val Loss: 0.9975\n",
      "  New best validation loss: 0.9975\n",
      "Epoch 78/200, Train Loss: 1.0089, Val Loss: 1.1023\n",
      "Epoch 79/200, Train Loss: 0.9754, Val Loss: 0.9905\n",
      "  New best validation loss: 0.9905\n",
      "Epoch 80/200, Train Loss: 0.9426, Val Loss: 0.9450\n",
      "  New best validation loss: 0.9450\n",
      "Epoch 81/200, Train Loss: 0.9037, Val Loss: 1.0639\n",
      "Epoch 82/200, Train Loss: 0.9712, Val Loss: 0.8802\n",
      "  New best validation loss: 0.8802\n",
      "Epoch 83/200, Train Loss: 0.9479, Val Loss: 0.8804\n",
      "Epoch 84/200, Train Loss: 0.8490, Val Loss: 0.8881\n",
      "Epoch 85/200, Train Loss: 0.8118, Val Loss: 0.8796\n",
      "  New best validation loss: 0.8796\n",
      "Epoch 86/200, Train Loss: 0.8007, Val Loss: 0.8153\n",
      "  New best validation loss: 0.8153\n",
      "Epoch 87/200, Train Loss: 0.8554, Val Loss: 0.9805\n",
      "Epoch 88/200, Train Loss: 0.8792, Val Loss: 0.8983\n",
      "Epoch 89/200, Train Loss: 0.8125, Val Loss: 0.9042\n",
      "Epoch 90/200, Train Loss: 0.8355, Val Loss: 1.0796\n",
      "Epoch 91/200, Train Loss: 0.8548, Val Loss: 0.9700\n",
      "Epoch 92/200, Train Loss: 0.7988, Val Loss: 0.8552\n",
      "Epoch 93/200, Train Loss: 0.7465, Val Loss: 0.8251\n",
      "Epoch 94/200, Train Loss: 0.6957, Val Loss: 0.7254\n",
      "  New best validation loss: 0.7254\n",
      "Epoch 95/200, Train Loss: 0.7402, Val Loss: 0.8774\n",
      "Epoch 96/200, Train Loss: 0.7007, Val Loss: 0.7243\n",
      "  New best validation loss: 0.7243\n",
      "Epoch 97/200, Train Loss: 0.6452, Val Loss: 0.6945\n",
      "  New best validation loss: 0.6945\n",
      "Epoch 98/200, Train Loss: 0.6720, Val Loss: 0.7251\n",
      "Epoch 99/200, Train Loss: 0.6767, Val Loss: 0.6975\n",
      "Epoch 100/200, Train Loss: 0.6454, Val Loss: 0.6937\n",
      "  New best validation loss: 0.6937\n",
      "Epoch 101/200, Train Loss: 0.6154, Val Loss: 0.6384\n",
      "  New best validation loss: 0.6384\n",
      "Epoch 102/200, Train Loss: 0.6317, Val Loss: 0.6992\n",
      "Epoch 103/200, Train Loss: 0.6570, Val Loss: 0.7471\n",
      "Epoch 104/200, Train Loss: 0.6746, Val Loss: 0.6319\n",
      "  New best validation loss: 0.6319\n",
      "Epoch 105/200, Train Loss: 0.6405, Val Loss: 0.6917\n",
      "Epoch 106/200, Train Loss: 0.6223, Val Loss: 0.5935\n",
      "  New best validation loss: 0.5935\n",
      "Epoch 107/200, Train Loss: 0.6134, Val Loss: 0.6730\n",
      "Epoch 108/200, Train Loss: 0.6534, Val Loss: 0.6530\n",
      "Epoch 109/200, Train Loss: 0.6512, Val Loss: 0.6350\n",
      "Epoch 110/200, Train Loss: 0.6239, Val Loss: 0.7209\n",
      "Epoch 111/200, Train Loss: 0.5934, Val Loss: 0.6067\n",
      "Epoch 112/200, Train Loss: 0.5688, Val Loss: 0.5956\n",
      "Epoch 113/200, Train Loss: 0.5392, Val Loss: 0.5622\n",
      "  New best validation loss: 0.5622\n",
      "Epoch 114/200, Train Loss: 0.5629, Val Loss: 0.6102\n",
      "Epoch 115/200, Train Loss: 0.5751, Val Loss: 0.5635\n",
      "Epoch 116/200, Train Loss: 0.5353, Val Loss: 0.5587\n",
      "  New best validation loss: 0.5587\n",
      "Epoch 117/200, Train Loss: 0.5307, Val Loss: 0.5790\n",
      "Epoch 118/200, Train Loss: 0.5479, Val Loss: 0.5475\n",
      "  New best validation loss: 0.5475\n",
      "Epoch 119/200, Train Loss: 0.5682, Val Loss: 0.6042\n",
      "Epoch 120/200, Train Loss: 0.5980, Val Loss: 0.6783\n",
      "Epoch 121/200, Train Loss: 0.5832, Val Loss: 0.5503\n",
      "Epoch 122/200, Train Loss: 0.5562, Val Loss: 0.6093\n",
      "Epoch 123/200, Train Loss: 0.6191, Val Loss: 0.6443\n",
      "Epoch 124/200, Train Loss: 0.6634, Val Loss: 0.5824\n",
      "Epoch 125/200, Train Loss: 0.5818, Val Loss: 0.5679\n",
      "Epoch 126/200, Train Loss: 0.5536, Val Loss: 0.5506\n",
      "Epoch 127/200, Train Loss: 0.5309, Val Loss: 0.5610\n",
      "Epoch 128/200, Train Loss: 0.5299, Val Loss: 0.5401\n",
      "  New best validation loss: 0.5401\n",
      "Epoch 129/200, Train Loss: 0.4978, Val Loss: 0.5692\n",
      "Epoch 130/200, Train Loss: 0.5497, Val Loss: 0.5498\n",
      "Epoch 131/200, Train Loss: 0.4992, Val Loss: 0.4947\n",
      "  New best validation loss: 0.4947\n",
      "Epoch 132/200, Train Loss: 0.4787, Val Loss: 0.5101\n",
      "Epoch 133/200, Train Loss: 0.4846, Val Loss: 0.5001\n",
      "Epoch 134/200, Train Loss: 0.4988, Val Loss: 0.5504\n",
      "Epoch 135/200, Train Loss: 0.4649, Val Loss: 0.4922\n",
      "  New best validation loss: 0.4922\n",
      "Epoch 136/200, Train Loss: 0.5087, Val Loss: 0.6348\n",
      "Epoch 137/200, Train Loss: 0.4950, Val Loss: 0.6402\n",
      "Epoch 138/200, Train Loss: 0.4960, Val Loss: 0.5472\n",
      "Epoch 139/200, Train Loss: 0.5214, Val Loss: 0.5328\n",
      "Epoch 140/200, Train Loss: 0.5524, Val Loss: 0.5071\n",
      "Epoch 141/200, Train Loss: 0.4993, Val Loss: 0.6149\n",
      "Epoch 142/200, Train Loss: 0.5085, Val Loss: 0.5414\n",
      "Epoch 143/200, Train Loss: 0.5318, Val Loss: 0.6129\n",
      "Epoch 144/200, Train Loss: 0.5153, Val Loss: 0.5570\n",
      "Epoch 145/200, Train Loss: 0.4834, Val Loss: 0.4573\n",
      "  New best validation loss: 0.4573\n",
      "Epoch 146/200, Train Loss: 0.4463, Val Loss: 0.4590\n",
      "Epoch 147/200, Train Loss: 0.4299, Val Loss: 0.4342\n",
      "  New best validation loss: 0.4342\n",
      "Epoch 148/200, Train Loss: 0.4187, Val Loss: 0.5298\n",
      "Epoch 149/200, Train Loss: 0.4504, Val Loss: 0.4596\n",
      "Epoch 150/200, Train Loss: 0.4420, Val Loss: 0.4372\n",
      "Epoch 151/200, Train Loss: 0.4338, Val Loss: 0.4424\n",
      "Epoch 152/200, Train Loss: 0.4749, Val Loss: 0.6237\n",
      "Epoch 153/200, Train Loss: 0.4834, Val Loss: 0.4879\n",
      "Epoch 154/200, Train Loss: 0.4872, Val Loss: 0.4799\n",
      "Epoch 155/200, Train Loss: 0.4810, Val Loss: 0.4679\n",
      "Epoch 156/200, Train Loss: 0.4663, Val Loss: 0.4409\n",
      "Epoch 157/200, Train Loss: 0.4389, Val Loss: 0.5042\n",
      "Epoch 158/200, Train Loss: 0.5070, Val Loss: 0.6090\n",
      "Epoch 159/200, Train Loss: 0.5111, Val Loss: 0.5302\n",
      "Epoch 160/200, Train Loss: 0.4453, Val Loss: 0.4132\n",
      "  New best validation loss: 0.4132\n",
      "Epoch 161/200, Train Loss: 0.4331, Val Loss: 0.4587\n",
      "Epoch 162/200, Train Loss: 0.4142, Val Loss: 0.4507\n",
      "Epoch 163/200, Train Loss: 0.4252, Val Loss: 0.4277\n",
      "Epoch 164/200, Train Loss: 0.4131, Val Loss: 0.4246\n",
      "Epoch 165/200, Train Loss: 0.4045, Val Loss: 0.4276\n",
      "Epoch 166/200, Train Loss: 0.4341, Val Loss: 0.4847\n",
      "Epoch 167/200, Train Loss: 0.5251, Val Loss: 0.5302\n",
      "Epoch 168/200, Train Loss: 0.4456, Val Loss: 0.4417\n",
      "Epoch 169/200, Train Loss: 0.4335, Val Loss: 0.4520\n",
      "Epoch 170/200, Train Loss: 0.4521, Val Loss: 0.4319\n",
      "Epoch 171/200, Train Loss: 0.4114, Val Loss: 0.4446\n",
      "Epoch 172/200, Train Loss: 0.4027, Val Loss: 0.3950\n",
      "  New best validation loss: 0.3950\n",
      "Epoch 173/200, Train Loss: 0.3775, Val Loss: 0.4443\n",
      "Epoch 174/200, Train Loss: 0.4035, Val Loss: 0.4393\n",
      "Epoch 175/200, Train Loss: 0.4622, Val Loss: 0.3885\n",
      "  New best validation loss: 0.3885\n",
      "Epoch 176/200, Train Loss: 0.5106, Val Loss: 0.5236\n",
      "Epoch 177/200, Train Loss: 0.4059, Val Loss: 0.3930\n",
      "Epoch 178/200, Train Loss: 0.3765, Val Loss: 0.3829\n",
      "  New best validation loss: 0.3829\n",
      "Epoch 179/200, Train Loss: 0.4482, Val Loss: 0.4496\n",
      "Epoch 180/200, Train Loss: 0.4181, Val Loss: 0.3945\n",
      "Epoch 181/200, Train Loss: 0.3899, Val Loss: 0.3809\n",
      "  New best validation loss: 0.3809\n",
      "Epoch 182/200, Train Loss: 0.3625, Val Loss: 0.3746\n",
      "  New best validation loss: 0.3746\n",
      "Epoch 183/200, Train Loss: 0.3724, Val Loss: 0.3944\n",
      "Epoch 184/200, Train Loss: 0.3901, Val Loss: 0.3816\n",
      "Epoch 185/200, Train Loss: 0.3939, Val Loss: 0.3869\n",
      "Epoch 186/200, Train Loss: 0.3910, Val Loss: 0.4910\n",
      "Epoch 187/200, Train Loss: 0.3941, Val Loss: 0.4761\n",
      "Epoch 188/200, Train Loss: 0.4108, Val Loss: 0.4437\n",
      "Epoch 189/200, Train Loss: 0.3980, Val Loss: 0.4607\n",
      "Epoch 190/200, Train Loss: 0.4190, Val Loss: 0.4074\n",
      "Epoch 191/200, Train Loss: 0.4091, Val Loss: 0.4328\n",
      "Epoch 192/200, Train Loss: 0.3876, Val Loss: 0.4313\n",
      "Epoch 193/200, Train Loss: 0.4623, Val Loss: 0.5150\n",
      "Epoch 194/200, Train Loss: 0.4231, Val Loss: 0.4190\n",
      "Epoch 195/200, Train Loss: 0.3909, Val Loss: 0.3542\n",
      "  New best validation loss: 0.3542\n",
      "Epoch 196/200, Train Loss: 0.3628, Val Loss: 0.3841\n",
      "Epoch 197/200, Train Loss: 0.3541, Val Loss: 0.3607\n",
      "Epoch 198/200, Train Loss: 0.3932, Val Loss: 0.3921\n",
      "Epoch 199/200, Train Loss: 0.3625, Val Loss: 0.3801\n",
      "Epoch 200/200, Train Loss: 0.3420, Val Loss: 0.3446\n",
      "  New best validation loss: 0.3446\n",
      "\n",
      "Loaded best model (Val Loss: 0.3446) for final hidden state extraction.\n",
      "Saved best model for RNN_GRU to results/20250508_043138/RNN_GRU_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for RNN_GRU ---\n",
      "  Analyzing decodability for RNN_GRU...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for RNN_GRU - Test MSE: 0.0025, Test R2: 0.9988 (best alpha: 0.0018)\n",
      "Decodability (R2 score) for RNN_GRU: 0.9988\n",
      "\n",
      "--- Training Transformer ---\n",
      "Number of parameters: 281345\n",
      "Epoch 1/200, Train Loss: 7.3247, Val Loss: 5.5662\n",
      "  New best validation loss: 5.5662\n",
      "Epoch 2/200, Train Loss: 5.0645, Val Loss: 5.2161\n",
      "  New best validation loss: 5.2161\n",
      "Epoch 3/200, Train Loss: 4.9023, Val Loss: 5.1638\n",
      "  New best validation loss: 5.1638\n",
      "Epoch 4/200, Train Loss: 4.8843, Val Loss: 5.1514\n",
      "  New best validation loss: 5.1514\n",
      "Epoch 5/200, Train Loss: 4.8586, Val Loss: 5.1412\n",
      "  New best validation loss: 5.1412\n",
      "Epoch 6/200, Train Loss: 4.8390, Val Loss: 5.0906\n",
      "  New best validation loss: 5.0906\n",
      "Epoch 7/200, Train Loss: 4.8161, Val Loss: 5.0753\n",
      "  New best validation loss: 5.0753\n",
      "Epoch 8/200, Train Loss: 4.8112, Val Loss: 5.1452\n",
      "Epoch 9/200, Train Loss: 4.8082, Val Loss: 5.0314\n",
      "  New best validation loss: 5.0314\n",
      "Epoch 10/200, Train Loss: 4.8066, Val Loss: 5.0800\n",
      "Epoch 11/200, Train Loss: 4.7890, Val Loss: 5.1511\n",
      "Epoch 12/200, Train Loss: 4.7894, Val Loss: 4.9763\n",
      "  New best validation loss: 4.9763\n",
      "Epoch 13/200, Train Loss: 4.7454, Val Loss: 4.9504\n",
      "  New best validation loss: 4.9504\n",
      "Epoch 14/200, Train Loss: 4.7413, Val Loss: 5.0762\n",
      "Epoch 15/200, Train Loss: 4.7214, Val Loss: 4.9203\n",
      "  New best validation loss: 4.9203\n",
      "Epoch 16/200, Train Loss: 4.7283, Val Loss: 4.8990\n",
      "  New best validation loss: 4.8990\n",
      "Epoch 17/200, Train Loss: 4.7374, Val Loss: 4.9095\n",
      "Epoch 18/200, Train Loss: 4.6995, Val Loss: 4.8483\n",
      "  New best validation loss: 4.8483\n",
      "Epoch 19/200, Train Loss: 4.7168, Val Loss: 4.8047\n",
      "  New best validation loss: 4.8047\n",
      "Epoch 20/200, Train Loss: 4.6803, Val Loss: 4.8300\n",
      "Epoch 21/200, Train Loss: 4.6481, Val Loss: 4.7611\n",
      "  New best validation loss: 4.7611\n",
      "Epoch 22/200, Train Loss: 4.6291, Val Loss: 4.8053\n",
      "Epoch 23/200, Train Loss: 4.6233, Val Loss: 4.9408\n",
      "Epoch 24/200, Train Loss: 4.5974, Val Loss: 4.6478\n",
      "  New best validation loss: 4.6478\n",
      "Epoch 25/200, Train Loss: 4.6437, Val Loss: 4.6450\n",
      "  New best validation loss: 4.6450\n",
      "Epoch 26/200, Train Loss: 4.6611, Val Loss: 4.6678\n",
      "Epoch 27/200, Train Loss: 4.6236, Val Loss: 4.6519\n",
      "Epoch 28/200, Train Loss: 4.5439, Val Loss: 4.7629\n",
      "Epoch 29/200, Train Loss: 4.5265, Val Loss: 4.6437\n",
      "  New best validation loss: 4.6437\n",
      "Epoch 30/200, Train Loss: 4.5609, Val Loss: 4.6577\n",
      "Epoch 31/200, Train Loss: 4.5693, Val Loss: 4.8126\n",
      "Epoch 32/200, Train Loss: 4.5064, Val Loss: 4.6685\n",
      "Epoch 33/200, Train Loss: 4.5156, Val Loss: 4.8525\n",
      "Epoch 34/200, Train Loss: 4.4631, Val Loss: 4.3566\n",
      "  New best validation loss: 4.3566\n",
      "Epoch 35/200, Train Loss: 4.4009, Val Loss: 4.3235\n",
      "  New best validation loss: 4.3235\n",
      "Epoch 36/200, Train Loss: 4.3220, Val Loss: 4.1892\n",
      "  New best validation loss: 4.1892\n",
      "Epoch 37/200, Train Loss: 4.3092, Val Loss: 4.1242\n",
      "  New best validation loss: 4.1242\n",
      "Epoch 38/200, Train Loss: 4.3139, Val Loss: 4.0214\n",
      "  New best validation loss: 4.0214\n",
      "Epoch 39/200, Train Loss: 4.2754, Val Loss: 4.0648\n",
      "Epoch 40/200, Train Loss: 4.3335, Val Loss: 3.8664\n",
      "  New best validation loss: 3.8664\n",
      "Epoch 41/200, Train Loss: 4.2715, Val Loss: 3.9140\n",
      "Epoch 42/200, Train Loss: 4.1946, Val Loss: 3.9249\n",
      "Epoch 43/200, Train Loss: 4.1012, Val Loss: 3.7798\n",
      "  New best validation loss: 3.7798\n",
      "Epoch 44/200, Train Loss: 3.9906, Val Loss: 3.6784\n",
      "  New best validation loss: 3.6784\n",
      "Epoch 45/200, Train Loss: 4.0632, Val Loss: 3.7685\n",
      "Epoch 46/200, Train Loss: 3.9246, Val Loss: 3.4976\n",
      "  New best validation loss: 3.4976\n",
      "Epoch 47/200, Train Loss: 3.8304, Val Loss: 3.3517\n",
      "  New best validation loss: 3.3517\n",
      "Epoch 48/200, Train Loss: 3.7826, Val Loss: 3.4236\n",
      "Epoch 49/200, Train Loss: 3.7330, Val Loss: 3.3674\n",
      "Epoch 50/200, Train Loss: 3.6428, Val Loss: 2.9965\n",
      "  New best validation loss: 2.9965\n",
      "Epoch 51/200, Train Loss: 3.4795, Val Loss: 3.1117\n",
      "Epoch 52/200, Train Loss: 3.3445, Val Loss: 2.5413\n",
      "  New best validation loss: 2.5413\n",
      "Epoch 53/200, Train Loss: 3.1537, Val Loss: 2.3902\n",
      "  New best validation loss: 2.3902\n",
      "Epoch 54/200, Train Loss: 2.9784, Val Loss: 2.3827\n",
      "  New best validation loss: 2.3827\n",
      "Epoch 55/200, Train Loss: 2.9727, Val Loss: 2.1961\n",
      "  New best validation loss: 2.1961\n",
      "Epoch 56/200, Train Loss: 2.6963, Val Loss: 1.9281\n",
      "  New best validation loss: 1.9281\n",
      "Epoch 57/200, Train Loss: 2.4571, Val Loss: 1.7057\n",
      "  New best validation loss: 1.7057\n",
      "Epoch 58/200, Train Loss: 2.3346, Val Loss: 1.6328\n",
      "  New best validation loss: 1.6328\n",
      "Epoch 59/200, Train Loss: 2.2705, Val Loss: 1.8156\n",
      "Epoch 60/200, Train Loss: 2.2183, Val Loss: 1.4616\n",
      "  New best validation loss: 1.4616\n",
      "Epoch 61/200, Train Loss: 2.0342, Val Loss: 1.3116\n",
      "  New best validation loss: 1.3116\n",
      "Epoch 62/200, Train Loss: 1.8330, Val Loss: 1.2654\n",
      "  New best validation loss: 1.2654\n",
      "Epoch 63/200, Train Loss: 1.7166, Val Loss: 1.0895\n",
      "  New best validation loss: 1.0895\n",
      "Epoch 64/200, Train Loss: 1.5733, Val Loss: 1.0045\n",
      "  New best validation loss: 1.0045\n",
      "Epoch 65/200, Train Loss: 1.5780, Val Loss: 1.0101\n",
      "Epoch 66/200, Train Loss: 1.4937, Val Loss: 0.9952\n",
      "  New best validation loss: 0.9952\n",
      "Epoch 67/200, Train Loss: 1.5287, Val Loss: 0.8281\n",
      "  New best validation loss: 0.8281\n",
      "Epoch 68/200, Train Loss: 1.4109, Val Loss: 0.7971\n",
      "  New best validation loss: 0.7971\n",
      "Epoch 69/200, Train Loss: 1.3568, Val Loss: 0.7986\n",
      "Epoch 70/200, Train Loss: 1.2812, Val Loss: 0.7688\n",
      "  New best validation loss: 0.7688\n",
      "Epoch 71/200, Train Loss: 1.2364, Val Loss: 0.7504\n",
      "  New best validation loss: 0.7504\n",
      "Epoch 72/200, Train Loss: 1.1800, Val Loss: 0.6486\n",
      "  New best validation loss: 0.6486\n",
      "Epoch 73/200, Train Loss: 1.1501, Val Loss: 0.7294\n",
      "Epoch 74/200, Train Loss: 1.1449, Val Loss: 0.6289\n",
      "  New best validation loss: 0.6289\n",
      "Epoch 75/200, Train Loss: 1.0705, Val Loss: 0.5620\n",
      "  New best validation loss: 0.5620\n",
      "Epoch 76/200, Train Loss: 1.0305, Val Loss: 0.6014\n",
      "Epoch 77/200, Train Loss: 1.0274, Val Loss: 0.5945\n",
      "Epoch 78/200, Train Loss: 0.9506, Val Loss: 0.4880\n",
      "  New best validation loss: 0.4880\n",
      "Epoch 79/200, Train Loss: 0.8985, Val Loss: 0.3828\n",
      "  New best validation loss: 0.3828\n",
      "Epoch 80/200, Train Loss: 0.8671, Val Loss: 0.4066\n",
      "Epoch 81/200, Train Loss: 0.8486, Val Loss: 0.3980\n",
      "Epoch 82/200, Train Loss: 0.8630, Val Loss: 0.4227\n",
      "Epoch 83/200, Train Loss: 0.8153, Val Loss: 0.3659\n",
      "  New best validation loss: 0.3659\n",
      "Epoch 84/200, Train Loss: 0.7921, Val Loss: 0.3221\n",
      "  New best validation loss: 0.3221\n",
      "Epoch 85/200, Train Loss: 0.7781, Val Loss: 0.3274\n",
      "Epoch 86/200, Train Loss: 0.7673, Val Loss: 0.3233\n",
      "Epoch 87/200, Train Loss: 0.7599, Val Loss: 0.3155\n",
      "  New best validation loss: 0.3155\n",
      "Epoch 88/200, Train Loss: 0.7681, Val Loss: 0.3772\n",
      "Epoch 89/200, Train Loss: 0.7326, Val Loss: 0.3214\n",
      "Epoch 90/200, Train Loss: 0.6948, Val Loss: 0.2743\n",
      "  New best validation loss: 0.2743\n",
      "Epoch 91/200, Train Loss: 0.6650, Val Loss: 0.2571\n",
      "  New best validation loss: 0.2571\n",
      "Epoch 92/200, Train Loss: 0.6411, Val Loss: 0.2929\n",
      "Epoch 93/200, Train Loss: 0.6309, Val Loss: 0.2482\n",
      "  New best validation loss: 0.2482\n",
      "Epoch 94/200, Train Loss: 0.6179, Val Loss: 0.1962\n",
      "  New best validation loss: 0.1962\n",
      "Epoch 95/200, Train Loss: 0.5956, Val Loss: 0.1906\n",
      "  New best validation loss: 0.1906\n",
      "Epoch 96/200, Train Loss: 0.6033, Val Loss: 0.2064\n",
      "Epoch 97/200, Train Loss: 0.5793, Val Loss: 0.2242\n",
      "Epoch 98/200, Train Loss: 0.5855, Val Loss: 0.1648\n",
      "  New best validation loss: 0.1648\n",
      "Epoch 99/200, Train Loss: 0.5605, Val Loss: 0.2071\n",
      "Epoch 100/200, Train Loss: 0.5481, Val Loss: 0.2063\n",
      "Epoch 101/200, Train Loss: 0.5328, Val Loss: 0.1583\n",
      "  New best validation loss: 0.1583\n",
      "Epoch 102/200, Train Loss: 0.5183, Val Loss: 0.1658\n",
      "Epoch 103/200, Train Loss: 0.4927, Val Loss: 0.1737\n",
      "Epoch 104/200, Train Loss: 0.4900, Val Loss: 0.2097\n",
      "Epoch 105/200, Train Loss: 0.4951, Val Loss: 0.1817\n",
      "Epoch 106/200, Train Loss: 0.4715, Val Loss: 0.1496\n",
      "  New best validation loss: 0.1496\n",
      "Epoch 107/200, Train Loss: 0.4612, Val Loss: 0.1550\n",
      "Epoch 108/200, Train Loss: 0.4826, Val Loss: 0.1527\n",
      "Epoch 109/200, Train Loss: 0.4553, Val Loss: 0.1208\n",
      "  New best validation loss: 0.1208\n",
      "Epoch 110/200, Train Loss: 0.4385, Val Loss: 0.1624\n",
      "Epoch 111/200, Train Loss: 0.4557, Val Loss: 0.1398\n",
      "Epoch 112/200, Train Loss: 0.4624, Val Loss: 0.1334\n",
      "Epoch 113/200, Train Loss: 0.4368, Val Loss: 0.1432\n",
      "Epoch 114/200, Train Loss: 0.4422, Val Loss: 0.1499\n",
      "Epoch 115/200, Train Loss: 0.4560, Val Loss: 0.1925\n",
      "Epoch 116/200, Train Loss: 0.4625, Val Loss: 0.1740\n",
      "Epoch 117/200, Train Loss: 0.4441, Val Loss: 0.1391\n",
      "Epoch 118/200, Train Loss: 0.4481, Val Loss: 0.1501\n",
      "Epoch 119/200, Train Loss: 0.4109, Val Loss: 0.1151\n",
      "  New best validation loss: 0.1151\n",
      "Epoch 120/200, Train Loss: 0.4029, Val Loss: 0.1270\n",
      "Epoch 121/200, Train Loss: 0.3876, Val Loss: 0.1101\n",
      "  New best validation loss: 0.1101\n",
      "Epoch 122/200, Train Loss: 0.3840, Val Loss: 0.0960\n",
      "  New best validation loss: 0.0960\n",
      "Epoch 123/200, Train Loss: 0.3949, Val Loss: 0.0986\n",
      "Epoch 124/200, Train Loss: 0.3925, Val Loss: 0.1495\n",
      "Epoch 125/200, Train Loss: 0.3901, Val Loss: 0.1343\n",
      "Epoch 126/200, Train Loss: 0.3804, Val Loss: 0.1049\n",
      "Epoch 127/200, Train Loss: 0.3669, Val Loss: 0.1103\n",
      "Epoch 128/200, Train Loss: 0.3660, Val Loss: 0.0897\n",
      "  New best validation loss: 0.0897\n",
      "Epoch 129/200, Train Loss: 0.3658, Val Loss: 0.1037\n",
      "Epoch 130/200, Train Loss: 0.3595, Val Loss: 0.0885\n",
      "  New best validation loss: 0.0885\n",
      "Epoch 131/200, Train Loss: 0.3661, Val Loss: 0.1053\n",
      "Epoch 132/200, Train Loss: 0.3646, Val Loss: 0.0932\n",
      "Epoch 133/200, Train Loss: 0.3578, Val Loss: 0.1038\n",
      "Epoch 134/200, Train Loss: 0.3508, Val Loss: 0.1324\n",
      "Epoch 135/200, Train Loss: 0.3515, Val Loss: 0.0956\n",
      "Epoch 136/200, Train Loss: 0.3395, Val Loss: 0.0903\n",
      "Epoch 137/200, Train Loss: 0.3401, Val Loss: 0.0913\n",
      "Epoch 138/200, Train Loss: 0.3423, Val Loss: 0.0983\n",
      "Epoch 139/200, Train Loss: 0.3359, Val Loss: 0.1044\n",
      "Epoch 140/200, Train Loss: 0.3345, Val Loss: 0.1076\n",
      "Epoch 141/200, Train Loss: 0.3294, Val Loss: 0.1302\n",
      "Epoch 142/200, Train Loss: 0.3230, Val Loss: 0.0862\n",
      "  New best validation loss: 0.0862\n",
      "Epoch 143/200, Train Loss: 0.3298, Val Loss: 0.0865\n",
      "Epoch 144/200, Train Loss: 0.3284, Val Loss: 0.1021\n",
      "Epoch 145/200, Train Loss: 0.3277, Val Loss: 0.0698\n",
      "  New best validation loss: 0.0698\n",
      "Epoch 146/200, Train Loss: 0.3183, Val Loss: 0.0824\n",
      "Epoch 147/200, Train Loss: 0.3212, Val Loss: 0.0892\n",
      "Epoch 148/200, Train Loss: 0.3233, Val Loss: 0.1421\n",
      "Epoch 149/200, Train Loss: 0.3326, Val Loss: 0.0925\n",
      "Epoch 150/200, Train Loss: 0.3331, Val Loss: 0.0863\n",
      "Epoch 151/200, Train Loss: 0.3205, Val Loss: 0.0970\n",
      "Epoch 152/200, Train Loss: 0.3141, Val Loss: 0.1025\n",
      "Epoch 153/200, Train Loss: 0.3069, Val Loss: 0.1107\n",
      "Epoch 154/200, Train Loss: 0.3101, Val Loss: 0.0913\n",
      "Epoch 155/200, Train Loss: 0.2976, Val Loss: 0.1092\n",
      "Epoch 156/200, Train Loss: 0.3224, Val Loss: 0.1135\n",
      "Epoch 157/200, Train Loss: 0.3109, Val Loss: 0.0862\n",
      "Epoch 158/200, Train Loss: 0.2930, Val Loss: 0.0958\n",
      "Epoch 159/200, Train Loss: 0.2896, Val Loss: 0.0972\n",
      "Epoch 160/200, Train Loss: 0.2997, Val Loss: 0.0852\n",
      "Epoch 161/200, Train Loss: 0.3055, Val Loss: 0.0718\n",
      "Epoch 162/200, Train Loss: 0.2994, Val Loss: 0.1524\n",
      "Epoch 163/200, Train Loss: 0.2998, Val Loss: 0.1162\n",
      "Epoch 164/200, Train Loss: 0.3017, Val Loss: 0.1173\n",
      "Epoch 165/200, Train Loss: 0.2947, Val Loss: 0.0792\n",
      "Epoch 166/200, Train Loss: 0.2853, Val Loss: 0.0899\n",
      "Epoch 167/200, Train Loss: 0.2797, Val Loss: 0.0784\n",
      "Epoch 168/200, Train Loss: 0.2814, Val Loss: 0.0770\n",
      "Epoch 169/200, Train Loss: 0.2700, Val Loss: 0.0595\n",
      "  New best validation loss: 0.0595\n",
      "Epoch 170/200, Train Loss: 0.2691, Val Loss: 0.0752\n",
      "Epoch 171/200, Train Loss: 0.2698, Val Loss: 0.0659\n",
      "Epoch 172/200, Train Loss: 0.2633, Val Loss: 0.1013\n",
      "Epoch 173/200, Train Loss: 0.2726, Val Loss: 0.0879\n",
      "Epoch 174/200, Train Loss: 0.2866, Val Loss: 0.1084\n",
      "Epoch 175/200, Train Loss: 0.2731, Val Loss: 0.0794\n",
      "Epoch 176/200, Train Loss: 0.2630, Val Loss: 0.0810\n",
      "Epoch 177/200, Train Loss: 0.2694, Val Loss: 0.0949\n",
      "Epoch 178/200, Train Loss: 0.2770, Val Loss: 0.0921\n",
      "Epoch 179/200, Train Loss: 0.2681, Val Loss: 0.0823\n",
      "Epoch 180/200, Train Loss: 0.2709, Val Loss: 0.0771\n",
      "Epoch 181/200, Train Loss: 0.2667, Val Loss: 0.0658\n",
      "Epoch 182/200, Train Loss: 0.2540, Val Loss: 0.0747\n",
      "Epoch 183/200, Train Loss: 0.2559, Val Loss: 0.0774\n",
      "Epoch 184/200, Train Loss: 0.2584, Val Loss: 0.0715\n",
      "Epoch 185/200, Train Loss: 0.2517, Val Loss: 0.0986\n",
      "Epoch 186/200, Train Loss: 0.2533, Val Loss: 0.0740\n",
      "Epoch 187/200, Train Loss: 0.2602, Val Loss: 0.0702\n",
      "Epoch 188/200, Train Loss: 0.2584, Val Loss: 0.1138\n",
      "Epoch 189/200, Train Loss: 0.2595, Val Loss: 0.0795\n",
      "Epoch 190/200, Train Loss: 0.2505, Val Loss: 0.0614\n",
      "Epoch 191/200, Train Loss: 0.2529, Val Loss: 0.0808\n",
      "Epoch 192/200, Train Loss: 0.2490, Val Loss: 0.0725\n",
      "Epoch 193/200, Train Loss: 0.2576, Val Loss: 0.0644\n",
      "Epoch 194/200, Train Loss: 0.2538, Val Loss: 0.0543\n",
      "  New best validation loss: 0.0543\n",
      "Epoch 195/200, Train Loss: 0.2440, Val Loss: 0.0767\n",
      "Epoch 196/200, Train Loss: 0.2436, Val Loss: 0.0602\n",
      "Epoch 197/200, Train Loss: 0.2325, Val Loss: 0.0669\n",
      "Epoch 198/200, Train Loss: 0.2395, Val Loss: 0.0564\n",
      "Epoch 199/200, Train Loss: 0.2380, Val Loss: 0.0706\n",
      "Epoch 200/200, Train Loss: 0.2457, Val Loss: 0.0695\n",
      "\n",
      "Loaded best model (Val Loss: 0.0543) for final hidden state extraction.\n",
      "Saved best model for Transformer to results/20250508_043138/Transformer_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for Transformer ---\n",
      "  Analyzing decodability for Transformer...\n",
      "  Hidden states shape: torch.Size([160, 200, 64])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 64])\n",
      "  RidgeCV Decoder for Transformer - Test MSE: 0.0056, Test R2: 0.9973 (best alpha: 0.0018)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decodability (R2 score) for Transformer: 0.9973\n",
      "\n",
      "--- Training HIPPORNN_LegT ---\n",
      "Number of parameters: 99716\n",
      "Epoch 1/200, Train Loss: 5.1424, Val Loss: 5.1597\n",
      "  New best validation loss: 5.1597\n",
      "Epoch 2/200, Train Loss: 5.0131, Val Loss: 5.1430\n",
      "  New best validation loss: 5.1430\n",
      "Epoch 3/200, Train Loss: 5.0040, Val Loss: 5.1367\n",
      "  New best validation loss: 5.1367\n",
      "Epoch 4/200, Train Loss: 4.9995, Val Loss: 5.1324\n",
      "  New best validation loss: 5.1324\n",
      "Epoch 5/200, Train Loss: 4.9971, Val Loss: 5.1320\n",
      "  New best validation loss: 5.1320\n",
      "Epoch 6/200, Train Loss: 4.9991, Val Loss: 5.1355\n",
      "Epoch 7/200, Train Loss: 5.0007, Val Loss: 5.1323\n",
      "Epoch 8/200, Train Loss: 4.9988, Val Loss: 5.1358\n",
      "Epoch 9/200, Train Loss: 4.9960, Val Loss: 5.1320\n",
      "Epoch 10/200, Train Loss: 4.9965, Val Loss: 5.1308\n",
      "  New best validation loss: 5.1308\n",
      "Epoch 11/200, Train Loss: 4.9952, Val Loss: 5.1330\n",
      "Epoch 12/200, Train Loss: 4.9960, Val Loss: 5.1310\n",
      "Epoch 13/200, Train Loss: 4.9962, Val Loss: 5.1275\n",
      "  New best validation loss: 5.1275\n",
      "Epoch 14/200, Train Loss: 4.9983, Val Loss: 5.1323\n",
      "Epoch 15/200, Train Loss: 4.9941, Val Loss: 5.1270\n",
      "  New best validation loss: 5.1270\n",
      "Epoch 16/200, Train Loss: 4.9917, Val Loss: 5.1254\n",
      "  New best validation loss: 5.1254\n",
      "Epoch 17/200, Train Loss: 4.9910, Val Loss: 5.1271\n",
      "Epoch 18/200, Train Loss: 4.9907, Val Loss: 5.1240\n",
      "  New best validation loss: 5.1240\n",
      "Epoch 19/200, Train Loss: 4.9902, Val Loss: 5.1254\n",
      "Epoch 20/200, Train Loss: 4.9856, Val Loss: 5.1217\n",
      "  New best validation loss: 5.1217\n",
      "Epoch 21/200, Train Loss: 4.9848, Val Loss: 5.1161\n",
      "  New best validation loss: 5.1161\n",
      "Epoch 22/200, Train Loss: 4.9750, Val Loss: 5.0786\n",
      "  New best validation loss: 5.0786\n",
      "Epoch 23/200, Train Loss: 4.8927, Val Loss: 5.0315\n",
      "  New best validation loss: 5.0315\n",
      "Epoch 24/200, Train Loss: 4.8500, Val Loss: 5.0187\n",
      "  New best validation loss: 5.0187\n",
      "Epoch 25/200, Train Loss: 4.8022, Val Loss: 5.0112\n",
      "  New best validation loss: 5.0112\n",
      "Epoch 26/200, Train Loss: 4.7998, Val Loss: 4.9505\n",
      "  New best validation loss: 4.9505\n",
      "Epoch 27/200, Train Loss: 4.7317, Val Loss: 4.8293\n",
      "  New best validation loss: 4.8293\n",
      "Epoch 28/200, Train Loss: 4.6877, Val Loss: 4.8263\n",
      "  New best validation loss: 4.8263\n",
      "Epoch 29/200, Train Loss: 4.6709, Val Loss: 4.8150\n",
      "  New best validation loss: 4.8150\n",
      "Epoch 30/200, Train Loss: 4.6764, Val Loss: 4.8184\n",
      "Epoch 31/200, Train Loss: 4.6942, Val Loss: 4.8015\n",
      "  New best validation loss: 4.8015\n",
      "Epoch 32/200, Train Loss: 4.6802, Val Loss: 4.8507\n",
      "Epoch 33/200, Train Loss: 4.6916, Val Loss: 4.8190\n",
      "Epoch 34/200, Train Loss: 4.6832, Val Loss: 4.8487\n",
      "Epoch 35/200, Train Loss: 4.6779, Val Loss: 4.8428\n",
      "Epoch 36/200, Train Loss: 4.6804, Val Loss: 4.8359\n",
      "Epoch 37/200, Train Loss: 4.6829, Val Loss: 4.8311\n",
      "Epoch 38/200, Train Loss: 4.6740, Val Loss: 4.8279\n",
      "Epoch 39/200, Train Loss: 4.6814, Val Loss: 4.8239\n",
      "Epoch 40/200, Train Loss: 4.6738, Val Loss: 4.8258\n",
      "Epoch 41/200, Train Loss: 4.6713, Val Loss: 4.8613\n",
      "Epoch 42/200, Train Loss: 4.9849, Val Loss: 5.2810\n",
      "Epoch 43/200, Train Loss: 5.0751, Val Loss: 5.1186\n",
      "Epoch 44/200, Train Loss: 4.9650, Val Loss: 5.0969\n",
      "Epoch 45/200, Train Loss: 4.9512, Val Loss: 5.0902\n",
      "Epoch 46/200, Train Loss: 4.9519, Val Loss: 5.0911\n",
      "Epoch 47/200, Train Loss: 4.9511, Val Loss: 5.0812\n",
      "Epoch 48/200, Train Loss: 4.9475, Val Loss: 5.0782\n",
      "Epoch 49/200, Train Loss: 4.9453, Val Loss: 5.0772\n",
      "Epoch 50/200, Train Loss: 4.9413, Val Loss: 5.0778\n",
      "Epoch 51/200, Train Loss: 4.9367, Val Loss: 5.0752\n",
      "Epoch 52/200, Train Loss: 4.9358, Val Loss: 5.0722\n",
      "Epoch 53/200, Train Loss: 4.9354, Val Loss: 5.0710\n",
      "Epoch 54/200, Train Loss: 4.9355, Val Loss: 5.0919\n",
      "Epoch 55/200, Train Loss: 4.9350, Val Loss: 5.0751\n",
      "Epoch 56/200, Train Loss: 4.9263, Val Loss: 5.0672\n",
      "Epoch 57/200, Train Loss: 4.9246, Val Loss: 5.0612\n",
      "Epoch 58/200, Train Loss: 4.9227, Val Loss: 5.0640\n",
      "Epoch 59/200, Train Loss: 4.9214, Val Loss: 5.0663\n",
      "Epoch 60/200, Train Loss: 4.9213, Val Loss: 5.0601\n",
      "Epoch 61/200, Train Loss: 4.9147, Val Loss: 5.0568\n",
      "Epoch 62/200, Train Loss: 4.9132, Val Loss: 5.0521\n",
      "Epoch 63/200, Train Loss: 4.9087, Val Loss: 5.0502\n",
      "Epoch 64/200, Train Loss: 4.9078, Val Loss: 5.0459\n",
      "Epoch 65/200, Train Loss: 4.9041, Val Loss: 5.0487\n",
      "Epoch 66/200, Train Loss: 4.9050, Val Loss: 5.0481\n",
      "Epoch 67/200, Train Loss: 4.9024, Val Loss: 5.0409\n",
      "Epoch 68/200, Train Loss: 4.8985, Val Loss: 5.0370\n",
      "Epoch 69/200, Train Loss: 4.8936, Val Loss: 5.0299\n",
      "Epoch 70/200, Train Loss: 4.8895, Val Loss: 5.0239\n",
      "Epoch 71/200, Train Loss: 4.8866, Val Loss: 5.0259\n",
      "Epoch 72/200, Train Loss: 4.8825, Val Loss: 5.0215\n",
      "Epoch 73/200, Train Loss: 4.8861, Val Loss: 5.0288\n",
      "Epoch 74/200, Train Loss: 4.8813, Val Loss: 5.0235\n",
      "Epoch 75/200, Train Loss: 4.8802, Val Loss: 5.0183\n",
      "Epoch 76/200, Train Loss: 4.8791, Val Loss: 5.0180\n",
      "Epoch 77/200, Train Loss: 4.8697, Val Loss: 5.0104\n",
      "Epoch 78/200, Train Loss: 4.8660, Val Loss: 5.0077\n",
      "Epoch 79/200, Train Loss: 4.8637, Val Loss: 5.0174\n",
      "Epoch 80/200, Train Loss: 4.8681, Val Loss: 5.0131\n",
      "Epoch 81/200, Train Loss: 4.8613, Val Loss: 5.0058\n",
      "Epoch 82/200, Train Loss: 4.8542, Val Loss: 4.9972\n",
      "Epoch 83/200, Train Loss: 4.8473, Val Loss: 4.9936\n",
      "Epoch 84/200, Train Loss: 4.8434, Val Loss: 4.9858\n",
      "Epoch 85/200, Train Loss: 4.8422, Val Loss: 4.9903\n",
      "Epoch 86/200, Train Loss: 4.8322, Val Loss: 4.9767\n",
      "Epoch 87/200, Train Loss: 4.8223, Val Loss: 4.9691\n",
      "Epoch 88/200, Train Loss: 4.8160, Val Loss: 4.9624\n",
      "Epoch 89/200, Train Loss: 4.8114, Val Loss: 4.9595\n",
      "Epoch 90/200, Train Loss: 4.8062, Val Loss: 4.9509\n",
      "Epoch 91/200, Train Loss: 4.7995, Val Loss: 4.9462\n",
      "Epoch 92/200, Train Loss: 4.7975, Val Loss: 4.9405\n",
      "Epoch 93/200, Train Loss: 4.7941, Val Loss: 4.9305\n",
      "Epoch 94/200, Train Loss: 4.7893, Val Loss: 4.9514\n",
      "Epoch 95/200, Train Loss: 4.7886, Val Loss: 4.9320\n",
      "Epoch 96/200, Train Loss: 4.7746, Val Loss: 4.9364\n",
      "Epoch 97/200, Train Loss: 4.7854, Val Loss: 4.9520\n",
      "Epoch 98/200, Train Loss: 4.7914, Val Loss: 4.9432\n",
      "Epoch 99/200, Train Loss: 4.7820, Val Loss: 4.9340\n",
      "Epoch 100/200, Train Loss: 4.7756, Val Loss: 4.9270\n",
      "Epoch 101/200, Train Loss: 4.7692, Val Loss: 4.9225\n",
      "Epoch 102/200, Train Loss: 4.7629, Val Loss: 4.9175\n",
      "Epoch 103/200, Train Loss: 4.7562, Val Loss: 4.9116\n",
      "Epoch 104/200, Train Loss: 4.7514, Val Loss: 4.9068\n",
      "Epoch 105/200, Train Loss: 4.7464, Val Loss: 4.9031\n",
      "Epoch 106/200, Train Loss: 4.7400, Val Loss: 4.8980\n",
      "Epoch 107/200, Train Loss: 4.7364, Val Loss: 4.8878\n",
      "Epoch 108/200, Train Loss: 4.7336, Val Loss: 4.8828\n",
      "Epoch 109/200, Train Loss: 4.7218, Val Loss: 4.8803\n",
      "Epoch 110/200, Train Loss: 4.7176, Val Loss: 4.8747\n",
      "Epoch 111/200, Train Loss: 4.7107, Val Loss: 4.8733\n",
      "Epoch 112/200, Train Loss: 4.7071, Val Loss: 4.8693\n",
      "Epoch 113/200, Train Loss: 4.7020, Val Loss: 4.8623\n",
      "Epoch 114/200, Train Loss: 4.6943, Val Loss: 4.8550\n",
      "Epoch 115/200, Train Loss: 4.6875, Val Loss: 4.8470\n",
      "Epoch 116/200, Train Loss: 4.6847, Val Loss: 4.8363\n",
      "Epoch 117/200, Train Loss: 4.6845, Val Loss: 4.8599\n",
      "Epoch 118/200, Train Loss: 4.6916, Val Loss: 4.8550\n",
      "Epoch 119/200, Train Loss: 4.6822, Val Loss: 4.8312\n",
      "Epoch 120/200, Train Loss: 4.6698, Val Loss: 4.8302\n",
      "Epoch 121/200, Train Loss: 4.6644, Val Loss: 4.8255\n",
      "Epoch 122/200, Train Loss: 4.6599, Val Loss: 4.8085\n",
      "Epoch 123/200, Train Loss: 4.6565, Val Loss: 4.8022\n",
      "Epoch 124/200, Train Loss: 4.6506, Val Loss: 4.8037\n",
      "Epoch 125/200, Train Loss: 4.6438, Val Loss: 4.7957\n",
      "  New best validation loss: 4.7957\n",
      "Epoch 126/200, Train Loss: 4.6320, Val Loss: 4.7930\n",
      "  New best validation loss: 4.7930\n",
      "Epoch 127/200, Train Loss: 4.6297, Val Loss: 4.7836\n",
      "  New best validation loss: 4.7836\n",
      "Epoch 128/200, Train Loss: 4.6127, Val Loss: 4.7853\n",
      "Epoch 129/200, Train Loss: 4.6095, Val Loss: 4.7814\n",
      "  New best validation loss: 4.7814\n",
      "Epoch 130/200, Train Loss: 4.5960, Val Loss: 4.7746\n",
      "  New best validation loss: 4.7746\n",
      "Epoch 131/200, Train Loss: 4.5905, Val Loss: 4.7725\n",
      "  New best validation loss: 4.7725\n",
      "Epoch 132/200, Train Loss: 4.5842, Val Loss: 4.7651\n",
      "  New best validation loss: 4.7651\n",
      "Epoch 133/200, Train Loss: 4.5977, Val Loss: 4.7652\n",
      "Epoch 134/200, Train Loss: 4.5915, Val Loss: 4.7606\n",
      "  New best validation loss: 4.7606\n",
      "Epoch 135/200, Train Loss: 4.5874, Val Loss: 4.7546\n",
      "  New best validation loss: 4.7546\n",
      "Epoch 136/200, Train Loss: 4.5815, Val Loss: 4.7494\n",
      "  New best validation loss: 4.7494\n",
      "Epoch 137/200, Train Loss: 4.5786, Val Loss: 4.7434\n",
      "  New best validation loss: 4.7434\n",
      "Epoch 138/200, Train Loss: 4.5743, Val Loss: 4.7420\n",
      "  New best validation loss: 4.7420\n",
      "Epoch 139/200, Train Loss: 4.5724, Val Loss: 4.7284\n",
      "  New best validation loss: 4.7284\n",
      "Epoch 140/200, Train Loss: 4.5654, Val Loss: 4.7262\n",
      "  New best validation loss: 4.7262\n",
      "Epoch 141/200, Train Loss: 4.5652, Val Loss: 4.7291\n",
      "Epoch 142/200, Train Loss: 4.5519, Val Loss: 4.7252\n",
      "  New best validation loss: 4.7252\n",
      "Epoch 143/200, Train Loss: 4.5534, Val Loss: 4.7224\n",
      "  New best validation loss: 4.7224\n",
      "Epoch 144/200, Train Loss: 4.5533, Val Loss: 4.7167\n",
      "  New best validation loss: 4.7167\n",
      "Epoch 145/200, Train Loss: 4.5972, Val Loss: 5.0036\n",
      "Epoch 146/200, Train Loss: 4.6656, Val Loss: 4.7541\n",
      "Epoch 147/200, Train Loss: 4.6195, Val Loss: 4.7169\n",
      "Epoch 148/200, Train Loss: 4.6022, Val Loss: 4.7105\n",
      "  New best validation loss: 4.7105\n",
      "Epoch 149/200, Train Loss: 4.5982, Val Loss: 4.7138\n",
      "Epoch 150/200, Train Loss: 4.5950, Val Loss: 4.7097\n",
      "  New best validation loss: 4.7097\n",
      "Epoch 151/200, Train Loss: 4.5981, Val Loss: 4.7251\n",
      "Epoch 152/200, Train Loss: 4.5991, Val Loss: 4.7345\n",
      "Epoch 153/200, Train Loss: 4.5950, Val Loss: 4.7287\n",
      "Epoch 154/200, Train Loss: 4.5954, Val Loss: 4.7247\n",
      "Epoch 155/200, Train Loss: 4.5933, Val Loss: 4.7263\n",
      "Epoch 156/200, Train Loss: 4.5914, Val Loss: 4.7263\n",
      "Epoch 157/200, Train Loss: 4.5868, Val Loss: 4.7159\n",
      "Epoch 158/200, Train Loss: 4.5910, Val Loss: 4.7161\n",
      "Epoch 159/200, Train Loss: 4.5867, Val Loss: 4.7165\n",
      "Epoch 160/200, Train Loss: 4.5868, Val Loss: 4.7147\n",
      "Epoch 161/200, Train Loss: 4.5864, Val Loss: 4.7200\n",
      "Epoch 162/200, Train Loss: 4.5847, Val Loss: 4.7182\n",
      "Epoch 163/200, Train Loss: 4.5842, Val Loss: 4.7235\n",
      "Epoch 164/200, Train Loss: 4.5843, Val Loss: 4.7211\n",
      "Epoch 165/200, Train Loss: 4.5828, Val Loss: 4.7197\n",
      "Epoch 166/200, Train Loss: 4.5838, Val Loss: 4.7166\n",
      "Epoch 167/200, Train Loss: 4.5821, Val Loss: 4.7158\n",
      "Epoch 168/200, Train Loss: 4.5838, Val Loss: 4.7153\n",
      "Epoch 169/200, Train Loss: 4.5816, Val Loss: 4.7157\n",
      "Epoch 170/200, Train Loss: 4.5811, Val Loss: 4.7155\n",
      "Epoch 171/200, Train Loss: 4.5821, Val Loss: 4.7084\n",
      "  New best validation loss: 4.7084\n",
      "Epoch 172/200, Train Loss: 4.5801, Val Loss: 4.7107\n",
      "Epoch 173/200, Train Loss: 4.5797, Val Loss: 4.7098\n",
      "Epoch 174/200, Train Loss: 4.5784, Val Loss: 4.7111\n",
      "Epoch 175/200, Train Loss: 4.5781, Val Loss: 4.7115\n",
      "Epoch 176/200, Train Loss: 4.5783, Val Loss: 4.7086\n",
      "Epoch 177/200, Train Loss: 4.5765, Val Loss: 4.7109\n",
      "Epoch 178/200, Train Loss: 4.5746, Val Loss: 4.7107\n",
      "Epoch 179/200, Train Loss: 4.5754, Val Loss: 4.7108\n",
      "Epoch 180/200, Train Loss: 4.5766, Val Loss: 4.7105\n",
      "Epoch 181/200, Train Loss: 4.5776, Val Loss: 4.7120\n",
      "Epoch 182/200, Train Loss: 4.5782, Val Loss: 4.7109\n",
      "Epoch 183/200, Train Loss: 4.5758, Val Loss: 4.7089\n",
      "Epoch 184/200, Train Loss: 4.5744, Val Loss: 4.7059\n",
      "  New best validation loss: 4.7059\n",
      "Epoch 185/200, Train Loss: 4.5741, Val Loss: 4.7078\n",
      "Epoch 186/200, Train Loss: 4.5736, Val Loss: 4.7091\n",
      "Epoch 187/200, Train Loss: 4.5734, Val Loss: 4.7091\n",
      "Epoch 188/200, Train Loss: 4.5739, Val Loss: 4.7084\n",
      "Epoch 189/200, Train Loss: 4.5763, Val Loss: 4.7045\n",
      "  New best validation loss: 4.7045\n",
      "Epoch 190/200, Train Loss: 4.5760, Val Loss: 4.7011\n",
      "  New best validation loss: 4.7011\n",
      "Epoch 191/200, Train Loss: 4.5732, Val Loss: 4.7023\n",
      "Epoch 192/200, Train Loss: 4.5730, Val Loss: 4.7052\n",
      "Epoch 193/200, Train Loss: 4.5736, Val Loss: 4.7023\n",
      "Epoch 194/200, Train Loss: 4.5727, Val Loss: 4.7029\n",
      "Epoch 195/200, Train Loss: 4.5736, Val Loss: 4.7021\n",
      "Epoch 196/200, Train Loss: 4.5736, Val Loss: 4.7050\n",
      "Epoch 197/200, Train Loss: 4.5743, Val Loss: 4.7007\n",
      "  New best validation loss: 4.7007\n",
      "Epoch 198/200, Train Loss: 4.5734, Val Loss: 4.7018\n",
      "Epoch 199/200, Train Loss: 4.5728, Val Loss: 4.7025\n",
      "Epoch 200/200, Train Loss: 4.5723, Val Loss: 4.7038\n",
      "\n",
      "Loaded best model (Val Loss: 4.7007) for final hidden state extraction.\n",
      "Saved best model for HIPPORNN_LegT to results/20250508_043138/HIPPORNN_LegT_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for HIPPORNN_LegT ---\n",
      "  Analyzing decodability for HIPPORNN_LegT...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for HIPPORNN_LegT - Test MSE: 1.4030, Test R2: 0.3160 (best alpha: 0.0001)\n",
      "Decodability (R2 score) for HIPPORNN_LegT: 0.3160\n",
      "\n",
      "--- Training NMRNN_Spatial_ModReadout ---\n",
      "Number of parameters: 66832\n",
      "Epoch 1/200, Train Loss: 4.8398, Val Loss: 5.2257\n",
      "  New best validation loss: 5.2257\n",
      "Epoch 2/200, Train Loss: 4.7008, Val Loss: 4.9290\n",
      "  New best validation loss: 4.9290\n",
      "Epoch 3/200, Train Loss: 4.6273, Val Loss: 4.7932\n",
      "  New best validation loss: 4.7932\n",
      "Epoch 4/200, Train Loss: 4.5279, Val Loss: 4.7123\n",
      "  New best validation loss: 4.7123\n",
      "Epoch 5/200, Train Loss: 4.4619, Val Loss: 4.6278\n",
      "  New best validation loss: 4.6278\n",
      "Epoch 6/200, Train Loss: 4.3261, Val Loss: 4.4514\n",
      "  New best validation loss: 4.4514\n",
      "Epoch 7/200, Train Loss: 4.1500, Val Loss: 4.2777\n",
      "  New best validation loss: 4.2777\n",
      "Epoch 8/200, Train Loss: 3.9685, Val Loss: 4.2263\n",
      "  New best validation loss: 4.2263\n",
      "Epoch 9/200, Train Loss: 3.8286, Val Loss: 4.1152\n",
      "  New best validation loss: 4.1152\n",
      "Epoch 10/200, Train Loss: 3.6924, Val Loss: 4.0171\n",
      "  New best validation loss: 4.0171\n",
      "Epoch 11/200, Train Loss: 3.6062, Val Loss: 3.8555\n",
      "  New best validation loss: 3.8555\n",
      "Epoch 12/200, Train Loss: 3.5600, Val Loss: 3.7416\n",
      "  New best validation loss: 3.7416\n",
      "Epoch 13/200, Train Loss: 3.3967, Val Loss: 3.6800\n",
      "  New best validation loss: 3.6800\n",
      "Epoch 14/200, Train Loss: 3.3378, Val Loss: 3.5075\n",
      "  New best validation loss: 3.5075\n",
      "Epoch 15/200, Train Loss: 3.1724, Val Loss: 3.4072\n",
      "  New best validation loss: 3.4072\n",
      "Epoch 16/200, Train Loss: 3.0981, Val Loss: 3.2661\n",
      "  New best validation loss: 3.2661\n",
      "Epoch 17/200, Train Loss: 2.9801, Val Loss: 3.3932\n",
      "Epoch 18/200, Train Loss: 2.9085, Val Loss: 3.2711\n",
      "Epoch 19/200, Train Loss: 2.9816, Val Loss: 3.3921\n",
      "Epoch 20/200, Train Loss: 2.9412, Val Loss: 3.1654\n",
      "  New best validation loss: 3.1654\n",
      "Epoch 21/200, Train Loss: 2.7590, Val Loss: 3.1803\n",
      "Epoch 22/200, Train Loss: 2.8214, Val Loss: 3.0417\n",
      "  New best validation loss: 3.0417\n",
      "Epoch 23/200, Train Loss: 2.6808, Val Loss: 2.9486\n",
      "  New best validation loss: 2.9486\n",
      "Epoch 24/200, Train Loss: 2.6283, Val Loss: 2.8970\n",
      "  New best validation loss: 2.8970\n",
      "Epoch 25/200, Train Loss: 2.5991, Val Loss: 2.8689\n",
      "  New best validation loss: 2.8689\n",
      "Epoch 26/200, Train Loss: 2.5347, Val Loss: 2.8122\n",
      "  New best validation loss: 2.8122\n",
      "Epoch 27/200, Train Loss: 2.6610, Val Loss: 3.0824\n",
      "Epoch 28/200, Train Loss: 2.5374, Val Loss: 2.7176\n",
      "  New best validation loss: 2.7176\n",
      "Epoch 29/200, Train Loss: 2.4867, Val Loss: 2.7296\n",
      "Epoch 30/200, Train Loss: 2.4578, Val Loss: 2.7211\n",
      "Epoch 31/200, Train Loss: 2.4000, Val Loss: 2.6019\n",
      "  New best validation loss: 2.6019\n",
      "Epoch 32/200, Train Loss: 2.3842, Val Loss: 2.5396\n",
      "  New best validation loss: 2.5396\n",
      "Epoch 33/200, Train Loss: 2.3359, Val Loss: 2.6283\n",
      "Epoch 34/200, Train Loss: 2.3219, Val Loss: 2.6559\n",
      "Epoch 35/200, Train Loss: 2.3256, Val Loss: 2.5792\n",
      "Epoch 36/200, Train Loss: 2.3052, Val Loss: 2.4982\n",
      "  New best validation loss: 2.4982\n",
      "Epoch 37/200, Train Loss: 2.2771, Val Loss: 2.4855\n",
      "  New best validation loss: 2.4855\n",
      "Epoch 38/200, Train Loss: 2.2000, Val Loss: 2.3938\n",
      "  New best validation loss: 2.3938\n",
      "Epoch 39/200, Train Loss: 2.1532, Val Loss: 2.3707\n",
      "  New best validation loss: 2.3707\n",
      "Epoch 40/200, Train Loss: 2.1630, Val Loss: 2.5207\n",
      "Epoch 41/200, Train Loss: 2.2278, Val Loss: 2.4800\n",
      "Epoch 42/200, Train Loss: 2.1565, Val Loss: 2.3140\n",
      "  New best validation loss: 2.3140\n",
      "Epoch 43/200, Train Loss: 2.1229, Val Loss: 2.3052\n",
      "  New best validation loss: 2.3052\n",
      "Epoch 44/200, Train Loss: 2.0625, Val Loss: 2.2989\n",
      "  New best validation loss: 2.2989\n",
      "Epoch 45/200, Train Loss: 2.1308, Val Loss: 2.6235\n",
      "Epoch 46/200, Train Loss: 2.2008, Val Loss: 2.4152\n",
      "Epoch 47/200, Train Loss: 2.0663, Val Loss: 2.2482\n",
      "  New best validation loss: 2.2482\n",
      "Epoch 48/200, Train Loss: 2.0063, Val Loss: 2.2478\n",
      "  New best validation loss: 2.2478\n",
      "Epoch 49/200, Train Loss: 1.9934, Val Loss: 2.2859\n",
      "Epoch 50/200, Train Loss: 2.0421, Val Loss: 2.1952\n",
      "  New best validation loss: 2.1952\n",
      "Epoch 51/200, Train Loss: 1.9606, Val Loss: 2.1617\n",
      "  New best validation loss: 2.1617\n",
      "Epoch 52/200, Train Loss: 1.9478, Val Loss: 2.1287\n",
      "  New best validation loss: 2.1287\n",
      "Epoch 53/200, Train Loss: 1.9338, Val Loss: 2.1247\n",
      "  New best validation loss: 2.1247\n",
      "Epoch 54/200, Train Loss: 1.9202, Val Loss: 2.1265\n",
      "Epoch 55/200, Train Loss: 1.8899, Val Loss: 2.0856\n",
      "  New best validation loss: 2.0856\n",
      "Epoch 56/200, Train Loss: 1.8527, Val Loss: 2.0469\n",
      "  New best validation loss: 2.0469\n",
      "Epoch 57/200, Train Loss: 1.8485, Val Loss: 2.1156\n",
      "Epoch 58/200, Train Loss: 1.8673, Val Loss: 1.9921\n",
      "  New best validation loss: 1.9921\n",
      "Epoch 59/200, Train Loss: 1.8489, Val Loss: 2.1486\n",
      "Epoch 60/200, Train Loss: 1.8687, Val Loss: 1.9288\n",
      "  New best validation loss: 1.9288\n",
      "Epoch 61/200, Train Loss: 1.8293, Val Loss: 1.9303\n",
      "Epoch 62/200, Train Loss: 1.8052, Val Loss: 2.0179\n",
      "Epoch 63/200, Train Loss: 1.7918, Val Loss: 1.9721\n",
      "Epoch 64/200, Train Loss: 1.8061, Val Loss: 1.9987\n",
      "Epoch 65/200, Train Loss: 1.7826, Val Loss: 1.9062\n",
      "  New best validation loss: 1.9062\n",
      "Epoch 66/200, Train Loss: 1.7455, Val Loss: 1.9503\n",
      "Epoch 67/200, Train Loss: 1.7478, Val Loss: 1.8749\n",
      "  New best validation loss: 1.8749\n",
      "Epoch 68/200, Train Loss: 1.7671, Val Loss: 2.1801\n",
      "Epoch 69/200, Train Loss: 1.7678, Val Loss: 1.8729\n",
      "  New best validation loss: 1.8729\n",
      "Epoch 70/200, Train Loss: 1.7321, Val Loss: 1.8812\n",
      "Epoch 71/200, Train Loss: 1.7575, Val Loss: 2.0063\n",
      "Epoch 72/200, Train Loss: 1.8267, Val Loss: 1.9652\n",
      "Epoch 73/200, Train Loss: 1.7756, Val Loss: 1.9522\n",
      "Epoch 74/200, Train Loss: 1.7086, Val Loss: 1.8476\n",
      "  New best validation loss: 1.8476\n",
      "Epoch 75/200, Train Loss: 1.7134, Val Loss: 1.8564\n",
      "Epoch 76/200, Train Loss: 1.6993, Val Loss: 1.8344\n",
      "  New best validation loss: 1.8344\n",
      "Epoch 77/200, Train Loss: 1.6812, Val Loss: 1.8534\n",
      "Epoch 78/200, Train Loss: 1.6709, Val Loss: 1.8139\n",
      "  New best validation loss: 1.8139\n",
      "Epoch 79/200, Train Loss: 1.7179, Val Loss: 1.9602\n",
      "Epoch 80/200, Train Loss: 1.7152, Val Loss: 1.7860\n",
      "  New best validation loss: 1.7860\n",
      "Epoch 81/200, Train Loss: 1.6592, Val Loss: 1.7626\n",
      "  New best validation loss: 1.7626\n",
      "Epoch 82/200, Train Loss: 1.6301, Val Loss: 1.9416\n",
      "Epoch 83/200, Train Loss: 1.6791, Val Loss: 1.8159\n",
      "Epoch 84/200, Train Loss: 1.6503, Val Loss: 1.7694\n",
      "Epoch 85/200, Train Loss: 1.6389, Val Loss: 1.7380\n",
      "  New best validation loss: 1.7380\n",
      "Epoch 86/200, Train Loss: 1.6518, Val Loss: 1.7503\n",
      "Epoch 87/200, Train Loss: 1.7295, Val Loss: 1.7845\n",
      "Epoch 88/200, Train Loss: 1.6487, Val Loss: 1.7673\n",
      "Epoch 89/200, Train Loss: 1.6294, Val Loss: 1.7605\n",
      "Epoch 90/200, Train Loss: 1.6005, Val Loss: 1.7106\n",
      "  New best validation loss: 1.7106\n",
      "Epoch 91/200, Train Loss: 1.5872, Val Loss: 1.8022\n",
      "Epoch 92/200, Train Loss: 1.7280, Val Loss: 1.7314\n",
      "Epoch 93/200, Train Loss: 1.6665, Val Loss: 1.7524\n",
      "Epoch 94/200, Train Loss: 1.6313, Val Loss: 1.8349\n",
      "Epoch 95/200, Train Loss: 1.6717, Val Loss: 1.7208\n",
      "Epoch 96/200, Train Loss: 1.5784, Val Loss: 1.7083\n",
      "  New best validation loss: 1.7083\n",
      "Epoch 97/200, Train Loss: 1.5737, Val Loss: 1.7838\n",
      "Epoch 98/200, Train Loss: 1.6188, Val Loss: 1.7421\n",
      "Epoch 99/200, Train Loss: 1.5910, Val Loss: 1.7167\n",
      "Epoch 100/200, Train Loss: 1.5804, Val Loss: 1.7007\n",
      "  New best validation loss: 1.7007\n",
      "Epoch 101/200, Train Loss: 1.5631, Val Loss: 1.7516\n",
      "Epoch 102/200, Train Loss: 1.6455, Val Loss: 1.8630\n",
      "Epoch 103/200, Train Loss: 1.6555, Val Loss: 1.7050\n",
      "Epoch 104/200, Train Loss: 1.5992, Val Loss: 1.9053\n",
      "Epoch 105/200, Train Loss: 1.6337, Val Loss: 1.6722\n",
      "  New best validation loss: 1.6722\n",
      "Epoch 106/200, Train Loss: 1.5428, Val Loss: 1.6320\n",
      "  New best validation loss: 1.6320\n",
      "Epoch 107/200, Train Loss: 1.5480, Val Loss: 1.6922\n",
      "Epoch 108/200, Train Loss: 1.5826, Val Loss: 1.7265\n",
      "Epoch 109/200, Train Loss: 1.5403, Val Loss: 1.6749\n",
      "Epoch 110/200, Train Loss: 1.5644, Val Loss: 1.7934\n",
      "Epoch 111/200, Train Loss: 1.5373, Val Loss: 1.6409\n",
      "Epoch 112/200, Train Loss: 1.5081, Val Loss: 1.6560\n",
      "Epoch 113/200, Train Loss: 1.5130, Val Loss: 1.6443\n",
      "Epoch 114/200, Train Loss: 1.4806, Val Loss: 1.6373\n",
      "Epoch 115/200, Train Loss: 1.5054, Val Loss: 1.6756\n",
      "Epoch 116/200, Train Loss: 1.5033, Val Loss: 1.6345\n",
      "Epoch 117/200, Train Loss: 1.4993, Val Loss: 1.6920\n",
      "Epoch 118/200, Train Loss: 1.5178, Val Loss: 1.6923\n",
      "Epoch 119/200, Train Loss: 1.4940, Val Loss: 1.6113\n",
      "  New best validation loss: 1.6113\n",
      "Epoch 120/200, Train Loss: 1.5062, Val Loss: 1.6353\n",
      "Epoch 121/200, Train Loss: 1.5136, Val Loss: 1.6576\n",
      "Epoch 122/200, Train Loss: 1.4981, Val Loss: 1.6416\n",
      "Epoch 123/200, Train Loss: 1.4741, Val Loss: 1.6201\n",
      "Epoch 124/200, Train Loss: 1.4583, Val Loss: 1.6113\n",
      "  New best validation loss: 1.6113\n",
      "Epoch 125/200, Train Loss: 1.5213, Val Loss: 1.6576\n",
      "Epoch 126/200, Train Loss: 1.4670, Val Loss: 1.6629\n",
      "Epoch 127/200, Train Loss: 1.4836, Val Loss: 1.5925\n",
      "  New best validation loss: 1.5925\n",
      "Epoch 128/200, Train Loss: 1.4851, Val Loss: 1.5845\n",
      "  New best validation loss: 1.5845\n",
      "Epoch 129/200, Train Loss: 1.4772, Val Loss: 1.6331\n",
      "Epoch 130/200, Train Loss: 1.4751, Val Loss: 1.6114\n",
      "Epoch 131/200, Train Loss: 1.4562, Val Loss: 1.6264\n",
      "Epoch 132/200, Train Loss: 1.4908, Val Loss: 1.6242\n",
      "Epoch 133/200, Train Loss: 1.4808, Val Loss: 1.5726\n",
      "  New best validation loss: 1.5726\n",
      "Epoch 134/200, Train Loss: 1.4231, Val Loss: 1.6016\n",
      "Epoch 135/200, Train Loss: 1.4364, Val Loss: 1.5995\n",
      "Epoch 136/200, Train Loss: 1.4372, Val Loss: 1.5740\n",
      "Epoch 137/200, Train Loss: 1.4656, Val Loss: 1.6121\n",
      "Epoch 138/200, Train Loss: 1.4628, Val Loss: 1.5488\n",
      "  New best validation loss: 1.5488\n",
      "Epoch 139/200, Train Loss: 1.4413, Val Loss: 1.5687\n",
      "Epoch 140/200, Train Loss: 1.4960, Val Loss: 1.6483\n",
      "Epoch 141/200, Train Loss: 1.4341, Val Loss: 1.7306\n",
      "Epoch 142/200, Train Loss: 1.5171, Val Loss: 1.8008\n",
      "Epoch 143/200, Train Loss: 1.5163, Val Loss: 1.5809\n",
      "Epoch 144/200, Train Loss: 1.4533, Val Loss: 1.5835\n",
      "Epoch 145/200, Train Loss: 1.4167, Val Loss: 1.5834\n",
      "Epoch 146/200, Train Loss: 1.4369, Val Loss: 1.6235\n",
      "Epoch 147/200, Train Loss: 1.4427, Val Loss: 1.6005\n",
      "Epoch 148/200, Train Loss: 1.4086, Val Loss: 1.5481\n",
      "  New best validation loss: 1.5481\n",
      "Epoch 149/200, Train Loss: 1.4170, Val Loss: 1.5554\n",
      "Epoch 150/200, Train Loss: 1.3982, Val Loss: 1.5312\n",
      "  New best validation loss: 1.5312\n",
      "Epoch 151/200, Train Loss: 1.3800, Val Loss: 1.5744\n",
      "Epoch 152/200, Train Loss: 1.3960, Val Loss: 1.6006\n",
      "Epoch 153/200, Train Loss: 1.3919, Val Loss: 1.5487\n",
      "Epoch 154/200, Train Loss: 1.4057, Val Loss: 1.6209\n",
      "Epoch 155/200, Train Loss: 1.4160, Val Loss: 1.5476\n",
      "Epoch 156/200, Train Loss: 1.3818, Val Loss: 1.5107\n",
      "  New best validation loss: 1.5107\n",
      "Epoch 157/200, Train Loss: 1.3695, Val Loss: 1.5503\n",
      "Epoch 158/200, Train Loss: 1.4191, Val Loss: 1.5727\n",
      "Epoch 159/200, Train Loss: 1.4037, Val Loss: 1.5356\n",
      "Epoch 160/200, Train Loss: 1.3826, Val Loss: 1.5062\n",
      "  New best validation loss: 1.5062\n",
      "Epoch 161/200, Train Loss: 1.3834, Val Loss: 1.5473\n",
      "Epoch 162/200, Train Loss: 1.3679, Val Loss: 1.5140\n",
      "Epoch 163/200, Train Loss: 1.3505, Val Loss: 1.5530\n",
      "Epoch 164/200, Train Loss: 1.3915, Val Loss: 1.5167\n",
      "Epoch 165/200, Train Loss: 1.3725, Val Loss: 1.5760\n",
      "Epoch 166/200, Train Loss: 1.3687, Val Loss: 1.4687\n",
      "  New best validation loss: 1.4687\n",
      "Epoch 167/200, Train Loss: 1.3720, Val Loss: 1.5820\n",
      "Epoch 168/200, Train Loss: 1.3975, Val Loss: 1.5468\n",
      "Epoch 169/200, Train Loss: 1.3629, Val Loss: 1.5134\n",
      "Epoch 170/200, Train Loss: 1.3392, Val Loss: 1.4802\n",
      "Epoch 171/200, Train Loss: 1.3441, Val Loss: 1.5373\n",
      "Epoch 172/200, Train Loss: 1.3355, Val Loss: 1.5115\n",
      "Epoch 173/200, Train Loss: 1.3505, Val Loss: 1.5235\n",
      "Epoch 174/200, Train Loss: 1.3362, Val Loss: 1.4799\n",
      "Epoch 175/200, Train Loss: 1.3227, Val Loss: 1.4785\n",
      "Epoch 176/200, Train Loss: 1.3156, Val Loss: 1.4740\n",
      "Epoch 177/200, Train Loss: 1.3175, Val Loss: 1.4675\n",
      "  New best validation loss: 1.4675\n",
      "Epoch 178/200, Train Loss: 1.3071, Val Loss: 1.4809\n",
      "Epoch 179/200, Train Loss: 1.3465, Val Loss: 1.5688\n",
      "Epoch 180/200, Train Loss: 1.3652, Val Loss: 1.5342\n",
      "Epoch 181/200, Train Loss: 1.3480, Val Loss: 1.4974\n",
      "Epoch 182/200, Train Loss: 1.3333, Val Loss: 1.4947\n",
      "Epoch 183/200, Train Loss: 1.3348, Val Loss: 1.4738\n",
      "Epoch 184/200, Train Loss: 1.3271, Val Loss: 1.4924\n",
      "Epoch 185/200, Train Loss: 1.2988, Val Loss: 1.5031\n",
      "Epoch 186/200, Train Loss: 1.3163, Val Loss: 1.5025\n",
      "Epoch 187/200, Train Loss: 1.3265, Val Loss: 1.4839\n",
      "Epoch 188/200, Train Loss: 1.3022, Val Loss: 1.4431\n",
      "  New best validation loss: 1.4431\n",
      "Epoch 189/200, Train Loss: 1.3130, Val Loss: 1.5032\n",
      "Epoch 190/200, Train Loss: 1.3285, Val Loss: 1.5265\n",
      "Epoch 191/200, Train Loss: 1.3356, Val Loss: 1.5007\n",
      "Epoch 192/200, Train Loss: 1.3172, Val Loss: 1.4904\n",
      "Epoch 193/200, Train Loss: 1.3269, Val Loss: 1.4495\n",
      "Epoch 194/200, Train Loss: 1.2955, Val Loss: 1.4627\n",
      "Epoch 195/200, Train Loss: 1.2818, Val Loss: 1.4610\n",
      "Epoch 196/200, Train Loss: 1.2751, Val Loss: 1.4599\n",
      "Epoch 197/200, Train Loss: 1.2862, Val Loss: 1.4885\n",
      "Epoch 198/200, Train Loss: 1.3247, Val Loss: 1.5257\n",
      "Epoch 199/200, Train Loss: 1.3111, Val Loss: 1.4659\n",
      "Epoch 200/200, Train Loss: 1.2760, Val Loss: 1.4988\n",
      "\n",
      "Loaded best model (Val Loss: 1.4431) for final hidden state extraction.\n",
      "Saved best model for NMRNN_Spatial_ModReadout to results/20250508_043138/NMRNN_Spatial_ModReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_Spatial_ModReadout ---\n",
      "  Analyzing decodability for NMRNN_Spatial_ModReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_Spatial_ModReadout - Test MSE: 0.0548, Test R2: 0.9732 (best alpha: 2.6367)\n",
      "Decodability (R2 score) for NMRNN_Spatial_ModReadout: 0.9732\n",
      "\n",
      "--- Training NMRNN_NoSpatial_ModReadout ---\n",
      "Number of parameters: 66832\n",
      "Epoch 1/200, Train Loss: 4.8267, Val Loss: 5.2125\n",
      "  New best validation loss: 5.2125\n",
      "Epoch 2/200, Train Loss: 4.7041, Val Loss: 4.9662\n",
      "  New best validation loss: 4.9662\n",
      "Epoch 3/200, Train Loss: 4.6626, Val Loss: 4.9032\n",
      "  New best validation loss: 4.9032\n",
      "Epoch 4/200, Train Loss: 4.5697, Val Loss: 4.8280\n",
      "  New best validation loss: 4.8280\n",
      "Epoch 5/200, Train Loss: 4.4826, Val Loss: 4.6918\n",
      "  New best validation loss: 4.6918\n",
      "Epoch 6/200, Train Loss: 4.3409, Val Loss: 4.5045\n",
      "  New best validation loss: 4.5045\n",
      "Epoch 7/200, Train Loss: 4.1676, Val Loss: 4.3979\n",
      "  New best validation loss: 4.3979\n",
      "Epoch 8/200, Train Loss: 4.0104, Val Loss: 4.1805\n",
      "  New best validation loss: 4.1805\n",
      "Epoch 9/200, Train Loss: 3.8999, Val Loss: 4.0638\n",
      "  New best validation loss: 4.0638\n",
      "Epoch 10/200, Train Loss: 3.7903, Val Loss: 4.0798\n",
      "Epoch 11/200, Train Loss: 3.7034, Val Loss: 3.9974\n",
      "  New best validation loss: 3.9974\n",
      "Epoch 12/200, Train Loss: 3.6387, Val Loss: 3.8468\n",
      "  New best validation loss: 3.8468\n",
      "Epoch 13/200, Train Loss: 3.4984, Val Loss: 3.7450\n",
      "  New best validation loss: 3.7450\n",
      "Epoch 14/200, Train Loss: 3.4129, Val Loss: 3.6062\n",
      "  New best validation loss: 3.6062\n",
      "Epoch 15/200, Train Loss: 3.2908, Val Loss: 3.5463\n",
      "  New best validation loss: 3.5463\n",
      "Epoch 16/200, Train Loss: 3.2439, Val Loss: 3.5358\n",
      "  New best validation loss: 3.5358\n",
      "Epoch 17/200, Train Loss: 3.1880, Val Loss: 3.4053\n",
      "  New best validation loss: 3.4053\n",
      "Epoch 18/200, Train Loss: 3.0861, Val Loss: 3.2678\n",
      "  New best validation loss: 3.2678\n",
      "Epoch 19/200, Train Loss: 2.9972, Val Loss: 3.1983\n",
      "  New best validation loss: 3.1983\n",
      "Epoch 20/200, Train Loss: 2.9295, Val Loss: 3.1810\n",
      "  New best validation loss: 3.1810\n",
      "Epoch 21/200, Train Loss: 2.8997, Val Loss: 3.1401\n",
      "  New best validation loss: 3.1401\n",
      "Epoch 22/200, Train Loss: 2.8964, Val Loss: 3.0706\n",
      "  New best validation loss: 3.0706\n",
      "Epoch 23/200, Train Loss: 2.9179, Val Loss: 3.0736\n",
      "Epoch 24/200, Train Loss: 2.8249, Val Loss: 3.0855\n",
      "Epoch 25/200, Train Loss: 2.7359, Val Loss: 3.1240\n",
      "Epoch 26/200, Train Loss: 2.6993, Val Loss: 3.0510\n",
      "  New best validation loss: 3.0510\n",
      "Epoch 27/200, Train Loss: 2.6683, Val Loss: 2.9562\n",
      "  New best validation loss: 2.9562\n",
      "Epoch 28/200, Train Loss: 2.6608, Val Loss: 3.1401\n",
      "Epoch 29/200, Train Loss: 2.6783, Val Loss: 2.8176\n",
      "  New best validation loss: 2.8176\n",
      "Epoch 30/200, Train Loss: 2.6020, Val Loss: 2.8533\n",
      "Epoch 31/200, Train Loss: 2.5469, Val Loss: 2.7304\n",
      "  New best validation loss: 2.7304\n",
      "Epoch 32/200, Train Loss: 2.5125, Val Loss: 2.7353\n",
      "Epoch 33/200, Train Loss: 2.4930, Val Loss: 2.6929\n",
      "  New best validation loss: 2.6929\n",
      "Epoch 34/200, Train Loss: 2.4669, Val Loss: 2.7830\n",
      "Epoch 35/200, Train Loss: 2.5092, Val Loss: 2.7144\n",
      "Epoch 36/200, Train Loss: 2.4461, Val Loss: 2.6226\n",
      "  New best validation loss: 2.6226\n",
      "Epoch 37/200, Train Loss: 2.3906, Val Loss: 2.5408\n",
      "  New best validation loss: 2.5408\n",
      "Epoch 38/200, Train Loss: 2.3557, Val Loss: 2.4888\n",
      "  New best validation loss: 2.4888\n",
      "Epoch 39/200, Train Loss: 2.3327, Val Loss: 2.5372\n",
      "Epoch 40/200, Train Loss: 2.3011, Val Loss: 2.4979\n",
      "Epoch 41/200, Train Loss: 2.2865, Val Loss: 2.4765\n",
      "  New best validation loss: 2.4765\n",
      "Epoch 42/200, Train Loss: 2.2984, Val Loss: 2.4458\n",
      "  New best validation loss: 2.4458\n",
      "Epoch 43/200, Train Loss: 2.2300, Val Loss: 2.3694\n",
      "  New best validation loss: 2.3694\n",
      "Epoch 44/200, Train Loss: 2.1926, Val Loss: 2.4123\n",
      "Epoch 45/200, Train Loss: 2.1958, Val Loss: 2.3953\n",
      "Epoch 46/200, Train Loss: 2.1730, Val Loss: 2.2828\n",
      "  New best validation loss: 2.2828\n",
      "Epoch 47/200, Train Loss: 2.0773, Val Loss: 2.2421\n",
      "  New best validation loss: 2.2421\n",
      "Epoch 48/200, Train Loss: 2.0683, Val Loss: 2.1687\n",
      "  New best validation loss: 2.1687\n",
      "Epoch 49/200, Train Loss: 2.0580, Val Loss: 2.3003\n",
      "Epoch 50/200, Train Loss: 2.0867, Val Loss: 2.3182\n",
      "Epoch 51/200, Train Loss: 2.0700, Val Loss: 2.4051\n",
      "Epoch 52/200, Train Loss: 2.1885, Val Loss: 2.3104\n",
      "Epoch 53/200, Train Loss: 2.1220, Val Loss: 2.3355\n",
      "Epoch 54/200, Train Loss: 2.2727, Val Loss: 2.6434\n",
      "Epoch 55/200, Train Loss: 2.1478, Val Loss: 2.2379\n",
      "Epoch 56/200, Train Loss: 2.0517, Val Loss: 2.1791\n",
      "Epoch 57/200, Train Loss: 1.9829, Val Loss: 2.2069\n",
      "Epoch 58/200, Train Loss: 1.9765, Val Loss: 2.1023\n",
      "  New best validation loss: 2.1023\n",
      "Epoch 59/200, Train Loss: 1.9110, Val Loss: 2.1569\n",
      "Epoch 60/200, Train Loss: 1.8960, Val Loss: 2.0127\n",
      "  New best validation loss: 2.0127\n",
      "Epoch 61/200, Train Loss: 1.8746, Val Loss: 2.1610\n",
      "Epoch 62/200, Train Loss: 1.8801, Val Loss: 2.1061\n",
      "Epoch 63/200, Train Loss: 1.9901, Val Loss: 1.9933\n",
      "  New best validation loss: 1.9933\n",
      "Epoch 64/200, Train Loss: 1.8266, Val Loss: 2.0322\n",
      "Epoch 65/200, Train Loss: 1.8318, Val Loss: 1.9380\n",
      "  New best validation loss: 1.9380\n",
      "Epoch 66/200, Train Loss: 1.8421, Val Loss: 1.9855\n",
      "Epoch 67/200, Train Loss: 1.8041, Val Loss: 1.9111\n",
      "  New best validation loss: 1.9111\n",
      "Epoch 68/200, Train Loss: 1.7745, Val Loss: 1.9924\n",
      "Epoch 69/200, Train Loss: 1.8154, Val Loss: 1.9323\n",
      "Epoch 70/200, Train Loss: 1.9112, Val Loss: 2.1050\n",
      "Epoch 71/200, Train Loss: 1.8538, Val Loss: 1.9394\n",
      "Epoch 72/200, Train Loss: 1.7685, Val Loss: 1.9091\n",
      "  New best validation loss: 1.9091\n",
      "Epoch 73/200, Train Loss: 1.7277, Val Loss: 1.8609\n",
      "  New best validation loss: 1.8609\n",
      "Epoch 74/200, Train Loss: 1.7704, Val Loss: 1.9405\n",
      "Epoch 75/200, Train Loss: 1.7431, Val Loss: 1.9046\n",
      "Epoch 76/200, Train Loss: 1.7173, Val Loss: 1.8449\n",
      "  New best validation loss: 1.8449\n",
      "Epoch 77/200, Train Loss: 1.7406, Val Loss: 2.1001\n",
      "Epoch 78/200, Train Loss: 1.7596, Val Loss: 1.8327\n",
      "  New best validation loss: 1.8327\n",
      "Epoch 79/200, Train Loss: 1.7648, Val Loss: 1.9998\n",
      "Epoch 80/200, Train Loss: 1.7784, Val Loss: 1.8280\n",
      "  New best validation loss: 1.8280\n",
      "Epoch 81/200, Train Loss: 1.7067, Val Loss: 1.8155\n",
      "  New best validation loss: 1.8155\n",
      "Epoch 82/200, Train Loss: 1.6890, Val Loss: 1.7800\n",
      "  New best validation loss: 1.7800\n",
      "Epoch 83/200, Train Loss: 1.6798, Val Loss: 1.7863\n",
      "Epoch 84/200, Train Loss: 1.7027, Val Loss: 2.0515\n",
      "Epoch 85/200, Train Loss: 1.7786, Val Loss: 1.8232\n",
      "Epoch 86/200, Train Loss: 1.6914, Val Loss: 1.7893\n",
      "Epoch 87/200, Train Loss: 1.6821, Val Loss: 1.7748\n",
      "  New best validation loss: 1.7748\n",
      "Epoch 88/200, Train Loss: 1.6422, Val Loss: 1.8324\n",
      "Epoch 89/200, Train Loss: 1.6921, Val Loss: 1.7966\n",
      "Epoch 90/200, Train Loss: 1.6516, Val Loss: 1.7208\n",
      "  New best validation loss: 1.7208\n",
      "Epoch 91/200, Train Loss: 1.6416, Val Loss: 1.8293\n",
      "Epoch 92/200, Train Loss: 1.6772, Val Loss: 1.7341\n",
      "Epoch 93/200, Train Loss: 1.6320, Val Loss: 1.7352\n",
      "Epoch 94/200, Train Loss: 1.6819, Val Loss: 1.8107\n",
      "Epoch 95/200, Train Loss: 1.6694, Val Loss: 1.8349\n",
      "Epoch 96/200, Train Loss: 1.7964, Val Loss: 2.1305\n",
      "Epoch 97/200, Train Loss: 1.7594, Val Loss: 1.7933\n",
      "Epoch 98/200, Train Loss: 1.6778, Val Loss: 1.7539\n",
      "Epoch 99/200, Train Loss: 1.7130, Val Loss: 1.7909\n",
      "Epoch 100/200, Train Loss: 1.6747, Val Loss: 1.9148\n",
      "Epoch 101/200, Train Loss: 1.6993, Val Loss: 1.7277\n",
      "Epoch 102/200, Train Loss: 1.6573, Val Loss: 1.7872\n",
      "Epoch 103/200, Train Loss: 1.6028, Val Loss: 1.6718\n",
      "  New best validation loss: 1.6718\n",
      "Epoch 104/200, Train Loss: 1.6079, Val Loss: 1.7453\n",
      "Epoch 105/200, Train Loss: 1.5777, Val Loss: 1.6489\n",
      "  New best validation loss: 1.6489\n",
      "Epoch 106/200, Train Loss: 1.5581, Val Loss: 1.6689\n",
      "Epoch 107/200, Train Loss: 1.5749, Val Loss: 1.7126\n",
      "Epoch 108/200, Train Loss: 1.5943, Val Loss: 1.6918\n",
      "Epoch 109/200, Train Loss: 1.5667, Val Loss: 1.6672\n",
      "Epoch 110/200, Train Loss: 1.5988, Val Loss: 1.7758\n",
      "Epoch 111/200, Train Loss: 1.6155, Val Loss: 1.8608\n",
      "Epoch 112/200, Train Loss: 1.6946, Val Loss: 1.7312\n",
      "Epoch 113/200, Train Loss: 1.6900, Val Loss: 1.8323\n",
      "Epoch 114/200, Train Loss: 1.6619, Val Loss: 1.9401\n",
      "Epoch 115/200, Train Loss: 1.6928, Val Loss: 1.7498\n",
      "Epoch 116/200, Train Loss: 1.6416, Val Loss: 1.7067\n",
      "Epoch 117/200, Train Loss: 1.6417, Val Loss: 1.7500\n",
      "Epoch 118/200, Train Loss: 1.6584, Val Loss: 1.7871\n",
      "Epoch 119/200, Train Loss: 1.5914, Val Loss: 1.6690\n",
      "Epoch 120/200, Train Loss: 1.5485, Val Loss: 1.6362\n",
      "  New best validation loss: 1.6362\n",
      "Epoch 121/200, Train Loss: 1.5318, Val Loss: 1.6468\n",
      "Epoch 122/200, Train Loss: 1.5292, Val Loss: 1.6118\n",
      "  New best validation loss: 1.6118\n",
      "Epoch 123/200, Train Loss: 1.5215, Val Loss: 1.6248\n",
      "Epoch 124/200, Train Loss: 1.5187, Val Loss: 1.6195\n",
      "Epoch 125/200, Train Loss: 1.5322, Val Loss: 1.6212\n",
      "Epoch 126/200, Train Loss: 1.5541, Val Loss: 1.6775\n",
      "Epoch 127/200, Train Loss: 1.5179, Val Loss: 1.6298\n",
      "Epoch 128/200, Train Loss: 1.5256, Val Loss: 1.6317\n",
      "Epoch 129/200, Train Loss: 1.5116, Val Loss: 1.6172\n",
      "Epoch 130/200, Train Loss: 1.5173, Val Loss: 1.6290\n",
      "Epoch 131/200, Train Loss: 1.5110, Val Loss: 1.6113\n",
      "  New best validation loss: 1.6113\n",
      "Epoch 132/200, Train Loss: 1.5700, Val Loss: 1.6472\n",
      "Epoch 133/200, Train Loss: 1.5295, Val Loss: 1.6167\n",
      "Epoch 134/200, Train Loss: 1.4840, Val Loss: 1.5823\n",
      "  New best validation loss: 1.5823\n",
      "Epoch 135/200, Train Loss: 1.5027, Val Loss: 1.5859\n",
      "Epoch 136/200, Train Loss: 1.5336, Val Loss: 1.6115\n",
      "Epoch 137/200, Train Loss: 1.5498, Val Loss: 1.6026\n",
      "Epoch 138/200, Train Loss: 1.5094, Val Loss: 1.5946\n",
      "Epoch 139/200, Train Loss: 1.5083, Val Loss: 1.6454\n",
      "Epoch 140/200, Train Loss: 1.5047, Val Loss: 1.6295\n",
      "Epoch 141/200, Train Loss: 1.4906, Val Loss: 1.7083\n",
      "Epoch 142/200, Train Loss: 1.5099, Val Loss: 1.6217\n",
      "Epoch 143/200, Train Loss: 1.5117, Val Loss: 1.6761\n",
      "Epoch 144/200, Train Loss: 1.5552, Val Loss: 1.6496\n",
      "Epoch 145/200, Train Loss: 1.5050, Val Loss: 1.6211\n",
      "Epoch 146/200, Train Loss: 1.5326, Val Loss: 1.6706\n",
      "Epoch 147/200, Train Loss: 1.4989, Val Loss: 1.6424\n",
      "Epoch 148/200, Train Loss: 1.5114, Val Loss: 1.5939\n",
      "Epoch 149/200, Train Loss: 1.4862, Val Loss: 1.6721\n",
      "Epoch 150/200, Train Loss: 1.5340, Val Loss: 1.7608\n",
      "Epoch 151/200, Train Loss: 1.5410, Val Loss: 1.5914\n",
      "Epoch 152/200, Train Loss: 1.4550, Val Loss: 1.5900\n",
      "Epoch 153/200, Train Loss: 1.4581, Val Loss: 1.5664\n",
      "  New best validation loss: 1.5664\n",
      "Epoch 154/200, Train Loss: 1.4517, Val Loss: 1.6547\n",
      "Epoch 155/200, Train Loss: 1.4838, Val Loss: 1.6381\n",
      "Epoch 156/200, Train Loss: 1.4894, Val Loss: 1.6587\n",
      "Epoch 157/200, Train Loss: 1.5620, Val Loss: 1.6130\n",
      "Epoch 158/200, Train Loss: 1.5082, Val Loss: 1.6194\n",
      "Epoch 159/200, Train Loss: 1.4608, Val Loss: 1.5464\n",
      "  New best validation loss: 1.5464\n",
      "Epoch 160/200, Train Loss: 1.4264, Val Loss: 1.5372\n",
      "  New best validation loss: 1.5372\n",
      "Epoch 161/200, Train Loss: 1.4383, Val Loss: 1.6428\n",
      "Epoch 162/200, Train Loss: 1.4527, Val Loss: 1.6493\n",
      "Epoch 163/200, Train Loss: 1.4663, Val Loss: 1.5555\n",
      "Epoch 164/200, Train Loss: 1.4205, Val Loss: 1.5430\n",
      "Epoch 165/200, Train Loss: 1.4134, Val Loss: 1.5557\n",
      "Epoch 166/200, Train Loss: 1.4169, Val Loss: 1.5257\n",
      "  New best validation loss: 1.5257\n",
      "Epoch 167/200, Train Loss: 1.4392, Val Loss: 1.5695\n",
      "Epoch 168/200, Train Loss: 1.4229, Val Loss: 1.5448\n",
      "Epoch 169/200, Train Loss: 1.4173, Val Loss: 1.5592\n",
      "Epoch 170/200, Train Loss: 1.4056, Val Loss: 1.5681\n",
      "Epoch 171/200, Train Loss: 1.4216, Val Loss: 1.5074\n",
      "  New best validation loss: 1.5074\n",
      "Epoch 172/200, Train Loss: 1.3987, Val Loss: 1.5923\n",
      "Epoch 173/200, Train Loss: 1.4045, Val Loss: 1.5526\n",
      "Epoch 174/200, Train Loss: 1.4091, Val Loss: 1.5625\n",
      "Epoch 175/200, Train Loss: 1.4306, Val Loss: 1.5841\n",
      "Epoch 176/200, Train Loss: 1.4046, Val Loss: 1.5148\n",
      "Epoch 177/200, Train Loss: 1.3972, Val Loss: 1.6275\n",
      "Epoch 178/200, Train Loss: 1.4814, Val Loss: 1.5986\n",
      "Epoch 179/200, Train Loss: 1.4357, Val Loss: 1.5267\n",
      "Epoch 180/200, Train Loss: 1.3911, Val Loss: 1.5557\n",
      "Epoch 181/200, Train Loss: 1.3697, Val Loss: 1.4909\n",
      "  New best validation loss: 1.4909\n",
      "Epoch 182/200, Train Loss: 1.4126, Val Loss: 1.6106\n",
      "Epoch 183/200, Train Loss: 1.4116, Val Loss: 1.5858\n",
      "Epoch 184/200, Train Loss: 1.4686, Val Loss: 1.7866\n",
      "Epoch 185/200, Train Loss: 1.4611, Val Loss: 1.5482\n",
      "Epoch 186/200, Train Loss: 1.3970, Val Loss: 1.5267\n",
      "Epoch 187/200, Train Loss: 1.3643, Val Loss: 1.5008\n",
      "Epoch 188/200, Train Loss: 1.3646, Val Loss: 1.4983\n",
      "Epoch 189/200, Train Loss: 1.3734, Val Loss: 1.4651\n",
      "  New best validation loss: 1.4651\n",
      "Epoch 190/200, Train Loss: 1.3980, Val Loss: 1.5343\n",
      "Epoch 191/200, Train Loss: 1.3416, Val Loss: 1.4756\n",
      "Epoch 192/200, Train Loss: 1.3558, Val Loss: 1.4957\n",
      "Epoch 193/200, Train Loss: 1.3433, Val Loss: 1.4884\n",
      "Epoch 194/200, Train Loss: 1.3369, Val Loss: 1.5230\n",
      "Epoch 195/200, Train Loss: 1.3775, Val Loss: 1.4823\n",
      "Epoch 196/200, Train Loss: 1.3895, Val Loss: 1.5845\n",
      "Epoch 197/200, Train Loss: 1.3833, Val Loss: 1.4738\n",
      "Epoch 198/200, Train Loss: 1.3417, Val Loss: 1.5238\n",
      "Epoch 199/200, Train Loss: 1.3473, Val Loss: 1.4965\n",
      "Epoch 200/200, Train Loss: 1.3378, Val Loss: 1.5045\n",
      "\n",
      "Loaded best model (Val Loss: 1.4651) for final hidden state extraction.\n",
      "Saved best model for NMRNN_NoSpatial_ModReadout to results/20250508_043138/NMRNN_NoSpatial_ModReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_NoSpatial_ModReadout ---\n",
      "  Analyzing decodability for NMRNN_NoSpatial_ModReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_NoSpatial_ModReadout - Test MSE: 0.0296, Test R2: 0.9856 (best alpha: 0.2976)\n",
      "Decodability (R2 score) for NMRNN_NoSpatial_ModReadout: 0.9856\n",
      "\n",
      "--- Training NMRNN_Spatial_FixedReadout ---\n",
      "Number of parameters: 66449\n",
      "Epoch 1/200, Train Loss: 4.9547, Val Loss: 5.1017\n",
      "  New best validation loss: 5.1017\n",
      "Epoch 2/200, Train Loss: 4.7681, Val Loss: 4.9848\n",
      "  New best validation loss: 4.9848\n",
      "Epoch 3/200, Train Loss: 4.6498, Val Loss: 4.9744\n",
      "  New best validation loss: 4.9744\n",
      "Epoch 4/200, Train Loss: 4.5583, Val Loss: 4.7323\n",
      "  New best validation loss: 4.7323\n",
      "Epoch 5/200, Train Loss: 4.4649, Val Loss: 4.6348\n",
      "  New best validation loss: 4.6348\n",
      "Epoch 6/200, Train Loss: 4.3317, Val Loss: 4.5233\n",
      "  New best validation loss: 4.5233\n",
      "Epoch 7/200, Train Loss: 4.1748, Val Loss: 4.4553\n",
      "  New best validation loss: 4.4553\n",
      "Epoch 8/200, Train Loss: 4.0285, Val Loss: 4.3778\n",
      "  New best validation loss: 4.3778\n",
      "Epoch 9/200, Train Loss: 3.8812, Val Loss: 4.1990\n",
      "  New best validation loss: 4.1990\n",
      "Epoch 10/200, Train Loss: 3.8068, Val Loss: 4.1288\n",
      "  New best validation loss: 4.1288\n",
      "Epoch 11/200, Train Loss: 3.7082, Val Loss: 4.0106\n",
      "  New best validation loss: 4.0106\n",
      "Epoch 12/200, Train Loss: 3.5043, Val Loss: 3.7815\n",
      "  New best validation loss: 3.7815\n",
      "Epoch 13/200, Train Loss: 3.3734, Val Loss: 3.6258\n",
      "  New best validation loss: 3.6258\n",
      "Epoch 14/200, Train Loss: 3.2796, Val Loss: 3.4758\n",
      "  New best validation loss: 3.4758\n",
      "Epoch 15/200, Train Loss: 3.1718, Val Loss: 3.4103\n",
      "  New best validation loss: 3.4103\n",
      "Epoch 16/200, Train Loss: 3.0445, Val Loss: 3.4812\n",
      "Epoch 17/200, Train Loss: 3.0812, Val Loss: 3.2006\n",
      "  New best validation loss: 3.2006\n",
      "Epoch 18/200, Train Loss: 2.8761, Val Loss: 3.1107\n",
      "  New best validation loss: 3.1107\n",
      "Epoch 19/200, Train Loss: 2.7468, Val Loss: 2.9030\n",
      "  New best validation loss: 2.9030\n",
      "Epoch 20/200, Train Loss: 2.6312, Val Loss: 3.0162\n",
      "Epoch 21/200, Train Loss: 2.6113, Val Loss: 2.7891\n",
      "  New best validation loss: 2.7891\n",
      "Epoch 22/200, Train Loss: 2.4734, Val Loss: 2.6305\n",
      "  New best validation loss: 2.6305\n",
      "Epoch 23/200, Train Loss: 2.4615, Val Loss: 2.6369\n",
      "Epoch 24/200, Train Loss: 2.3612, Val Loss: 2.5471\n",
      "  New best validation loss: 2.5471\n",
      "Epoch 25/200, Train Loss: 2.3134, Val Loss: 2.5064\n",
      "  New best validation loss: 2.5064\n",
      "Epoch 26/200, Train Loss: 2.2518, Val Loss: 2.4571\n",
      "  New best validation loss: 2.4571\n",
      "Epoch 27/200, Train Loss: 2.1869, Val Loss: 2.3842\n",
      "  New best validation loss: 2.3842\n",
      "Epoch 28/200, Train Loss: 2.1993, Val Loss: 2.3591\n",
      "  New best validation loss: 2.3591\n",
      "Epoch 29/200, Train Loss: 2.1872, Val Loss: 2.7976\n",
      "Epoch 30/200, Train Loss: 2.2386, Val Loss: 2.4040\n",
      "Epoch 31/200, Train Loss: 2.1281, Val Loss: 2.2658\n",
      "  New best validation loss: 2.2658\n",
      "Epoch 32/200, Train Loss: 2.0727, Val Loss: 2.2657\n",
      "  New best validation loss: 2.2657\n",
      "Epoch 33/200, Train Loss: 2.0772, Val Loss: 2.2379\n",
      "  New best validation loss: 2.2379\n",
      "Epoch 34/200, Train Loss: 1.9873, Val Loss: 2.1532\n",
      "  New best validation loss: 2.1532\n",
      "Epoch 35/200, Train Loss: 1.9707, Val Loss: 2.1692\n",
      "Epoch 36/200, Train Loss: 1.9364, Val Loss: 2.1106\n",
      "  New best validation loss: 2.1106\n",
      "Epoch 37/200, Train Loss: 1.9085, Val Loss: 2.0972\n",
      "  New best validation loss: 2.0972\n",
      "Epoch 38/200, Train Loss: 1.8899, Val Loss: 2.0417\n",
      "  New best validation loss: 2.0417\n",
      "Epoch 39/200, Train Loss: 1.8862, Val Loss: 2.1282\n",
      "Epoch 40/200, Train Loss: 1.8953, Val Loss: 2.0541\n",
      "Epoch 41/200, Train Loss: 1.8370, Val Loss: 2.0768\n",
      "Epoch 42/200, Train Loss: 1.8778, Val Loss: 2.0211\n",
      "  New best validation loss: 2.0211\n",
      "Epoch 43/200, Train Loss: 1.7953, Val Loss: 2.0245\n",
      "Epoch 44/200, Train Loss: 1.7861, Val Loss: 2.0078\n",
      "  New best validation loss: 2.0078\n",
      "Epoch 45/200, Train Loss: 1.7969, Val Loss: 1.9838\n",
      "  New best validation loss: 1.9838\n",
      "Epoch 46/200, Train Loss: 1.7623, Val Loss: 1.9301\n",
      "  New best validation loss: 1.9301\n",
      "Epoch 47/200, Train Loss: 1.7431, Val Loss: 1.9823\n",
      "Epoch 48/200, Train Loss: 1.7572, Val Loss: 1.9241\n",
      "  New best validation loss: 1.9241\n",
      "Epoch 49/200, Train Loss: 1.7010, Val Loss: 1.9116\n",
      "  New best validation loss: 1.9116\n",
      "Epoch 50/200, Train Loss: 1.6867, Val Loss: 1.9051\n",
      "  New best validation loss: 1.9051\n",
      "Epoch 51/200, Train Loss: 1.6727, Val Loss: 1.8707\n",
      "  New best validation loss: 1.8707\n",
      "Epoch 52/200, Train Loss: 1.6676, Val Loss: 1.8436\n",
      "  New best validation loss: 1.8436\n",
      "Epoch 53/200, Train Loss: 1.6596, Val Loss: 1.8604\n",
      "Epoch 54/200, Train Loss: 1.6476, Val Loss: 1.8289\n",
      "  New best validation loss: 1.8289\n",
      "Epoch 55/200, Train Loss: 1.6214, Val Loss: 1.8389\n",
      "Epoch 56/200, Train Loss: 1.6084, Val Loss: 1.8144\n",
      "  New best validation loss: 1.8144\n",
      "Epoch 57/200, Train Loss: 1.6380, Val Loss: 1.8895\n",
      "Epoch 58/200, Train Loss: 1.6326, Val Loss: 1.8017\n",
      "  New best validation loss: 1.8017\n",
      "Epoch 59/200, Train Loss: 1.6040, Val Loss: 1.8563\n",
      "Epoch 60/200, Train Loss: 1.6163, Val Loss: 1.8258\n",
      "Epoch 61/200, Train Loss: 1.5719, Val Loss: 1.7906\n",
      "  New best validation loss: 1.7906\n",
      "Epoch 62/200, Train Loss: 1.5519, Val Loss: 1.7810\n",
      "  New best validation loss: 1.7810\n",
      "Epoch 63/200, Train Loss: 1.5733, Val Loss: 1.7398\n",
      "  New best validation loss: 1.7398\n",
      "Epoch 64/200, Train Loss: 1.6127, Val Loss: 1.8349\n",
      "Epoch 65/200, Train Loss: 1.5932, Val Loss: 1.7830\n",
      "Epoch 66/200, Train Loss: 1.5450, Val Loss: 1.7883\n",
      "Epoch 67/200, Train Loss: 1.5358, Val Loss: 1.7270\n",
      "  New best validation loss: 1.7270\n",
      "Epoch 68/200, Train Loss: 1.5285, Val Loss: 1.7183\n",
      "  New best validation loss: 1.7183\n",
      "Epoch 69/200, Train Loss: 1.5110, Val Loss: 1.7459\n",
      "Epoch 70/200, Train Loss: 1.5175, Val Loss: 1.7007\n",
      "  New best validation loss: 1.7007\n",
      "Epoch 71/200, Train Loss: 1.5082, Val Loss: 1.6985\n",
      "  New best validation loss: 1.6985\n",
      "Epoch 72/200, Train Loss: 1.4775, Val Loss: 1.6698\n",
      "  New best validation loss: 1.6698\n",
      "Epoch 73/200, Train Loss: 1.4584, Val Loss: 1.6941\n",
      "Epoch 74/200, Train Loss: 1.4493, Val Loss: 1.6917\n",
      "Epoch 75/200, Train Loss: 1.4667, Val Loss: 1.6671\n",
      "  New best validation loss: 1.6671\n",
      "Epoch 76/200, Train Loss: 1.4555, Val Loss: 1.6394\n",
      "  New best validation loss: 1.6394\n",
      "Epoch 77/200, Train Loss: 1.4603, Val Loss: 1.6795\n",
      "Epoch 78/200, Train Loss: 1.4602, Val Loss: 1.7105\n",
      "Epoch 79/200, Train Loss: 1.4455, Val Loss: 1.6425\n",
      "Epoch 80/200, Train Loss: 1.4257, Val Loss: 1.6376\n",
      "  New best validation loss: 1.6376\n",
      "Epoch 81/200, Train Loss: 1.4153, Val Loss: 1.6739\n",
      "Epoch 82/200, Train Loss: 1.4110, Val Loss: 1.6119\n",
      "  New best validation loss: 1.6119\n",
      "Epoch 83/200, Train Loss: 1.4104, Val Loss: 1.6493\n",
      "Epoch 84/200, Train Loss: 1.3921, Val Loss: 1.6319\n",
      "Epoch 85/200, Train Loss: 1.3821, Val Loss: 1.5986\n",
      "  New best validation loss: 1.5986\n",
      "Epoch 86/200, Train Loss: 1.3882, Val Loss: 1.5836\n",
      "  New best validation loss: 1.5836\n",
      "Epoch 87/200, Train Loss: 1.3832, Val Loss: 1.6019\n",
      "Epoch 88/200, Train Loss: 1.3583, Val Loss: 1.5756\n",
      "  New best validation loss: 1.5756\n",
      "Epoch 89/200, Train Loss: 1.3498, Val Loss: 1.6143\n",
      "Epoch 90/200, Train Loss: 1.3510, Val Loss: 1.5714\n",
      "  New best validation loss: 1.5714\n",
      "Epoch 91/200, Train Loss: 1.3379, Val Loss: 1.6052\n",
      "Epoch 92/200, Train Loss: 1.3587, Val Loss: 1.5409\n",
      "  New best validation loss: 1.5409\n",
      "Epoch 93/200, Train Loss: 1.3193, Val Loss: 1.5509\n",
      "Epoch 94/200, Train Loss: 1.3063, Val Loss: 1.5285\n",
      "  New best validation loss: 1.5285\n",
      "Epoch 95/200, Train Loss: 1.2907, Val Loss: 1.5293\n",
      "Epoch 96/200, Train Loss: 1.2973, Val Loss: 1.4959\n",
      "  New best validation loss: 1.4959\n",
      "Epoch 97/200, Train Loss: 1.2902, Val Loss: 1.5698\n",
      "Epoch 98/200, Train Loss: 1.3235, Val Loss: 1.5189\n",
      "Epoch 99/200, Train Loss: 1.2811, Val Loss: 1.5174\n",
      "Epoch 100/200, Train Loss: 1.2760, Val Loss: 1.5084\n",
      "Epoch 101/200, Train Loss: 1.2748, Val Loss: 1.5612\n",
      "Epoch 102/200, Train Loss: 1.2678, Val Loss: 1.5237\n",
      "Epoch 103/200, Train Loss: 1.2743, Val Loss: 1.5565\n",
      "Epoch 104/200, Train Loss: 1.2572, Val Loss: 1.4707\n",
      "  New best validation loss: 1.4707\n",
      "Epoch 105/200, Train Loss: 1.2534, Val Loss: 1.4924\n",
      "Epoch 106/200, Train Loss: 1.2662, Val Loss: 1.4849\n",
      "Epoch 107/200, Train Loss: 1.2350, Val Loss: 1.5074\n",
      "Epoch 108/200, Train Loss: 1.2354, Val Loss: 1.4743\n",
      "Epoch 109/200, Train Loss: 1.2358, Val Loss: 1.5265\n",
      "Epoch 110/200, Train Loss: 1.2502, Val Loss: 1.5007\n",
      "Epoch 111/200, Train Loss: 1.2217, Val Loss: 1.4953\n",
      "Epoch 112/200, Train Loss: 1.2228, Val Loss: 1.4482\n",
      "  New best validation loss: 1.4482\n",
      "Epoch 113/200, Train Loss: 1.2122, Val Loss: 1.4639\n",
      "Epoch 114/200, Train Loss: 1.2003, Val Loss: 1.4767\n",
      "Epoch 115/200, Train Loss: 1.2101, Val Loss: 1.4908\n",
      "Epoch 116/200, Train Loss: 1.1981, Val Loss: 1.4490\n",
      "Epoch 117/200, Train Loss: 1.2190, Val Loss: 1.5164\n",
      "Epoch 118/200, Train Loss: 1.1958, Val Loss: 1.4287\n",
      "  New best validation loss: 1.4287\n",
      "Epoch 119/200, Train Loss: 1.1815, Val Loss: 1.4942\n",
      "Epoch 120/200, Train Loss: 1.1808, Val Loss: 1.4089\n",
      "  New best validation loss: 1.4089\n",
      "Epoch 121/200, Train Loss: 1.1710, Val Loss: 1.4353\n",
      "Epoch 122/200, Train Loss: 1.1531, Val Loss: 1.4067\n",
      "  New best validation loss: 1.4067\n",
      "Epoch 123/200, Train Loss: 1.1604, Val Loss: 1.4328\n",
      "Epoch 124/200, Train Loss: 1.1793, Val Loss: 1.4465\n",
      "Epoch 125/200, Train Loss: 1.1876, Val Loss: 1.3877\n",
      "  New best validation loss: 1.3877\n",
      "Epoch 126/200, Train Loss: 1.1595, Val Loss: 1.4352\n",
      "Epoch 127/200, Train Loss: 1.1547, Val Loss: 1.4157\n",
      "Epoch 128/200, Train Loss: 1.1584, Val Loss: 1.3886\n",
      "Epoch 129/200, Train Loss: 1.1279, Val Loss: 1.4516\n",
      "Epoch 130/200, Train Loss: 1.1445, Val Loss: 1.3863\n",
      "  New best validation loss: 1.3863\n",
      "Epoch 131/200, Train Loss: 1.1147, Val Loss: 1.3545\n",
      "  New best validation loss: 1.3545\n",
      "Epoch 132/200, Train Loss: 1.1247, Val Loss: 1.3930\n",
      "Epoch 133/200, Train Loss: 1.1073, Val Loss: 1.3662\n",
      "Epoch 134/200, Train Loss: 1.1295, Val Loss: 1.3644\n",
      "Epoch 135/200, Train Loss: 1.1403, Val Loss: 1.3774\n",
      "Epoch 136/200, Train Loss: 1.0947, Val Loss: 1.4074\n",
      "Epoch 137/200, Train Loss: 1.1048, Val Loss: 1.3669\n",
      "Epoch 138/200, Train Loss: 1.0785, Val Loss: 1.3443\n",
      "  New best validation loss: 1.3443\n",
      "Epoch 139/200, Train Loss: 1.0688, Val Loss: 1.3447\n",
      "Epoch 140/200, Train Loss: 1.0673, Val Loss: 1.3214\n",
      "  New best validation loss: 1.3214\n",
      "Epoch 141/200, Train Loss: 1.0767, Val Loss: 1.3490\n",
      "Epoch 142/200, Train Loss: 1.0585, Val Loss: 1.3640\n",
      "Epoch 143/200, Train Loss: 1.0560, Val Loss: 1.3333\n",
      "Epoch 144/200, Train Loss: 1.0483, Val Loss: 1.3160\n",
      "  New best validation loss: 1.3160\n",
      "Epoch 145/200, Train Loss: 1.0535, Val Loss: 1.2967\n",
      "  New best validation loss: 1.2967\n",
      "Epoch 146/200, Train Loss: 1.0385, Val Loss: 1.3095\n",
      "Epoch 147/200, Train Loss: 1.0483, Val Loss: 1.2968\n",
      "Epoch 148/200, Train Loss: 1.0614, Val Loss: 1.3130\n",
      "Epoch 149/200, Train Loss: 1.0427, Val Loss: 1.3221\n",
      "Epoch 150/200, Train Loss: 1.0273, Val Loss: 1.2867\n",
      "  New best validation loss: 1.2867\n",
      "Epoch 151/200, Train Loss: 1.0378, Val Loss: 1.3254\n",
      "Epoch 152/200, Train Loss: 1.0359, Val Loss: 1.2897\n",
      "Epoch 153/200, Train Loss: 1.0175, Val Loss: 1.2768\n",
      "  New best validation loss: 1.2768\n",
      "Epoch 154/200, Train Loss: 1.0009, Val Loss: 1.3101\n",
      "Epoch 155/200, Train Loss: 1.0116, Val Loss: 1.2953\n",
      "Epoch 156/200, Train Loss: 1.0015, Val Loss: 1.2799\n",
      "Epoch 157/200, Train Loss: 0.9997, Val Loss: 1.2755\n",
      "  New best validation loss: 1.2755\n",
      "Epoch 158/200, Train Loss: 0.9806, Val Loss: 1.2503\n",
      "  New best validation loss: 1.2503\n",
      "Epoch 159/200, Train Loss: 0.9723, Val Loss: 1.2556\n",
      "Epoch 160/200, Train Loss: 0.9663, Val Loss: 1.2070\n",
      "  New best validation loss: 1.2070\n",
      "Epoch 161/200, Train Loss: 0.9650, Val Loss: 1.2496\n",
      "Epoch 162/200, Train Loss: 0.9585, Val Loss: 1.2215\n",
      "Epoch 163/200, Train Loss: 0.9538, Val Loss: 1.2067\n",
      "  New best validation loss: 1.2067\n",
      "Epoch 164/200, Train Loss: 0.9503, Val Loss: 1.2198\n",
      "Epoch 165/200, Train Loss: 0.9387, Val Loss: 1.1855\n",
      "  New best validation loss: 1.1855\n",
      "Epoch 166/200, Train Loss: 0.9396, Val Loss: 1.1851\n",
      "  New best validation loss: 1.1851\n",
      "Epoch 167/200, Train Loss: 0.9259, Val Loss: 1.2000\n",
      "Epoch 168/200, Train Loss: 0.9326, Val Loss: 1.1765\n",
      "  New best validation loss: 1.1765\n",
      "Epoch 169/200, Train Loss: 0.9187, Val Loss: 1.1781\n",
      "Epoch 170/200, Train Loss: 0.9146, Val Loss: 1.1736\n",
      "  New best validation loss: 1.1736\n",
      "Epoch 171/200, Train Loss: 0.9094, Val Loss: 1.1669\n",
      "  New best validation loss: 1.1669\n",
      "Epoch 172/200, Train Loss: 0.9109, Val Loss: 1.2001\n",
      "Epoch 173/200, Train Loss: 0.9127, Val Loss: 1.1669\n",
      "Epoch 174/200, Train Loss: 0.9108, Val Loss: 1.1947\n",
      "Epoch 175/200, Train Loss: 0.9035, Val Loss: 1.1576\n",
      "  New best validation loss: 1.1576\n",
      "Epoch 176/200, Train Loss: 0.8930, Val Loss: 1.1473\n",
      "  New best validation loss: 1.1473\n",
      "Epoch 177/200, Train Loss: 0.8819, Val Loss: 1.1349\n",
      "  New best validation loss: 1.1349\n",
      "Epoch 178/200, Train Loss: 0.8823, Val Loss: 1.1435\n",
      "Epoch 179/200, Train Loss: 0.8758, Val Loss: 1.1386\n",
      "Epoch 180/200, Train Loss: 0.8708, Val Loss: 1.1214\n",
      "  New best validation loss: 1.1214\n",
      "Epoch 181/200, Train Loss: 0.8622, Val Loss: 1.0981\n",
      "  New best validation loss: 1.0981\n",
      "Epoch 182/200, Train Loss: 0.8562, Val Loss: 1.0877\n",
      "  New best validation loss: 1.0877\n",
      "Epoch 183/200, Train Loss: 0.8642, Val Loss: 1.0906\n",
      "Epoch 184/200, Train Loss: 0.8614, Val Loss: 1.1895\n",
      "Epoch 185/200, Train Loss: 0.8652, Val Loss: 1.0992\n",
      "Epoch 186/200, Train Loss: 0.8489, Val Loss: 1.0880\n",
      "Epoch 187/200, Train Loss: 0.8371, Val Loss: 1.0909\n",
      "Epoch 188/200, Train Loss: 0.8415, Val Loss: 1.0841\n",
      "  New best validation loss: 1.0841\n",
      "Epoch 189/200, Train Loss: 0.8367, Val Loss: 1.0785\n",
      "  New best validation loss: 1.0785\n",
      "Epoch 190/200, Train Loss: 0.8319, Val Loss: 1.1134\n",
      "Epoch 191/200, Train Loss: 0.8470, Val Loss: 1.0693\n",
      "  New best validation loss: 1.0693\n",
      "Epoch 192/200, Train Loss: 0.8155, Val Loss: 1.0880\n",
      "Epoch 193/200, Train Loss: 0.8215, Val Loss: 1.1075\n",
      "Epoch 194/200, Train Loss: 0.8121, Val Loss: 1.0871\n",
      "Epoch 195/200, Train Loss: 0.8192, Val Loss: 1.0646\n",
      "  New best validation loss: 1.0646\n",
      "Epoch 196/200, Train Loss: 0.8162, Val Loss: 1.1040\n",
      "Epoch 197/200, Train Loss: 0.8327, Val Loss: 1.0564\n",
      "  New best validation loss: 1.0564\n",
      "Epoch 198/200, Train Loss: 0.8035, Val Loss: 1.0366\n",
      "  New best validation loss: 1.0366\n",
      "Epoch 199/200, Train Loss: 0.7933, Val Loss: 1.0340\n",
      "  New best validation loss: 1.0340\n",
      "Epoch 200/200, Train Loss: 0.7838, Val Loss: 1.0241\n",
      "  New best validation loss: 1.0241\n",
      "\n",
      "Loaded best model (Val Loss: 1.0241) for final hidden state extraction.\n",
      "Saved best model for NMRNN_Spatial_FixedReadout to results/20250508_043138/NMRNN_Spatial_FixedReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_Spatial_FixedReadout ---\n",
      "  Analyzing decodability for NMRNN_Spatial_FixedReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_Spatial_FixedReadout - Test MSE: 0.1035, Test R2: 0.9497 (best alpha: 2.6367)\n",
      "Decodability (R2 score) for NMRNN_Spatial_FixedReadout: 0.9497\n",
      "Learning curves saved to results/20250508_043138/learning_curves.png\n",
      "\n",
      "Learning curves plotted to results/20250508_043138/learning_curves.png\n",
      "\n",
      "--- Decodability Results (R2 Score) ---\n",
      "ComplexOscillatorNet: 0.9709\n",
      "RNN_GRU: 0.9988\n",
      "Transformer: 0.9973\n",
      "HIPPORNN_LegT: 0.3160\n",
      "NMRNN_Spatial_ModReadout: 0.9732\n",
      "NMRNN_NoSpatial_ModReadout: 0.9856\n",
      "NMRNN_Spatial_FixedReadout: 0.9497\n",
      "\n",
      "Experiment finished. All results in results/20250508_043138\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Project specific imports\n",
    "from dataset import CompositionalDataset, create_dataloaders\n",
    "from model import NonlinearOscillatorNet, RNNModel, TransformerModel, HippoRNNModel, NMRNN_Spatial_ModulatedReadout, NMRNN_NoSpatial_ModulatedReadout, NMRNN_Spatial_FixedReadout\n",
    "from training import train_model_comparative\n",
    "from analysis import plot_learning_curves, perform_decodability_analysis\n",
    "from utils import set_seed, get_device, count_parameters\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"seed\": 0,\n",
    "    \"num_task_coefficients\": 5, \n",
    "    \"seq_length\": 200,          \n",
    "    \"train_samples\": 32 * 10,  # Reduced for quicker testing, increase for real runs\n",
    "    \"test_samples\": 32 * 5,   # Reduced for quicker testing\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200, # Reduced for quick test, increase for real runs (e.g., 50-200)\n",
    "    \"lr\": 1e-3,\n",
    "    \n",
    "    # Model-specific hidden sizes / main dimension\n",
    "    \"hidden_size_oscillator\": 64, \n",
    "    \"hidden_size_rnn\": 128,       # For standard GRU/LSTM\n",
    "    \"d_model_transformer\": 64,   \n",
    "    \"hidden_size_hippo\": 128,     # N for HIPPO\n",
    "    \"hidden_size_nm_rnn\": 128,    # n_rnn for nmRNN variants\n",
    "\n",
    "    # Transformer specific\n",
    "    \"nhead_transformer\": 2,\n",
    "    \"num_layers_transformer\": 1,\n",
    "    \n",
    "    # HIPPORNN specific\n",
    "    \"hippo_method\": 'legt', # 'legs' or 'legt'\n",
    "    \"hippo_theta\": 1.0,     # Required for 'legt'\n",
    "    \"hippo_dt\": 1.0 / 200,  # Discretization step for HIPPO (e.g., 1.0 / seq_length)\n",
    "    \"hippo_inv_eps\": 1e-6, # Epsilon for LegS matrix inversion regularization\n",
    "    \"hippo_clip_val\": 50.0, # Clipping for HIPPO state c_t\n",
    "\n",
    "    # nmRNN specific (shared for variants where applicable)\n",
    "    \"nm_N_NM\": 4,               # Number of neuromodulators\n",
    "    \"nm_activation\": 'tanh',    # 'relu', 'tanh' (original code had 'relu-tanh', simplified here)\n",
    "    \"nm_decay\": 0.05, # dt_sec / tau_rnn, e.g., (20ms/step) / (100ms tau) -> exp(-0.2)\n",
    "                                     # Original: math.exp(-20/100) - assuming 20ms step, 100ms tau\n",
    "    \"nm_bias\": True,\n",
    "    \"nm_keepW0_spatial\": False, # For the version with spatial connections\n",
    "    \"nm_keepW0_no_spatial\": False,\n",
    "    \"nm_grad_clip\": 1.0,\n",
    "    \"nm_spatial_ell\": 0.1,      # For SpatialWeight\n",
    "    \"nm_spatial_scale\": 1.0,    # For SpatialWeight\n",
    "\n",
    "    # General task params\n",
    "    \"output_dim\": 1, \n",
    "    \"input_dim\": 1,  \n",
    "    \"noise_level_data\": 0.01,\n",
    "    \"run_timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"results_dir\": \"results\"\n",
    "}\n",
    "\n",
    "def run_experiment():\n",
    "    \"\"\"\n",
    "    Runs the full comparative analysis experiment.\n",
    "    \"\"\"\n",
    "    set_seed(CONFIG[\"seed\"])\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    os.makedirs(CONFIG[\"results_dir\"], exist_ok=True)\n",
    "    run_results_dir = os.path.join(CONFIG[\"results_dir\"], CONFIG[\"run_timestamp\"])\n",
    "    os.makedirs(run_results_dir, exist_ok=True)\n",
    "    print(f\"Results will be saved in: {run_results_dir}\")\n",
    "\n",
    "    # --- 1. Dataset ---\n",
    "    print(\"Loading dataset...\")\n",
    "    train_loader, val_loader, test_loader, (input_basis, output_basis) = create_dataloaders(\n",
    "        num_train_samples=CONFIG[\"train_samples\"],\n",
    "        num_val_samples=CONFIG[\"test_samples\"], \n",
    "        num_test_samples=CONFIG[\"test_samples\"],\n",
    "        num_basis=CONFIG[\"num_task_coefficients\"],\n",
    "        seq_length=CONFIG[\"seq_length\"],\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        noise=CONFIG[\"noise_level_data\"]\n",
    "    )\n",
    "    print(\"Dataset loaded.\")\n",
    "\n",
    "    # --- 2. Models ---\n",
    "    models_to_test = {\n",
    "        \"ComplexOscillatorNet\": NonlinearOscillatorNet(\n",
    "            N_oscillators=CONFIG[\"hidden_size_oscillator\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            seq_length=CONFIG[\"seq_length\"], \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"RNN_GRU\": RNNModel(\n",
    "            hidden_size=CONFIG[\"hidden_size_rnn\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            num_layers=1, \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"Transformer\": TransformerModel(\n",
    "            d_model=CONFIG[\"d_model_transformer\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            num_heads=CONFIG[\"nhead_transformer\"],\n",
    "            num_layers=CONFIG[\"num_layers_transformer\"],\n",
    "            seq_length=CONFIG[\"seq_length\"], \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"HIPPORNN_LegT\": HippoRNNModel( # Using LegT by default as per config\n",
    "            hidden_size=CONFIG[\"hidden_size_hippo\"],\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            method=CONFIG[\"hippo_method\"], \n",
    "            theta=CONFIG[\"hippo_theta\"],\n",
    "            dt=CONFIG[\"hippo_dt\"],\n",
    "            inv_eps=CONFIG[\"hippo_inv_eps\"],\n",
    "            clip_val=CONFIG[\"hippo_clip_val\"],\n",
    "            device=device, # Pass device\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_Spatial_ModReadout\": NMRNN_Spatial_ModulatedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"],\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_spatial\"],\n",
    "            spatial_ell=CONFIG[\"nm_spatial_ell\"],\n",
    "            spatial_scale=CONFIG[\"nm_spatial_scale\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_NoSpatial_ModReadout\": NMRNN_NoSpatial_ModulatedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"],\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_no_spatial\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_Spatial_FixedReadout\": NMRNN_Spatial_FixedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"], # N_nm still needed for the core recurrence, just not readout\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_spatial\"],\n",
    "            spatial_ell=CONFIG[\"nm_spatial_ell\"],\n",
    "            spatial_scale=CONFIG[\"nm_spatial_scale\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    all_val_losses = {}\n",
    "    decodability_results = {}\n",
    "    trained_models_paths = {}\n",
    "\n",
    "    # --- 3. Training & Evaluation Loop ---\n",
    "    for model_name, model in models_to_test.items():\n",
    "        print(f\"\\n--- Training {model_name} ---\")\n",
    "        model.to(device)\n",
    "        print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "\n",
    "        try:\n",
    "            val_losses, best_model_state, hidden_states_test, coeffs_test = train_model_comparative(\n",
    "                model,\n",
    "                model_name,\n",
    "                train_loader,\n",
    "                val_loader, \n",
    "                test_loader, \n",
    "                CONFIG[\"epochs\"],\n",
    "                CONFIG[\"lr\"],\n",
    "                device,\n",
    "                CONFIG[\"num_task_coefficients\"], \n",
    "                run_results_dir,\n",
    "                plot_intermediate_results=(len(models_to_test) == 1) \n",
    "            )\n",
    "            all_val_losses[model_name] = val_losses\n",
    "            \n",
    "            if best_model_state:\n",
    "                model_path = os.path.join(run_results_dir, f\"{model_name}_best.pt\")\n",
    "                torch.save(best_model_state, model_path)\n",
    "                trained_models_paths[model_name] = model_path\n",
    "                print(f\"Saved best model for {model_name} to {model_path}\")\n",
    "            else:\n",
    "                print(f\"No best model state saved for {model_name} (possibly due to training issues).\")\n",
    "\n",
    "            # --- 4. Decodability Analysis ---\n",
    "            if hidden_states_test is not None and coeffs_test is not None:\n",
    "                print(f\"\\n--- Performing Decodability Analysis for {model_name} ---\")\n",
    "                decodability_score = perform_decodability_analysis(\n",
    "                    model_name=model_name, \n",
    "                    hidden_states=hidden_states_test, \n",
    "                    coefficients=coeffs_test,       \n",
    "                    decoder_type='ridge', # Using RidgeCV as a robust default\n",
    "                    decoding_metric='r2', # R-squared is often more interpretable than MSE here\n",
    "                    results_dir=run_results_dir,\n",
    "                    device=device,\n",
    "                )\n",
    "                decodability_results[model_name] = decodability_score\n",
    "                print(f\"Decodability (R2 score) for {model_name}: {decodability_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"Skipping decodability for {model_name} due to missing hidden states or coefficients.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"!!!!!! ERROR during training or analysis for {model_name}: {e} !!!!!!\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            all_val_losses[model_name] = [float('nan')] * CONFIG[\"epochs\"] # Log error for this model\n",
    "            decodability_results[model_name] = float('nan')\n",
    "\n",
    "\n",
    "    # --- 5. Plot Learning Curves ---\n",
    "    if any(all_val_losses.values()): # Check if there's anything to plot\n",
    "        plot_learning_curves(all_val_losses, title=\"Validation Learning Curves\", save_path=os.path.join(run_results_dir, \"learning_curves.png\"))\n",
    "        print(f\"\\nLearning curves plotted to {os.path.join(run_results_dir, 'learning_curves.png')}\")\n",
    "\n",
    "    # --- 6. Report Decodability ---\n",
    "    print(\"\\n--- Decodability Results (R2 Score) ---\")\n",
    "    if decodability_results:\n",
    "        for model_name, score in decodability_results.items():\n",
    "            print(f\"{model_name}: {score:.4f}\")\n",
    "        with open(os.path.join(run_results_dir, \"decodability_summary.txt\"), \"w\") as f:\n",
    "            f.write(\"Model,R2_Score\\n\")\n",
    "            for model_name, score in decodability_results.items():\n",
    "                f.write(f\"{model_name},{score:.4f}\\n\")\n",
    "    else:\n",
    "        print(\"No decodability results to report.\")\n",
    "        \n",
    "    print(f\"\\nExperiment finished. All results in {run_results_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ec43da-6ade-4862-81be-ed723ffe257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951229424500714"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-1.0 / 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba850af2-8fe7-4ac3-a31b-87c844aef3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
