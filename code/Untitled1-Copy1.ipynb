{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfca335-29cc-423c-a5a1-16b167672c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Results will be saved in: results/20250508_082527\n",
      "Loading dataset...\n",
      "Dataset loaded.\n",
      "\n",
      "--- Training ComplexOscillatorNet ---\n",
      "Number of parameters: 4480\n",
      "Epoch 1/200, Train Loss: 4.9201, Val Loss: 4.7620\n",
      "  New best validation loss: 4.7620\n",
      "Epoch 2/200, Train Loss: 4.5789, Val Loss: 4.4091\n",
      "  New best validation loss: 4.4091\n",
      "Epoch 3/200, Train Loss: 4.3282, Val Loss: 4.1753\n",
      "  New best validation loss: 4.1753\n",
      "Epoch 4/200, Train Loss: 4.0661, Val Loss: 3.9659\n",
      "  New best validation loss: 3.9659\n",
      "Epoch 5/200, Train Loss: 3.8658, Val Loss: 3.8017\n",
      "  New best validation loss: 3.8017\n",
      "Epoch 6/200, Train Loss: 3.7076, Val Loss: 3.6618\n",
      "  New best validation loss: 3.6618\n",
      "Epoch 7/200, Train Loss: 3.5907, Val Loss: 3.5625\n",
      "  New best validation loss: 3.5625\n",
      "Epoch 8/200, Train Loss: 3.5033, Val Loss: 3.4988\n",
      "  New best validation loss: 3.4988\n",
      "Epoch 9/200, Train Loss: 3.4445, Val Loss: 3.4559\n",
      "  New best validation loss: 3.4559\n",
      "Epoch 10/200, Train Loss: 3.3918, Val Loss: 3.4240\n",
      "  New best validation loss: 3.4240\n",
      "Epoch 11/200, Train Loss: 3.3591, Val Loss: 3.3929\n",
      "  New best validation loss: 3.3929\n",
      "Epoch 12/200, Train Loss: 3.3200, Val Loss: 3.3670\n",
      "  New best validation loss: 3.3670\n",
      "Epoch 13/200, Train Loss: 3.2859, Val Loss: 3.3369\n",
      "  New best validation loss: 3.3369\n",
      "Epoch 14/200, Train Loss: 3.2548, Val Loss: 3.3249\n",
      "  New best validation loss: 3.3249\n",
      "Epoch 15/200, Train Loss: 3.2252, Val Loss: 3.3029\n",
      "  New best validation loss: 3.3029\n",
      "Epoch 16/200, Train Loss: 3.2038, Val Loss: 3.2829\n",
      "  New best validation loss: 3.2829\n",
      "Epoch 17/200, Train Loss: 3.1829, Val Loss: 3.2637\n",
      "  New best validation loss: 3.2637\n",
      "Epoch 18/200, Train Loss: 3.1683, Val Loss: 3.2503\n",
      "  New best validation loss: 3.2503\n",
      "Epoch 19/200, Train Loss: 3.1526, Val Loss: 3.2527\n",
      "Epoch 20/200, Train Loss: 3.1344, Val Loss: 3.2244\n",
      "  New best validation loss: 3.2244\n",
      "Epoch 21/200, Train Loss: 3.1222, Val Loss: 3.2162\n",
      "  New best validation loss: 3.2162\n",
      "Epoch 22/200, Train Loss: 3.1127, Val Loss: 3.2097\n",
      "  New best validation loss: 3.2097\n",
      "Epoch 23/200, Train Loss: 3.0990, Val Loss: 3.1974\n",
      "  New best validation loss: 3.1974\n",
      "Epoch 24/200, Train Loss: 3.0880, Val Loss: 3.1916\n",
      "  New best validation loss: 3.1916\n",
      "Epoch 25/200, Train Loss: 3.0796, Val Loss: 3.1830\n",
      "  New best validation loss: 3.1830\n",
      "Epoch 26/200, Train Loss: 3.0722, Val Loss: 3.1798\n",
      "  New best validation loss: 3.1798\n",
      "Epoch 27/200, Train Loss: 3.0609, Val Loss: 3.1648\n",
      "  New best validation loss: 3.1648\n",
      "Epoch 28/200, Train Loss: 3.0537, Val Loss: 3.1605\n",
      "  New best validation loss: 3.1605\n",
      "Epoch 29/200, Train Loss: 3.0473, Val Loss: 3.1547\n",
      "  New best validation loss: 3.1547\n",
      "Epoch 30/200, Train Loss: 3.0411, Val Loss: 3.1548\n",
      "Epoch 31/200, Train Loss: 3.0285, Val Loss: 3.1424\n",
      "  New best validation loss: 3.1424\n",
      "Epoch 32/200, Train Loss: 3.0221, Val Loss: 3.1414\n",
      "  New best validation loss: 3.1414\n",
      "Epoch 33/200, Train Loss: 3.0143, Val Loss: 3.1280\n",
      "  New best validation loss: 3.1280\n",
      "Epoch 34/200, Train Loss: 3.0051, Val Loss: 3.1212\n",
      "  New best validation loss: 3.1212\n",
      "Epoch 35/200, Train Loss: 2.9973, Val Loss: 3.1180\n",
      "  New best validation loss: 3.1180\n",
      "Epoch 36/200, Train Loss: 2.9925, Val Loss: 3.1161\n",
      "  New best validation loss: 3.1161\n",
      "Epoch 37/200, Train Loss: 2.9853, Val Loss: 3.1096\n",
      "  New best validation loss: 3.1096\n",
      "Epoch 38/200, Train Loss: 2.9811, Val Loss: 3.1058\n",
      "  New best validation loss: 3.1058\n",
      "Epoch 39/200, Train Loss: 2.9749, Val Loss: 3.0975\n",
      "  New best validation loss: 3.0975\n",
      "Epoch 40/200, Train Loss: 2.9718, Val Loss: 3.0915\n",
      "  New best validation loss: 3.0915\n",
      "Epoch 41/200, Train Loss: 2.9652, Val Loss: 3.0912\n",
      "  New best validation loss: 3.0912\n",
      "Epoch 42/200, Train Loss: 2.9636, Val Loss: 3.0828\n",
      "  New best validation loss: 3.0828\n",
      "Epoch 43/200, Train Loss: 2.9553, Val Loss: 3.0812\n",
      "  New best validation loss: 3.0812\n",
      "Epoch 44/200, Train Loss: 2.9524, Val Loss: 3.0761\n",
      "  New best validation loss: 3.0761\n",
      "Epoch 45/200, Train Loss: 2.9533, Val Loss: 3.0762\n",
      "Epoch 46/200, Train Loss: 2.9443, Val Loss: 3.0717\n",
      "  New best validation loss: 3.0717\n",
      "Epoch 47/200, Train Loss: 2.9444, Val Loss: 3.0649\n",
      "  New best validation loss: 3.0649\n",
      "Epoch 48/200, Train Loss: 2.9384, Val Loss: 3.0687\n",
      "Epoch 49/200, Train Loss: 2.9332, Val Loss: 3.0638\n",
      "  New best validation loss: 3.0638\n",
      "Epoch 50/200, Train Loss: 2.9295, Val Loss: 3.0577\n",
      "  New best validation loss: 3.0577\n",
      "Epoch 51/200, Train Loss: 2.9243, Val Loss: 3.0577\n",
      "  New best validation loss: 3.0577\n",
      "Epoch 52/200, Train Loss: 2.9237, Val Loss: 3.0549\n",
      "  New best validation loss: 3.0549\n",
      "Epoch 53/200, Train Loss: 2.9202, Val Loss: 3.0481\n",
      "  New best validation loss: 3.0481\n",
      "Epoch 54/200, Train Loss: 2.9152, Val Loss: 3.0540\n",
      "Epoch 55/200, Train Loss: 2.9139, Val Loss: 3.0547\n",
      "Epoch 56/200, Train Loss: 2.9112, Val Loss: 3.0484\n",
      "Epoch 57/200, Train Loss: 2.9075, Val Loss: 3.0553\n",
      "Epoch 58/200, Train Loss: 2.9074, Val Loss: 3.0453\n",
      "  New best validation loss: 3.0453\n",
      "Epoch 59/200, Train Loss: 2.9017, Val Loss: 3.0495\n",
      "Epoch 60/200, Train Loss: 2.9004, Val Loss: 3.0392\n",
      "  New best validation loss: 3.0392\n",
      "Epoch 61/200, Train Loss: 2.8962, Val Loss: 3.0464\n",
      "Epoch 62/200, Train Loss: 2.8972, Val Loss: 3.0347\n",
      "  New best validation loss: 3.0347\n",
      "Epoch 63/200, Train Loss: 2.8954, Val Loss: 3.0349\n",
      "Epoch 64/200, Train Loss: 2.8937, Val Loss: 3.0320\n",
      "  New best validation loss: 3.0320\n",
      "Epoch 65/200, Train Loss: 2.8872, Val Loss: 3.0309\n",
      "  New best validation loss: 3.0309\n",
      "Epoch 66/200, Train Loss: 2.8823, Val Loss: 3.0261\n",
      "  New best validation loss: 3.0261\n",
      "Epoch 67/200, Train Loss: 2.8784, Val Loss: 3.0196\n",
      "  New best validation loss: 3.0196\n",
      "Epoch 68/200, Train Loss: 2.8799, Val Loss: 3.0272\n",
      "Epoch 69/200, Train Loss: 2.8812, Val Loss: 3.0308\n",
      "Epoch 70/200, Train Loss: 2.8794, Val Loss: 3.0282\n",
      "Epoch 71/200, Train Loss: 2.8757, Val Loss: 3.0209\n",
      "Epoch 72/200, Train Loss: 2.8712, Val Loss: 3.0196\n",
      "Epoch 73/200, Train Loss: 2.8724, Val Loss: 3.0093\n",
      "  New best validation loss: 3.0093\n",
      "Epoch 74/200, Train Loss: 2.8722, Val Loss: 3.0315\n",
      "Epoch 75/200, Train Loss: 2.8765, Val Loss: 3.0196\n",
      "Epoch 76/200, Train Loss: 2.8665, Val Loss: 3.0165\n",
      "Epoch 77/200, Train Loss: 2.8677, Val Loss: 3.0153\n",
      "Epoch 78/200, Train Loss: 2.8640, Val Loss: 3.0086\n",
      "  New best validation loss: 3.0086\n",
      "Epoch 79/200, Train Loss: 2.8599, Val Loss: 3.0116\n",
      "Epoch 80/200, Train Loss: 2.8584, Val Loss: 3.0149\n",
      "Epoch 81/200, Train Loss: 2.8570, Val Loss: 3.0188\n",
      "Epoch 82/200, Train Loss: 2.8550, Val Loss: 3.0106\n",
      "Epoch 83/200, Train Loss: 2.8512, Val Loss: 3.0065\n",
      "  New best validation loss: 3.0065\n",
      "Epoch 84/200, Train Loss: 2.8502, Val Loss: 3.0114\n",
      "Epoch 85/200, Train Loss: 2.8513, Val Loss: 3.0002\n",
      "  New best validation loss: 3.0002\n",
      "Epoch 86/200, Train Loss: 2.8513, Val Loss: 3.0111\n",
      "Epoch 87/200, Train Loss: 2.8464, Val Loss: 2.9976\n",
      "  New best validation loss: 2.9976\n",
      "Epoch 88/200, Train Loss: 2.8463, Val Loss: 3.0061\n",
      "Epoch 89/200, Train Loss: 2.8474, Val Loss: 3.0015\n",
      "Epoch 90/200, Train Loss: 2.8456, Val Loss: 2.9980\n",
      "Epoch 91/200, Train Loss: 2.8397, Val Loss: 3.0026\n",
      "Epoch 92/200, Train Loss: 2.8391, Val Loss: 2.9956\n",
      "  New best validation loss: 2.9956\n",
      "Epoch 93/200, Train Loss: 2.8369, Val Loss: 3.0068\n",
      "Epoch 94/200, Train Loss: 2.8355, Val Loss: 2.9937\n",
      "  New best validation loss: 2.9937\n",
      "Epoch 95/200, Train Loss: 2.8337, Val Loss: 2.9892\n",
      "  New best validation loss: 2.9892\n",
      "Epoch 96/200, Train Loss: 2.8335, Val Loss: 2.9934\n",
      "Epoch 97/200, Train Loss: 2.8385, Val Loss: 2.9988\n",
      "Epoch 98/200, Train Loss: 2.8354, Val Loss: 2.9921\n",
      "Epoch 99/200, Train Loss: 2.8335, Val Loss: 3.0028\n",
      "Epoch 100/200, Train Loss: 2.8307, Val Loss: 2.9957\n",
      "Epoch 101/200, Train Loss: 2.8301, Val Loss: 2.9938\n",
      "Epoch 102/200, Train Loss: 2.8304, Val Loss: 3.0002\n",
      "Epoch 103/200, Train Loss: 2.8301, Val Loss: 2.9992\n",
      "Epoch 104/200, Train Loss: 2.8272, Val Loss: 2.9937\n",
      "Epoch 105/200, Train Loss: 2.8275, Val Loss: 2.9979\n",
      "Epoch 106/200, Train Loss: 2.8289, Val Loss: 2.9941\n",
      "Epoch 107/200, Train Loss: 2.8314, Val Loss: 3.0030\n",
      "Epoch 108/200, Train Loss: 2.8391, Val Loss: 3.0062\n",
      "Epoch 109/200, Train Loss: 2.8396, Val Loss: 3.0040\n",
      "Epoch 110/200, Train Loss: 2.8313, Val Loss: 2.9871\n",
      "  New best validation loss: 2.9871\n",
      "Epoch 111/200, Train Loss: 2.8255, Val Loss: 2.9834\n",
      "  New best validation loss: 2.9834\n",
      "Epoch 112/200, Train Loss: 2.8215, Val Loss: 2.9841\n",
      "Epoch 113/200, Train Loss: 2.8203, Val Loss: 2.9895\n",
      "Epoch 114/200, Train Loss: 2.8208, Val Loss: 2.9813\n",
      "  New best validation loss: 2.9813\n",
      "Epoch 115/200, Train Loss: 2.8141, Val Loss: 2.9874\n",
      "Epoch 116/200, Train Loss: 2.8167, Val Loss: 2.9860\n",
      "Epoch 117/200, Train Loss: 2.8132, Val Loss: 2.9898\n",
      "Epoch 118/200, Train Loss: 2.8155, Val Loss: 2.9857\n",
      "Epoch 119/200, Train Loss: 2.8147, Val Loss: 2.9938\n",
      "Epoch 120/200, Train Loss: 2.8192, Val Loss: 2.9811\n",
      "  New best validation loss: 2.9811\n",
      "Epoch 121/200, Train Loss: 2.8132, Val Loss: 2.9873\n",
      "Epoch 122/200, Train Loss: 2.8076, Val Loss: 2.9948\n",
      "Epoch 123/200, Train Loss: 2.8103, Val Loss: 2.9860\n",
      "Epoch 124/200, Train Loss: 2.8099, Val Loss: 2.9786\n",
      "  New best validation loss: 2.9786\n",
      "Epoch 125/200, Train Loss: 2.8097, Val Loss: 2.9942\n",
      "Epoch 126/200, Train Loss: 2.8155, Val Loss: 2.9965\n",
      "Epoch 127/200, Train Loss: 2.8139, Val Loss: 2.9894\n",
      "Epoch 128/200, Train Loss: 2.8107, Val Loss: 2.9833\n",
      "Epoch 129/200, Train Loss: 2.8128, Val Loss: 2.9869\n",
      "Epoch 130/200, Train Loss: 2.8098, Val Loss: 2.9752\n",
      "  New best validation loss: 2.9752\n",
      "Epoch 131/200, Train Loss: 2.8054, Val Loss: 2.9841\n",
      "Epoch 132/200, Train Loss: 2.8065, Val Loss: 2.9824\n",
      "Epoch 133/200, Train Loss: 2.8035, Val Loss: 2.9831\n",
      "Epoch 134/200, Train Loss: 2.8053, Val Loss: 2.9779\n",
      "Epoch 135/200, Train Loss: 2.7984, Val Loss: 2.9985\n",
      "Epoch 136/200, Train Loss: 2.8007, Val Loss: 2.9838\n",
      "Epoch 137/200, Train Loss: 2.7962, Val Loss: 2.9778\n",
      "Epoch 138/200, Train Loss: 2.7926, Val Loss: 2.9761\n",
      "Epoch 139/200, Train Loss: 2.7960, Val Loss: 2.9833\n",
      "Epoch 140/200, Train Loss: 2.7947, Val Loss: 2.9793\n",
      "Epoch 141/200, Train Loss: 2.7934, Val Loss: 2.9804\n",
      "Epoch 142/200, Train Loss: 2.7979, Val Loss: 2.9935\n",
      "Epoch 143/200, Train Loss: 2.8013, Val Loss: 2.9843\n",
      "Epoch 144/200, Train Loss: 2.8007, Val Loss: 2.9783\n",
      "Epoch 145/200, Train Loss: 2.7987, Val Loss: 2.9769\n",
      "Epoch 146/200, Train Loss: 2.7971, Val Loss: 2.9783\n",
      "Epoch 147/200, Train Loss: 2.7976, Val Loss: 2.9757\n",
      "Epoch 148/200, Train Loss: 2.7970, Val Loss: 2.9900\n",
      "Epoch 149/200, Train Loss: 2.7954, Val Loss: 2.9882\n",
      "Epoch 150/200, Train Loss: 2.7979, Val Loss: 2.9724\n",
      "  New best validation loss: 2.9724\n",
      "Epoch 151/200, Train Loss: 2.7977, Val Loss: 2.9792\n",
      "Epoch 152/200, Train Loss: 2.8010, Val Loss: 2.9852\n",
      "Epoch 153/200, Train Loss: 2.8111, Val Loss: 2.9819\n",
      "Epoch 154/200, Train Loss: 2.7982, Val Loss: 2.9832\n",
      "Epoch 155/200, Train Loss: 2.7930, Val Loss: 2.9705\n",
      "  New best validation loss: 2.9705\n",
      "Epoch 156/200, Train Loss: 2.7951, Val Loss: 2.9743\n",
      "Epoch 157/200, Train Loss: 2.7948, Val Loss: 2.9881\n",
      "Epoch 158/200, Train Loss: 2.7964, Val Loss: 2.9828\n",
      "Epoch 159/200, Train Loss: 2.7886, Val Loss: 2.9781\n",
      "Epoch 160/200, Train Loss: 2.7877, Val Loss: 2.9758\n",
      "Epoch 161/200, Train Loss: 2.7883, Val Loss: 2.9798\n",
      "Epoch 162/200, Train Loss: 2.7884, Val Loss: 2.9756\n",
      "Epoch 163/200, Train Loss: 2.7845, Val Loss: 2.9728\n",
      "Epoch 164/200, Train Loss: 2.7799, Val Loss: 2.9743\n",
      "Epoch 165/200, Train Loss: 2.7798, Val Loss: 2.9681\n",
      "  New best validation loss: 2.9681\n",
      "Epoch 166/200, Train Loss: 2.7783, Val Loss: 2.9735\n",
      "Epoch 167/200, Train Loss: 2.7803, Val Loss: 2.9782\n",
      "Epoch 168/200, Train Loss: 2.7803, Val Loss: 2.9728\n",
      "Epoch 169/200, Train Loss: 2.7861, Val Loss: 2.9854\n",
      "Epoch 170/200, Train Loss: 2.7844, Val Loss: 2.9783\n",
      "Epoch 171/200, Train Loss: 2.7818, Val Loss: 2.9710\n",
      "Epoch 172/200, Train Loss: 2.7777, Val Loss: 2.9709\n",
      "Epoch 173/200, Train Loss: 2.7797, Val Loss: 2.9814\n",
      "Epoch 174/200, Train Loss: 2.7837, Val Loss: 2.9744\n",
      "Epoch 175/200, Train Loss: 2.7801, Val Loss: 2.9741\n",
      "Epoch 176/200, Train Loss: 2.7783, Val Loss: 2.9629\n",
      "  New best validation loss: 2.9629\n",
      "Epoch 177/200, Train Loss: 2.7747, Val Loss: 2.9659\n",
      "Epoch 178/200, Train Loss: 2.7737, Val Loss: 2.9703\n",
      "Epoch 179/200, Train Loss: 2.7745, Val Loss: 2.9711\n",
      "Epoch 180/200, Train Loss: 2.7764, Val Loss: 2.9827\n",
      "Epoch 181/200, Train Loss: 2.7778, Val Loss: 2.9791\n",
      "Epoch 182/200, Train Loss: 2.7779, Val Loss: 2.9782\n",
      "Epoch 183/200, Train Loss: 2.7825, Val Loss: 2.9727\n",
      "Epoch 184/200, Train Loss: 2.7763, Val Loss: 2.9672\n",
      "Epoch 185/200, Train Loss: 2.7796, Val Loss: 2.9699\n",
      "Epoch 186/200, Train Loss: 2.7738, Val Loss: 2.9782\n",
      "Epoch 187/200, Train Loss: 2.7799, Val Loss: 2.9763\n",
      "Epoch 188/200, Train Loss: 2.7760, Val Loss: 2.9696\n",
      "Epoch 189/200, Train Loss: 2.7727, Val Loss: 2.9631\n",
      "Epoch 190/200, Train Loss: 2.7764, Val Loss: 2.9674\n",
      "Epoch 191/200, Train Loss: 2.7745, Val Loss: 2.9659\n",
      "Epoch 192/200, Train Loss: 2.7769, Val Loss: 2.9622\n",
      "  New best validation loss: 2.9622\n",
      "Epoch 193/200, Train Loss: 2.7734, Val Loss: 2.9649\n",
      "Epoch 194/200, Train Loss: 2.7733, Val Loss: 2.9770\n",
      "Epoch 195/200, Train Loss: 2.7739, Val Loss: 2.9706\n",
      "Epoch 196/200, Train Loss: 2.7711, Val Loss: 2.9697\n",
      "Epoch 197/200, Train Loss: 2.7710, Val Loss: 2.9783\n",
      "Epoch 198/200, Train Loss: 2.7754, Val Loss: 2.9681\n",
      "Epoch 199/200, Train Loss: 2.7702, Val Loss: 2.9644\n",
      "Epoch 200/200, Train Loss: 2.7716, Val Loss: 2.9645\n",
      "\n",
      "Loaded best model (Val Loss: 2.9622) for final hidden state extraction.\n",
      "Saved best model for ComplexOscillatorNet to results/20250508_082527/ComplexOscillatorNet_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for ComplexOscillatorNet ---\n",
      "  Analyzing decodability for ComplexOscillatorNet...\n",
      "  Hidden states shape: torch.Size([160, 200, 64])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for ComplexOscillatorNet - Test MSE: 0.0605, Test R2: 0.9709 (best alpha: 1.2743)\n",
      "Decodability (R2 score) for ComplexOscillatorNet: 0.9709\n",
      "\n",
      "--- Training RNN_GRU ---\n",
      "Number of parameters: 50561\n",
      "Epoch 1/200, Train Loss: 4.8265, Val Loss: 5.2117\n",
      "  New best validation loss: 5.2117\n",
      "Epoch 2/200, Train Loss: 4.7924, Val Loss: 5.0764\n",
      "  New best validation loss: 5.0764\n",
      "Epoch 3/200, Train Loss: 4.7707, Val Loss: 5.1396\n",
      "Epoch 4/200, Train Loss: 4.7394, Val Loss: 5.0988\n",
      "Epoch 5/200, Train Loss: 4.7277, Val Loss: 5.1501\n",
      "Epoch 6/200, Train Loss: 4.7182, Val Loss: 5.0424\n",
      "  New best validation loss: 5.0424\n",
      "Epoch 7/200, Train Loss: 4.7303, Val Loss: 5.1388\n",
      "Epoch 8/200, Train Loss: 4.7113, Val Loss: 5.0679\n",
      "Epoch 9/200, Train Loss: 4.7093, Val Loss: 5.0527\n",
      "Epoch 10/200, Train Loss: 4.6897, Val Loss: 5.0944\n",
      "Epoch 11/200, Train Loss: 4.6845, Val Loss: 5.0749\n",
      "Epoch 12/200, Train Loss: 4.6761, Val Loss: 5.0849\n",
      "Epoch 13/200, Train Loss: 4.6684, Val Loss: 5.0234\n",
      "  New best validation loss: 5.0234\n",
      "Epoch 14/200, Train Loss: 4.6521, Val Loss: 5.0573\n",
      "Epoch 15/200, Train Loss: 4.6441, Val Loss: 5.0201\n",
      "  New best validation loss: 5.0201\n",
      "Epoch 16/200, Train Loss: 4.6142, Val Loss: 5.0198\n",
      "  New best validation loss: 5.0198\n",
      "Epoch 17/200, Train Loss: 4.6353, Val Loss: 4.9464\n",
      "  New best validation loss: 4.9464\n",
      "Epoch 18/200, Train Loss: 4.5297, Val Loss: 4.7721\n",
      "  New best validation loss: 4.7721\n",
      "Epoch 19/200, Train Loss: 4.2350, Val Loss: 4.6043\n",
      "  New best validation loss: 4.6043\n",
      "Epoch 20/200, Train Loss: 4.0221, Val Loss: 4.3158\n",
      "  New best validation loss: 4.3158\n",
      "Epoch 21/200, Train Loss: 3.8879, Val Loss: 4.3527\n",
      "Epoch 22/200, Train Loss: 3.8762, Val Loss: 4.1611\n",
      "  New best validation loss: 4.1611\n",
      "Epoch 23/200, Train Loss: 3.7864, Val Loss: 4.0002\n",
      "  New best validation loss: 4.0002\n",
      "Epoch 24/200, Train Loss: 3.7110, Val Loss: 3.9996\n",
      "  New best validation loss: 3.9996\n",
      "Epoch 25/200, Train Loss: 3.6700, Val Loss: 3.9559\n",
      "  New best validation loss: 3.9559\n",
      "Epoch 26/200, Train Loss: 3.6367, Val Loss: 3.8993\n",
      "  New best validation loss: 3.8993\n",
      "Epoch 27/200, Train Loss: 3.6100, Val Loss: 3.9718\n",
      "Epoch 28/200, Train Loss: 3.5717, Val Loss: 3.8616\n",
      "  New best validation loss: 3.8616\n",
      "Epoch 29/200, Train Loss: 3.5076, Val Loss: 3.7183\n",
      "  New best validation loss: 3.7183\n",
      "Epoch 30/200, Train Loss: 3.4259, Val Loss: 3.8250\n",
      "Epoch 31/200, Train Loss: 3.3407, Val Loss: 3.7150\n",
      "  New best validation loss: 3.7150\n",
      "Epoch 32/200, Train Loss: 3.2029, Val Loss: 3.5306\n",
      "  New best validation loss: 3.5306\n",
      "Epoch 33/200, Train Loss: 3.2129, Val Loss: 3.6834\n",
      "Epoch 34/200, Train Loss: 3.2131, Val Loss: 3.5791\n",
      "Epoch 35/200, Train Loss: 3.0419, Val Loss: 3.3802\n",
      "  New best validation loss: 3.3802\n",
      "Epoch 36/200, Train Loss: 2.9639, Val Loss: 3.2087\n",
      "  New best validation loss: 3.2087\n",
      "Epoch 37/200, Train Loss: 2.8188, Val Loss: 3.2107\n",
      "Epoch 38/200, Train Loss: 2.7033, Val Loss: 2.8835\n",
      "  New best validation loss: 2.8835\n",
      "Epoch 39/200, Train Loss: 2.6397, Val Loss: 2.8881\n",
      "Epoch 40/200, Train Loss: 2.5716, Val Loss: 2.7611\n",
      "  New best validation loss: 2.7611\n",
      "Epoch 41/200, Train Loss: 2.4875, Val Loss: 2.7465\n",
      "  New best validation loss: 2.7465\n",
      "Epoch 42/200, Train Loss: 2.4313, Val Loss: 2.7671\n",
      "Epoch 43/200, Train Loss: 2.3899, Val Loss: 2.3981\n",
      "  New best validation loss: 2.3981\n",
      "Epoch 44/200, Train Loss: 2.2914, Val Loss: 2.8026\n",
      "Epoch 45/200, Train Loss: 2.1411, Val Loss: 2.2558\n",
      "  New best validation loss: 2.2558\n",
      "Epoch 46/200, Train Loss: 2.0057, Val Loss: 2.3667\n",
      "Epoch 47/200, Train Loss: 1.9921, Val Loss: 2.1346\n",
      "  New best validation loss: 2.1346\n",
      "Epoch 48/200, Train Loss: 1.9283, Val Loss: 2.2170\n",
      "Epoch 49/200, Train Loss: 1.9242, Val Loss: 2.1341\n",
      "  New best validation loss: 2.1341\n",
      "Epoch 50/200, Train Loss: 1.7978, Val Loss: 2.0001\n",
      "  New best validation loss: 2.0001\n",
      "Epoch 51/200, Train Loss: 1.7662, Val Loss: 2.0080\n",
      "Epoch 52/200, Train Loss: 1.8101, Val Loss: 1.7647\n",
      "  New best validation loss: 1.7647\n",
      "Epoch 53/200, Train Loss: 1.7172, Val Loss: 1.9012\n",
      "Epoch 54/200, Train Loss: 1.7041, Val Loss: 1.7692\n",
      "Epoch 55/200, Train Loss: 1.5998, Val Loss: 1.7658\n",
      "Epoch 56/200, Train Loss: 1.6262, Val Loss: 1.6791\n",
      "  New best validation loss: 1.6791\n",
      "Epoch 57/200, Train Loss: 1.5168, Val Loss: 1.7050\n",
      "Epoch 58/200, Train Loss: 1.4586, Val Loss: 1.6818\n",
      "Epoch 59/200, Train Loss: 1.4286, Val Loss: 1.5502\n",
      "  New best validation loss: 1.5502\n",
      "Epoch 60/200, Train Loss: 1.3855, Val Loss: 1.4945\n",
      "  New best validation loss: 1.4945\n",
      "Epoch 61/200, Train Loss: 1.3051, Val Loss: 1.3722\n",
      "  New best validation loss: 1.3722\n",
      "Epoch 62/200, Train Loss: 1.3128, Val Loss: 1.5600\n",
      "Epoch 63/200, Train Loss: 1.4269, Val Loss: 1.6335\n",
      "Epoch 64/200, Train Loss: 1.3769, Val Loss: 1.3611\n",
      "  New best validation loss: 1.3611\n",
      "Epoch 65/200, Train Loss: 1.3025, Val Loss: 1.4723\n",
      "Epoch 66/200, Train Loss: 1.2574, Val Loss: 1.3940\n",
      "Epoch 67/200, Train Loss: 1.1896, Val Loss: 1.2475\n",
      "  New best validation loss: 1.2475\n",
      "Epoch 68/200, Train Loss: 1.1586, Val Loss: 1.2656\n",
      "Epoch 69/200, Train Loss: 1.1581, Val Loss: 1.3047\n",
      "Epoch 70/200, Train Loss: 1.1382, Val Loss: 1.1991\n",
      "  New best validation loss: 1.1991\n",
      "Epoch 71/200, Train Loss: 1.0917, Val Loss: 1.1702\n",
      "  New best validation loss: 1.1702\n",
      "Epoch 72/200, Train Loss: 1.1130, Val Loss: 1.2809\n",
      "Epoch 73/200, Train Loss: 1.1004, Val Loss: 1.1909\n",
      "Epoch 74/200, Train Loss: 1.0841, Val Loss: 1.2496\n",
      "Epoch 75/200, Train Loss: 1.0407, Val Loss: 1.1111\n",
      "  New best validation loss: 1.1111\n",
      "Epoch 76/200, Train Loss: 0.9958, Val Loss: 1.0101\n",
      "  New best validation loss: 1.0101\n",
      "Epoch 77/200, Train Loss: 1.0270, Val Loss: 0.9975\n",
      "  New best validation loss: 0.9975\n",
      "Epoch 78/200, Train Loss: 1.0089, Val Loss: 1.1023\n",
      "Epoch 79/200, Train Loss: 0.9754, Val Loss: 0.9905\n",
      "  New best validation loss: 0.9905\n",
      "Epoch 80/200, Train Loss: 0.9426, Val Loss: 0.9450\n",
      "  New best validation loss: 0.9450\n",
      "Epoch 81/200, Train Loss: 0.9037, Val Loss: 1.0639\n",
      "Epoch 82/200, Train Loss: 0.9712, Val Loss: 0.8802\n",
      "  New best validation loss: 0.8802\n",
      "Epoch 83/200, Train Loss: 0.9479, Val Loss: 0.8804\n",
      "Epoch 84/200, Train Loss: 0.8490, Val Loss: 0.8881\n",
      "Epoch 85/200, Train Loss: 0.8118, Val Loss: 0.8796\n",
      "  New best validation loss: 0.8796\n",
      "Epoch 86/200, Train Loss: 0.8007, Val Loss: 0.8153\n",
      "  New best validation loss: 0.8153\n",
      "Epoch 87/200, Train Loss: 0.8554, Val Loss: 0.9805\n",
      "Epoch 88/200, Train Loss: 0.8792, Val Loss: 0.8983\n",
      "Epoch 89/200, Train Loss: 0.8125, Val Loss: 0.9042\n",
      "Epoch 90/200, Train Loss: 0.8355, Val Loss: 1.0796\n",
      "Epoch 91/200, Train Loss: 0.8548, Val Loss: 0.9700\n",
      "Epoch 92/200, Train Loss: 0.7988, Val Loss: 0.8552\n",
      "Epoch 93/200, Train Loss: 0.7465, Val Loss: 0.8251\n",
      "Epoch 94/200, Train Loss: 0.6957, Val Loss: 0.7254\n",
      "  New best validation loss: 0.7254\n",
      "Epoch 95/200, Train Loss: 0.7402, Val Loss: 0.8774\n",
      "Epoch 96/200, Train Loss: 0.7007, Val Loss: 0.7243\n",
      "  New best validation loss: 0.7243\n",
      "Epoch 97/200, Train Loss: 0.6452, Val Loss: 0.6945\n",
      "  New best validation loss: 0.6945\n",
      "Epoch 98/200, Train Loss: 0.6720, Val Loss: 0.7251\n",
      "Epoch 99/200, Train Loss: 0.6767, Val Loss: 0.6975\n",
      "Epoch 100/200, Train Loss: 0.6454, Val Loss: 0.6937\n",
      "  New best validation loss: 0.6937\n",
      "Epoch 101/200, Train Loss: 0.6154, Val Loss: 0.6384\n",
      "  New best validation loss: 0.6384\n",
      "Epoch 102/200, Train Loss: 0.6317, Val Loss: 0.6992\n",
      "Epoch 103/200, Train Loss: 0.6570, Val Loss: 0.7471\n",
      "Epoch 104/200, Train Loss: 0.6746, Val Loss: 0.6319\n",
      "  New best validation loss: 0.6319\n",
      "Epoch 105/200, Train Loss: 0.6405, Val Loss: 0.6917\n",
      "Epoch 106/200, Train Loss: 0.6223, Val Loss: 0.5935\n",
      "  New best validation loss: 0.5935\n",
      "Epoch 107/200, Train Loss: 0.6134, Val Loss: 0.6730\n",
      "Epoch 108/200, Train Loss: 0.6534, Val Loss: 0.6530\n",
      "Epoch 109/200, Train Loss: 0.6512, Val Loss: 0.6350\n",
      "Epoch 110/200, Train Loss: 0.6239, Val Loss: 0.7209\n",
      "Epoch 111/200, Train Loss: 0.5934, Val Loss: 0.6067\n",
      "Epoch 112/200, Train Loss: 0.5688, Val Loss: 0.5956\n",
      "Epoch 113/200, Train Loss: 0.5392, Val Loss: 0.5622\n",
      "  New best validation loss: 0.5622\n",
      "Epoch 114/200, Train Loss: 0.5629, Val Loss: 0.6102\n",
      "Epoch 115/200, Train Loss: 0.5751, Val Loss: 0.5635\n",
      "Epoch 116/200, Train Loss: 0.5353, Val Loss: 0.5587\n",
      "  New best validation loss: 0.5587\n",
      "Epoch 117/200, Train Loss: 0.5307, Val Loss: 0.5790\n",
      "Epoch 118/200, Train Loss: 0.5479, Val Loss: 0.5475\n",
      "  New best validation loss: 0.5475\n",
      "Epoch 119/200, Train Loss: 0.5682, Val Loss: 0.6042\n",
      "Epoch 120/200, Train Loss: 0.5980, Val Loss: 0.6783\n",
      "Epoch 121/200, Train Loss: 0.5832, Val Loss: 0.5503\n",
      "Epoch 122/200, Train Loss: 0.5562, Val Loss: 0.6093\n",
      "Epoch 123/200, Train Loss: 0.6191, Val Loss: 0.6443\n",
      "Epoch 124/200, Train Loss: 0.6634, Val Loss: 0.5824\n",
      "Epoch 125/200, Train Loss: 0.5818, Val Loss: 0.5679\n",
      "Epoch 126/200, Train Loss: 0.5536, Val Loss: 0.5506\n",
      "Epoch 127/200, Train Loss: 0.5309, Val Loss: 0.5610\n",
      "Epoch 128/200, Train Loss: 0.5299, Val Loss: 0.5401\n",
      "  New best validation loss: 0.5401\n",
      "Epoch 129/200, Train Loss: 0.4978, Val Loss: 0.5692\n",
      "Epoch 130/200, Train Loss: 0.5497, Val Loss: 0.5498\n",
      "Epoch 131/200, Train Loss: 0.4992, Val Loss: 0.4947\n",
      "  New best validation loss: 0.4947\n",
      "Epoch 132/200, Train Loss: 0.4787, Val Loss: 0.5101\n",
      "Epoch 133/200, Train Loss: 0.4846, Val Loss: 0.5001\n",
      "Epoch 134/200, Train Loss: 0.4988, Val Loss: 0.5504\n",
      "Epoch 135/200, Train Loss: 0.4649, Val Loss: 0.4922\n",
      "  New best validation loss: 0.4922\n",
      "Epoch 136/200, Train Loss: 0.5087, Val Loss: 0.6348\n",
      "Epoch 137/200, Train Loss: 0.4950, Val Loss: 0.6402\n",
      "Epoch 138/200, Train Loss: 0.4960, Val Loss: 0.5472\n",
      "Epoch 139/200, Train Loss: 0.5214, Val Loss: 0.5328\n",
      "Epoch 140/200, Train Loss: 0.5524, Val Loss: 0.5071\n",
      "Epoch 141/200, Train Loss: 0.4993, Val Loss: 0.6149\n",
      "Epoch 142/200, Train Loss: 0.5085, Val Loss: 0.5414\n",
      "Epoch 143/200, Train Loss: 0.5318, Val Loss: 0.6129\n",
      "Epoch 144/200, Train Loss: 0.5153, Val Loss: 0.5570\n",
      "Epoch 145/200, Train Loss: 0.4834, Val Loss: 0.4573\n",
      "  New best validation loss: 0.4573\n",
      "Epoch 146/200, Train Loss: 0.4463, Val Loss: 0.4590\n",
      "Epoch 147/200, Train Loss: 0.4299, Val Loss: 0.4342\n",
      "  New best validation loss: 0.4342\n",
      "Epoch 148/200, Train Loss: 0.4187, Val Loss: 0.5298\n",
      "Epoch 149/200, Train Loss: 0.4504, Val Loss: 0.4596\n",
      "Epoch 150/200, Train Loss: 0.4420, Val Loss: 0.4372\n",
      "Epoch 151/200, Train Loss: 0.4338, Val Loss: 0.4424\n",
      "Epoch 152/200, Train Loss: 0.4749, Val Loss: 0.6237\n",
      "Epoch 153/200, Train Loss: 0.4834, Val Loss: 0.4879\n",
      "Epoch 154/200, Train Loss: 0.4872, Val Loss: 0.4799\n",
      "Epoch 155/200, Train Loss: 0.4810, Val Loss: 0.4679\n",
      "Epoch 156/200, Train Loss: 0.4663, Val Loss: 0.4409\n",
      "Epoch 157/200, Train Loss: 0.4389, Val Loss: 0.5042\n",
      "Epoch 158/200, Train Loss: 0.5070, Val Loss: 0.6090\n",
      "Epoch 159/200, Train Loss: 0.5111, Val Loss: 0.5302\n",
      "Epoch 160/200, Train Loss: 0.4453, Val Loss: 0.4132\n",
      "  New best validation loss: 0.4132\n",
      "Epoch 161/200, Train Loss: 0.4331, Val Loss: 0.4587\n",
      "Epoch 162/200, Train Loss: 0.4142, Val Loss: 0.4507\n",
      "Epoch 163/200, Train Loss: 0.4252, Val Loss: 0.4277\n",
      "Epoch 164/200, Train Loss: 0.4131, Val Loss: 0.4246\n",
      "Epoch 165/200, Train Loss: 0.4045, Val Loss: 0.4276\n",
      "Epoch 166/200, Train Loss: 0.4341, Val Loss: 0.4847\n",
      "Epoch 167/200, Train Loss: 0.5251, Val Loss: 0.5302\n",
      "Epoch 168/200, Train Loss: 0.4456, Val Loss: 0.4417\n",
      "Epoch 169/200, Train Loss: 0.4335, Val Loss: 0.4520\n",
      "Epoch 170/200, Train Loss: 0.4521, Val Loss: 0.4319\n",
      "Epoch 171/200, Train Loss: 0.4114, Val Loss: 0.4446\n",
      "Epoch 172/200, Train Loss: 0.4027, Val Loss: 0.3950\n",
      "  New best validation loss: 0.3950\n",
      "Epoch 173/200, Train Loss: 0.3775, Val Loss: 0.4443\n",
      "Epoch 174/200, Train Loss: 0.4035, Val Loss: 0.4393\n",
      "Epoch 175/200, Train Loss: 0.4622, Val Loss: 0.3885\n",
      "  New best validation loss: 0.3885\n",
      "Epoch 176/200, Train Loss: 0.5106, Val Loss: 0.5236\n",
      "Epoch 177/200, Train Loss: 0.4059, Val Loss: 0.3930\n",
      "Epoch 178/200, Train Loss: 0.3765, Val Loss: 0.3829\n",
      "  New best validation loss: 0.3829\n",
      "Epoch 179/200, Train Loss: 0.4482, Val Loss: 0.4496\n",
      "Epoch 180/200, Train Loss: 0.4181, Val Loss: 0.3945\n",
      "Epoch 181/200, Train Loss: 0.3899, Val Loss: 0.3809\n",
      "  New best validation loss: 0.3809\n",
      "Epoch 182/200, Train Loss: 0.3625, Val Loss: 0.3746\n",
      "  New best validation loss: 0.3746\n",
      "Epoch 183/200, Train Loss: 0.3724, Val Loss: 0.3944\n",
      "Epoch 184/200, Train Loss: 0.3901, Val Loss: 0.3816\n",
      "Epoch 185/200, Train Loss: 0.3939, Val Loss: 0.3869\n",
      "Epoch 186/200, Train Loss: 0.3910, Val Loss: 0.4910\n",
      "Epoch 187/200, Train Loss: 0.3941, Val Loss: 0.4761\n",
      "Epoch 188/200, Train Loss: 0.4108, Val Loss: 0.4437\n",
      "Epoch 189/200, Train Loss: 0.3980, Val Loss: 0.4607\n",
      "Epoch 190/200, Train Loss: 0.4190, Val Loss: 0.4074\n",
      "Epoch 191/200, Train Loss: 0.4091, Val Loss: 0.4328\n",
      "Epoch 192/200, Train Loss: 0.3876, Val Loss: 0.4313\n",
      "Epoch 193/200, Train Loss: 0.4623, Val Loss: 0.5150\n",
      "Epoch 194/200, Train Loss: 0.4231, Val Loss: 0.4190\n",
      "Epoch 195/200, Train Loss: 0.3909, Val Loss: 0.3542\n",
      "  New best validation loss: 0.3542\n",
      "Epoch 196/200, Train Loss: 0.3628, Val Loss: 0.3841\n",
      "Epoch 197/200, Train Loss: 0.3541, Val Loss: 0.3607\n",
      "Epoch 198/200, Train Loss: 0.3932, Val Loss: 0.3921\n",
      "Epoch 199/200, Train Loss: 0.3625, Val Loss: 0.3801\n",
      "Epoch 200/200, Train Loss: 0.3420, Val Loss: 0.3446\n",
      "  New best validation loss: 0.3446\n",
      "\n",
      "Loaded best model (Val Loss: 0.3446) for final hidden state extraction.\n",
      "Saved best model for RNN_GRU to results/20250508_082527/RNN_GRU_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for RNN_GRU ---\n",
      "  Analyzing decodability for RNN_GRU...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for RNN_GRU - Test MSE: 0.0025, Test R2: 0.9988 (best alpha: 0.0018)\n",
      "Decodability (R2 score) for RNN_GRU: 0.9988\n",
      "\n",
      "--- Training Transformer ---\n",
      "Number of parameters: 281345\n",
      "Epoch 1/200, Train Loss: 6.3340, Val Loss: 5.1674\n",
      "  New best validation loss: 5.1674\n",
      "Epoch 2/200, Train Loss: 5.0713, Val Loss: 5.1637\n",
      "  New best validation loss: 5.1637\n",
      "Epoch 3/200, Train Loss: 4.9144, Val Loss: 5.0780\n",
      "  New best validation loss: 5.0780\n",
      "Epoch 4/200, Train Loss: 4.8872, Val Loss: 5.1059\n",
      "Epoch 5/200, Train Loss: 4.8702, Val Loss: 5.2255\n",
      "Epoch 6/200, Train Loss: 4.8611, Val Loss: 5.1043\n",
      "Epoch 7/200, Train Loss: 4.8276, Val Loss: 5.0451\n",
      "  New best validation loss: 5.0451\n",
      "Epoch 8/200, Train Loss: 4.8400, Val Loss: 5.1266\n",
      "Epoch 9/200, Train Loss: 4.7974, Val Loss: 5.0273\n",
      "  New best validation loss: 5.0273\n",
      "Epoch 10/200, Train Loss: 4.8014, Val Loss: 5.0947\n",
      "Epoch 11/200, Train Loss: 4.7987, Val Loss: 5.2067\n",
      "Epoch 12/200, Train Loss: 4.7881, Val Loss: 5.1057\n",
      "Epoch 13/200, Train Loss: 4.7792, Val Loss: 5.0071\n",
      "  New best validation loss: 5.0071\n",
      "Epoch 14/200, Train Loss: 4.7531, Val Loss: 5.1384\n",
      "Epoch 15/200, Train Loss: 4.7360, Val Loss: 5.0039\n",
      "  New best validation loss: 5.0039\n",
      "Epoch 16/200, Train Loss: 4.7693, Val Loss: 4.9479\n",
      "  New best validation loss: 4.9479\n",
      "Epoch 17/200, Train Loss: 4.7244, Val Loss: 4.8679\n",
      "  New best validation loss: 4.8679\n",
      "Epoch 18/200, Train Loss: 4.7423, Val Loss: 4.9348\n",
      "Epoch 19/200, Train Loss: 4.7533, Val Loss: 4.8165\n",
      "  New best validation loss: 4.8165\n",
      "Epoch 20/200, Train Loss: 4.7086, Val Loss: 4.8221\n",
      "Epoch 21/200, Train Loss: 4.6462, Val Loss: 4.8061\n",
      "  New best validation loss: 4.8061\n",
      "Epoch 22/200, Train Loss: 4.6248, Val Loss: 4.7985\n",
      "  New best validation loss: 4.7985\n",
      "Epoch 23/200, Train Loss: 4.6136, Val Loss: 4.8650\n",
      "Epoch 24/200, Train Loss: 4.6038, Val Loss: 4.6911\n",
      "  New best validation loss: 4.6911\n",
      "Epoch 25/200, Train Loss: 4.6234, Val Loss: 4.6145\n",
      "  New best validation loss: 4.6145\n",
      "Epoch 26/200, Train Loss: 4.6277, Val Loss: 4.6189\n",
      "Epoch 27/200, Train Loss: 4.5939, Val Loss: 4.5884\n",
      "  New best validation loss: 4.5884\n",
      "Epoch 28/200, Train Loss: 4.4966, Val Loss: 4.7713\n",
      "Epoch 29/200, Train Loss: 4.4686, Val Loss: 4.7347\n",
      "Epoch 30/200, Train Loss: 4.4904, Val Loss: 4.5492\n",
      "  New best validation loss: 4.5492\n",
      "Epoch 31/200, Train Loss: 4.4846, Val Loss: 4.6944\n",
      "Epoch 32/200, Train Loss: 4.4715, Val Loss: 4.6044\n",
      "Epoch 33/200, Train Loss: 4.4310, Val Loss: 4.4642\n",
      "  New best validation loss: 4.4642\n",
      "Epoch 34/200, Train Loss: 4.4478, Val Loss: 4.3370\n",
      "  New best validation loss: 4.3370\n",
      "Epoch 35/200, Train Loss: 4.3583, Val Loss: 4.2102\n",
      "  New best validation loss: 4.2102\n",
      "Epoch 36/200, Train Loss: 4.3053, Val Loss: 4.0215\n",
      "  New best validation loss: 4.0215\n",
      "Epoch 37/200, Train Loss: 4.1688, Val Loss: 3.8164\n",
      "  New best validation loss: 3.8164\n",
      "Epoch 38/200, Train Loss: 4.1726, Val Loss: 3.8139\n",
      "  New best validation loss: 3.8139\n",
      "Epoch 39/200, Train Loss: 4.1023, Val Loss: 3.9078\n",
      "Epoch 40/200, Train Loss: 4.1397, Val Loss: 3.5954\n",
      "  New best validation loss: 3.5954\n",
      "Epoch 41/200, Train Loss: 4.0634, Val Loss: 3.5222\n",
      "  New best validation loss: 3.5222\n",
      "Epoch 42/200, Train Loss: 3.9095, Val Loss: 3.5752\n",
      "Epoch 43/200, Train Loss: 3.8536, Val Loss: 3.4564\n",
      "  New best validation loss: 3.4564\n",
      "Epoch 44/200, Train Loss: 3.6967, Val Loss: 3.1144\n",
      "  New best validation loss: 3.1144\n",
      "Epoch 45/200, Train Loss: 3.5255, Val Loss: 2.9384\n",
      "  New best validation loss: 2.9384\n",
      "Epoch 46/200, Train Loss: 3.4072, Val Loss: 2.6993\n",
      "  New best validation loss: 2.6993\n",
      "Epoch 47/200, Train Loss: 3.3376, Val Loss: 3.1177\n",
      "Epoch 48/200, Train Loss: 3.1609, Val Loss: 2.5584\n",
      "  New best validation loss: 2.5584\n",
      "Epoch 49/200, Train Loss: 2.9729, Val Loss: 2.3823\n",
      "  New best validation loss: 2.3823\n",
      "Epoch 50/200, Train Loss: 2.8960, Val Loss: 2.1574\n",
      "  New best validation loss: 2.1574\n",
      "Epoch 51/200, Train Loss: 2.6566, Val Loss: 1.9757\n",
      "  New best validation loss: 1.9757\n",
      "Epoch 52/200, Train Loss: 2.5844, Val Loss: 2.1428\n",
      "Epoch 53/200, Train Loss: 2.4314, Val Loss: 1.7401\n",
      "  New best validation loss: 1.7401\n",
      "Epoch 54/200, Train Loss: 2.3003, Val Loss: 1.8741\n",
      "Epoch 55/200, Train Loss: 2.2035, Val Loss: 1.4617\n",
      "  New best validation loss: 1.4617\n",
      "Epoch 56/200, Train Loss: 2.0218, Val Loss: 1.2837\n",
      "  New best validation loss: 1.2837\n",
      "Epoch 57/200, Train Loss: 1.8841, Val Loss: 1.5053\n",
      "Epoch 58/200, Train Loss: 1.9898, Val Loss: 1.3452\n",
      "Epoch 59/200, Train Loss: 1.8136, Val Loss: 1.2895\n",
      "Epoch 60/200, Train Loss: 1.7735, Val Loss: 1.1655\n",
      "  New best validation loss: 1.1655\n",
      "Epoch 61/200, Train Loss: 1.5904, Val Loss: 1.1597\n",
      "  New best validation loss: 1.1597\n",
      "Epoch 62/200, Train Loss: 1.4636, Val Loss: 0.9653\n",
      "  New best validation loss: 0.9653\n",
      "Epoch 63/200, Train Loss: 1.4330, Val Loss: 0.8039\n",
      "  New best validation loss: 0.8039\n",
      "Epoch 64/200, Train Loss: 1.3043, Val Loss: 0.8020\n",
      "  New best validation loss: 0.8020\n",
      "Epoch 65/200, Train Loss: 1.2497, Val Loss: 0.6884\n",
      "  New best validation loss: 0.6884\n",
      "Epoch 66/200, Train Loss: 1.1985, Val Loss: 0.6567\n",
      "  New best validation loss: 0.6567\n",
      "Epoch 67/200, Train Loss: 1.1242, Val Loss: 0.7802\n",
      "Epoch 68/200, Train Loss: 1.1278, Val Loss: 0.5650\n",
      "  New best validation loss: 0.5650\n",
      "Epoch 69/200, Train Loss: 1.0313, Val Loss: 0.5761\n",
      "Epoch 70/200, Train Loss: 0.9568, Val Loss: 0.4443\n",
      "  New best validation loss: 0.4443\n",
      "Epoch 71/200, Train Loss: 0.9209, Val Loss: 0.4656\n",
      "Epoch 72/200, Train Loss: 0.8815, Val Loss: 0.4931\n",
      "Epoch 73/200, Train Loss: 0.9059, Val Loss: 0.5496\n",
      "Epoch 74/200, Train Loss: 0.8566, Val Loss: 0.3722\n",
      "  New best validation loss: 0.3722\n",
      "Epoch 75/200, Train Loss: 0.8198, Val Loss: 0.3309\n",
      "  New best validation loss: 0.3309\n",
      "Epoch 76/200, Train Loss: 0.7859, Val Loss: 0.4272\n",
      "Epoch 77/200, Train Loss: 0.8094, Val Loss: 0.3155\n",
      "  New best validation loss: 0.3155\n",
      "Epoch 78/200, Train Loss: 0.7273, Val Loss: 0.3932\n",
      "Epoch 79/200, Train Loss: 0.7124, Val Loss: 0.2767\n",
      "  New best validation loss: 0.2767\n",
      "Epoch 80/200, Train Loss: 0.6849, Val Loss: 0.3185\n",
      "Epoch 81/200, Train Loss: 0.6732, Val Loss: 0.3208\n",
      "Epoch 82/200, Train Loss: 0.6511, Val Loss: 0.2682\n",
      "  New best validation loss: 0.2682\n",
      "Epoch 83/200, Train Loss: 0.6550, Val Loss: 0.3020\n",
      "Epoch 84/200, Train Loss: 0.6393, Val Loss: 0.3206\n",
      "Epoch 85/200, Train Loss: 0.6375, Val Loss: 0.3154\n",
      "Epoch 86/200, Train Loss: 0.6029, Val Loss: 0.3228\n",
      "Epoch 87/200, Train Loss: 0.6092, Val Loss: 0.2891\n",
      "Epoch 88/200, Train Loss: 0.5819, Val Loss: 0.3025\n",
      "Epoch 89/200, Train Loss: 0.5933, Val Loss: 0.2390\n",
      "  New best validation loss: 0.2390\n",
      "Epoch 90/200, Train Loss: 0.5818, Val Loss: 0.2385\n",
      "  New best validation loss: 0.2385\n",
      "Epoch 91/200, Train Loss: 0.5674, Val Loss: 0.2386\n",
      "Epoch 92/200, Train Loss: 0.5385, Val Loss: 0.1712\n",
      "  New best validation loss: 0.1712\n",
      "Epoch 93/200, Train Loss: 0.5452, Val Loss: 0.1992\n",
      "Epoch 94/200, Train Loss: 0.5194, Val Loss: 0.2048\n",
      "Epoch 95/200, Train Loss: 0.5077, Val Loss: 0.1761\n",
      "Epoch 96/200, Train Loss: 0.5210, Val Loss: 0.1718\n",
      "Epoch 97/200, Train Loss: 0.5222, Val Loss: 0.2255\n",
      "Epoch 98/200, Train Loss: 0.5153, Val Loss: 0.1883\n",
      "Epoch 99/200, Train Loss: 0.5089, Val Loss: 0.1785\n",
      "Epoch 100/200, Train Loss: 0.4805, Val Loss: 0.1729\n",
      "Epoch 101/200, Train Loss: 0.4751, Val Loss: 0.1704\n",
      "  New best validation loss: 0.1704\n",
      "Epoch 102/200, Train Loss: 0.4654, Val Loss: 0.2114\n",
      "Epoch 103/200, Train Loss: 0.4744, Val Loss: 0.2233\n",
      "Epoch 104/200, Train Loss: 0.4737, Val Loss: 0.1724\n",
      "Epoch 105/200, Train Loss: 0.4679, Val Loss: 0.2324\n",
      "Epoch 106/200, Train Loss: 0.4515, Val Loss: 0.1786\n",
      "Epoch 107/200, Train Loss: 0.4350, Val Loss: 0.1373\n",
      "  New best validation loss: 0.1373\n",
      "Epoch 108/200, Train Loss: 0.4267, Val Loss: 0.1797\n",
      "Epoch 109/200, Train Loss: 0.4638, Val Loss: 0.1863\n",
      "Epoch 110/200, Train Loss: 0.4694, Val Loss: 0.2763\n",
      "Epoch 111/200, Train Loss: 0.4809, Val Loss: 0.1776\n",
      "Epoch 112/200, Train Loss: 0.4652, Val Loss: 0.2782\n",
      "Epoch 113/200, Train Loss: 0.4588, Val Loss: 0.1526\n",
      "Epoch 114/200, Train Loss: 0.4189, Val Loss: 0.1694\n",
      "Epoch 115/200, Train Loss: 0.4249, Val Loss: 0.1224\n",
      "  New best validation loss: 0.1224\n",
      "Epoch 116/200, Train Loss: 0.4082, Val Loss: 0.1804\n",
      "Epoch 117/200, Train Loss: 0.4327, Val Loss: 0.1204\n",
      "  New best validation loss: 0.1204\n",
      "Epoch 118/200, Train Loss: 0.4418, Val Loss: 0.2415\n",
      "Epoch 119/200, Train Loss: 0.4162, Val Loss: 0.1550\n",
      "Epoch 120/200, Train Loss: 0.4045, Val Loss: 0.1472\n",
      "Epoch 121/200, Train Loss: 0.3969, Val Loss: 0.1280\n",
      "Epoch 122/200, Train Loss: 0.3851, Val Loss: 0.1412\n",
      "Epoch 123/200, Train Loss: 0.3851, Val Loss: 0.1338\n",
      "Epoch 124/200, Train Loss: 0.3953, Val Loss: 0.1130\n",
      "  New best validation loss: 0.1130\n",
      "Epoch 125/200, Train Loss: 0.3834, Val Loss: 0.1168\n",
      "Epoch 126/200, Train Loss: 0.3843, Val Loss: 0.1125\n",
      "  New best validation loss: 0.1125\n",
      "Epoch 127/200, Train Loss: 0.3748, Val Loss: 0.1365\n",
      "Epoch 128/200, Train Loss: 0.3740, Val Loss: 0.1082\n",
      "  New best validation loss: 0.1082\n",
      "Epoch 129/200, Train Loss: 0.3600, Val Loss: 0.1371\n",
      "Epoch 130/200, Train Loss: 0.3642, Val Loss: 0.1216\n",
      "Epoch 131/200, Train Loss: 0.3779, Val Loss: 0.1406\n",
      "Epoch 132/200, Train Loss: 0.3679, Val Loss: 0.1252\n",
      "Epoch 133/200, Train Loss: 0.3777, Val Loss: 0.1227\n",
      "Epoch 134/200, Train Loss: 0.3616, Val Loss: 0.1630\n",
      "Epoch 135/200, Train Loss: 0.3507, Val Loss: 0.1065\n",
      "  New best validation loss: 0.1065\n",
      "Epoch 136/200, Train Loss: 0.3475, Val Loss: 0.1086\n",
      "Epoch 137/200, Train Loss: 0.3388, Val Loss: 0.0970\n",
      "  New best validation loss: 0.0970\n",
      "Epoch 138/200, Train Loss: 0.3496, Val Loss: 0.1030\n",
      "Epoch 139/200, Train Loss: 0.3533, Val Loss: 0.0985\n",
      "Epoch 140/200, Train Loss: 0.3498, Val Loss: 0.0823\n",
      "  New best validation loss: 0.0823\n",
      "Epoch 141/200, Train Loss: 0.3392, Val Loss: 0.0954\n",
      "Epoch 142/200, Train Loss: 0.3466, Val Loss: 0.0915\n",
      "Epoch 143/200, Train Loss: 0.3524, Val Loss: 0.1331\n",
      "Epoch 144/200, Train Loss: 0.3489, Val Loss: 0.0883\n",
      "Epoch 145/200, Train Loss: 0.3271, Val Loss: 0.0981\n",
      "Epoch 146/200, Train Loss: 0.3345, Val Loss: 0.1167\n",
      "Epoch 147/200, Train Loss: 0.3756, Val Loss: 0.1523\n",
      "Epoch 148/200, Train Loss: 0.3504, Val Loss: 0.1175\n",
      "Epoch 149/200, Train Loss: 0.3482, Val Loss: 0.1296\n",
      "Epoch 150/200, Train Loss: 0.3498, Val Loss: 0.1178\n",
      "Epoch 151/200, Train Loss: 0.3399, Val Loss: 0.1099\n",
      "Epoch 152/200, Train Loss: 0.3218, Val Loss: 0.1291\n",
      "Epoch 153/200, Train Loss: 0.3238, Val Loss: 0.1378\n",
      "Epoch 154/200, Train Loss: 0.3243, Val Loss: 0.1702\n",
      "Epoch 155/200, Train Loss: 0.3367, Val Loss: 0.0821\n",
      "  New best validation loss: 0.0821\n",
      "Epoch 156/200, Train Loss: 0.3348, Val Loss: 0.1207\n",
      "Epoch 157/200, Train Loss: 0.3085, Val Loss: 0.1050\n",
      "Epoch 158/200, Train Loss: 0.3138, Val Loss: 0.0902\n",
      "Epoch 159/200, Train Loss: 0.3240, Val Loss: 0.1173\n",
      "Epoch 160/200, Train Loss: 0.3170, Val Loss: 0.0853\n",
      "Epoch 161/200, Train Loss: 0.3135, Val Loss: 0.1363\n",
      "Epoch 162/200, Train Loss: 0.3086, Val Loss: 0.0679\n",
      "  New best validation loss: 0.0679\n",
      "Epoch 163/200, Train Loss: 0.3004, Val Loss: 0.1088\n",
      "Epoch 164/200, Train Loss: 0.3046, Val Loss: 0.1189\n",
      "Epoch 165/200, Train Loss: 0.3136, Val Loss: 0.0941\n",
      "Epoch 166/200, Train Loss: 0.2936, Val Loss: 0.0888\n",
      "Epoch 167/200, Train Loss: 0.2927, Val Loss: 0.0735\n",
      "Epoch 168/200, Train Loss: 0.2989, Val Loss: 0.0731\n",
      "Epoch 169/200, Train Loss: 0.3099, Val Loss: 0.0971\n",
      "Epoch 170/200, Train Loss: 0.3290, Val Loss: 0.0787\n",
      "Epoch 171/200, Train Loss: 0.3120, Val Loss: 0.1115\n",
      "Epoch 172/200, Train Loss: 0.2922, Val Loss: 0.0860\n",
      "Epoch 173/200, Train Loss: 0.2951, Val Loss: 0.1108\n",
      "Epoch 174/200, Train Loss: 0.2871, Val Loss: 0.0705\n",
      "Epoch 175/200, Train Loss: 0.2984, Val Loss: 0.0914\n",
      "Epoch 176/200, Train Loss: 0.2850, Val Loss: 0.0801\n",
      "Epoch 177/200, Train Loss: 0.2961, Val Loss: 0.1047\n",
      "Epoch 178/200, Train Loss: 0.3007, Val Loss: 0.0855\n",
      "Epoch 179/200, Train Loss: 0.2953, Val Loss: 0.0853\n",
      "Epoch 180/200, Train Loss: 0.2785, Val Loss: 0.0845\n",
      "Epoch 181/200, Train Loss: 0.2723, Val Loss: 0.0666\n",
      "  New best validation loss: 0.0666\n",
      "Epoch 182/200, Train Loss: 0.2709, Val Loss: 0.0823\n",
      "Epoch 183/200, Train Loss: 0.2816, Val Loss: 0.0643\n",
      "  New best validation loss: 0.0643\n",
      "Epoch 184/200, Train Loss: 0.2777, Val Loss: 0.0762\n",
      "Epoch 185/200, Train Loss: 0.2931, Val Loss: 0.0736\n",
      "Epoch 186/200, Train Loss: 0.2715, Val Loss: 0.0676\n",
      "Epoch 187/200, Train Loss: 0.2822, Val Loss: 0.0661\n",
      "Epoch 188/200, Train Loss: 0.2839, Val Loss: 0.0639\n",
      "  New best validation loss: 0.0639\n",
      "Epoch 189/200, Train Loss: 0.2738, Val Loss: 0.0677\n",
      "Epoch 190/200, Train Loss: 0.2835, Val Loss: 0.0863\n",
      "Epoch 191/200, Train Loss: 0.2816, Val Loss: 0.0655\n",
      "Epoch 192/200, Train Loss: 0.2783, Val Loss: 0.0868\n",
      "Epoch 193/200, Train Loss: 0.2742, Val Loss: 0.0784\n",
      "Epoch 194/200, Train Loss: 0.2775, Val Loss: 0.0650\n",
      "Epoch 195/200, Train Loss: 0.2725, Val Loss: 0.0621\n",
      "  New best validation loss: 0.0621\n",
      "Epoch 196/200, Train Loss: 0.2675, Val Loss: 0.0711\n",
      "Epoch 197/200, Train Loss: 0.2789, Val Loss: 0.0574\n",
      "  New best validation loss: 0.0574\n",
      "Epoch 198/200, Train Loss: 0.2644, Val Loss: 0.0915\n",
      "Epoch 199/200, Train Loss: 0.2840, Val Loss: 0.0999\n",
      "Epoch 200/200, Train Loss: 0.2563, Val Loss: 0.0779\n",
      "\n",
      "Loaded best model (Val Loss: 0.0574) for final hidden state extraction.\n",
      "Saved best model for Transformer to results/20250508_082527/Transformer_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for Transformer ---\n",
      "  Analyzing decodability for Transformer...\n",
      "  Hidden states shape: torch.Size([160, 200, 64])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 64])\n",
      "  RidgeCV Decoder for Transformer - Test MSE: 0.0093, Test R2: 0.9955 (best alpha: 0.0038)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decodability (R2 score) for Transformer: 0.9955\n",
      "\n",
      "--- Training HIPPORNN_LegT ---\n",
      "Number of parameters: 99716\n",
      "Epoch 1/200, Train Loss: 5.0083, Val Loss: 5.1213\n",
      "  New best validation loss: 5.1213\n",
      "Epoch 2/200, Train Loss: 5.0074, Val Loss: 5.1210\n",
      "  New best validation loss: 5.1210\n",
      "Epoch 3/200, Train Loss: 5.0041, Val Loss: 5.1205\n",
      "  New best validation loss: 5.1205\n",
      "Epoch 4/200, Train Loss: 4.9989, Val Loss: 5.1176\n",
      "  New best validation loss: 5.1176\n",
      "Epoch 5/200, Train Loss: 4.9818, Val Loss: 5.1115\n",
      "  New best validation loss: 5.1115\n",
      "Epoch 6/200, Train Loss: 4.9218, Val Loss: 5.0782\n",
      "  New best validation loss: 5.0782\n",
      "Epoch 7/200, Train Loss: 4.7406, Val Loss: 4.9201\n",
      "  New best validation loss: 4.9201\n",
      "Epoch 8/200, Train Loss: 4.5687, Val Loss: 4.8765\n",
      "  New best validation loss: 4.8765\n",
      "Epoch 9/200, Train Loss: 4.3929, Val Loss: 4.7006\n",
      "  New best validation loss: 4.7006\n",
      "Epoch 10/200, Train Loss: 4.3576, Val Loss: 4.7061\n",
      "Epoch 11/200, Train Loss: 4.1944, Val Loss: 4.4919\n",
      "  New best validation loss: 4.4919\n",
      "Epoch 12/200, Train Loss: 4.0346, Val Loss: 4.2581\n",
      "  New best validation loss: 4.2581\n",
      "Epoch 13/200, Train Loss: 3.9406, Val Loss: 4.1949\n",
      "  New best validation loss: 4.1949\n",
      "Epoch 14/200, Train Loss: 3.7608, Val Loss: 3.9479\n",
      "  New best validation loss: 3.9479\n",
      "Epoch 15/200, Train Loss: 3.5593, Val Loss: 3.8430\n",
      "  New best validation loss: 3.8430\n",
      "Epoch 16/200, Train Loss: 3.4206, Val Loss: 3.6531\n",
      "  New best validation loss: 3.6531\n",
      "Epoch 17/200, Train Loss: 3.2835, Val Loss: 3.4307\n",
      "  New best validation loss: 3.4307\n",
      "Epoch 18/200, Train Loss: 3.1909, Val Loss: 3.3476\n",
      "  New best validation loss: 3.3476\n",
      "Epoch 19/200, Train Loss: 3.0816, Val Loss: 3.2669\n",
      "  New best validation loss: 3.2669\n",
      "Epoch 20/200, Train Loss: 3.0284, Val Loss: 3.1651\n",
      "  New best validation loss: 3.1651\n",
      "Epoch 21/200, Train Loss: 2.8408, Val Loss: 3.0035\n",
      "  New best validation loss: 3.0035\n",
      "Epoch 22/200, Train Loss: 2.7643, Val Loss: 2.9089\n",
      "  New best validation loss: 2.9089\n",
      "Epoch 23/200, Train Loss: 2.6665, Val Loss: 2.7724\n",
      "  New best validation loss: 2.7724\n",
      "Epoch 24/200, Train Loss: 2.5444, Val Loss: 2.6748\n",
      "  New best validation loss: 2.6748\n",
      "Epoch 25/200, Train Loss: 2.4834, Val Loss: 2.5370\n",
      "  New best validation loss: 2.5370\n",
      "Epoch 26/200, Train Loss: 2.3030, Val Loss: 2.3903\n",
      "  New best validation loss: 2.3903\n",
      "Epoch 27/200, Train Loss: 2.2112, Val Loss: 2.2829\n",
      "  New best validation loss: 2.2829\n",
      "Epoch 28/200, Train Loss: 2.2558, Val Loss: 2.4154\n",
      "Epoch 29/200, Train Loss: 2.1721, Val Loss: 2.2244\n",
      "  New best validation loss: 2.2244\n",
      "Epoch 30/200, Train Loss: 2.0514, Val Loss: 2.1888\n",
      "  New best validation loss: 2.1888\n",
      "Epoch 31/200, Train Loss: 1.9693, Val Loss: 2.0233\n",
      "  New best validation loss: 2.0233\n",
      "Epoch 32/200, Train Loss: 1.9192, Val Loss: 1.9444\n",
      "  New best validation loss: 1.9444\n",
      "Epoch 33/200, Train Loss: 1.8111, Val Loss: 1.8883\n",
      "  New best validation loss: 1.8883\n",
      "Epoch 34/200, Train Loss: 1.7510, Val Loss: 1.8225\n",
      "  New best validation loss: 1.8225\n",
      "Epoch 35/200, Train Loss: 1.6871, Val Loss: 1.7423\n",
      "  New best validation loss: 1.7423\n",
      "Epoch 36/200, Train Loss: 1.6524, Val Loss: 1.7062\n",
      "  New best validation loss: 1.7062\n",
      "Epoch 37/200, Train Loss: 1.6166, Val Loss: 1.6624\n",
      "  New best validation loss: 1.6624\n",
      "Epoch 38/200, Train Loss: 1.5712, Val Loss: 1.6307\n",
      "  New best validation loss: 1.6307\n",
      "Epoch 39/200, Train Loss: 1.5545, Val Loss: 1.6470\n",
      "Epoch 40/200, Train Loss: 1.5528, Val Loss: 1.6838\n",
      "Epoch 41/200, Train Loss: 1.5532, Val Loss: 1.6122\n",
      "  New best validation loss: 1.6122\n",
      "Epoch 42/200, Train Loss: 1.5055, Val Loss: 1.6443\n",
      "Epoch 43/200, Train Loss: 1.4995, Val Loss: 1.5794\n",
      "  New best validation loss: 1.5794\n",
      "Epoch 44/200, Train Loss: 1.4634, Val Loss: 1.5188\n",
      "  New best validation loss: 1.5188\n",
      "Epoch 45/200, Train Loss: 1.4742, Val Loss: 1.5702\n",
      "Epoch 46/200, Train Loss: 1.4592, Val Loss: 1.4954\n",
      "  New best validation loss: 1.4954\n",
      "Epoch 47/200, Train Loss: 1.4277, Val Loss: 1.5711\n",
      "Epoch 48/200, Train Loss: 1.4490, Val Loss: 1.4825\n",
      "  New best validation loss: 1.4825\n",
      "Epoch 49/200, Train Loss: 1.4220, Val Loss: 1.5710\n",
      "Epoch 50/200, Train Loss: 1.4930, Val Loss: 1.5065\n",
      "Epoch 51/200, Train Loss: 1.4494, Val Loss: 1.5246\n",
      "Epoch 52/200, Train Loss: 1.3899, Val Loss: 1.4698\n",
      "  New best validation loss: 1.4698\n",
      "Epoch 53/200, Train Loss: 1.3692, Val Loss: 1.4356\n",
      "  New best validation loss: 1.4356\n",
      "Epoch 54/200, Train Loss: 1.3692, Val Loss: 1.5226\n",
      "Epoch 55/200, Train Loss: 1.3483, Val Loss: 1.4160\n",
      "  New best validation loss: 1.4160\n",
      "Epoch 56/200, Train Loss: 1.3407, Val Loss: 1.3817\n",
      "  New best validation loss: 1.3817\n",
      "Epoch 57/200, Train Loss: 1.2977, Val Loss: 1.4403\n",
      "Epoch 58/200, Train Loss: 1.2989, Val Loss: 1.3901\n",
      "Epoch 59/200, Train Loss: 1.2645, Val Loss: 1.3029\n",
      "  New best validation loss: 1.3029\n",
      "Epoch 60/200, Train Loss: 1.2714, Val Loss: 1.5641\n",
      "Epoch 61/200, Train Loss: 1.3259, Val Loss: 1.3187\n",
      "Epoch 62/200, Train Loss: 1.2623, Val Loss: 1.3492\n",
      "Epoch 63/200, Train Loss: 1.2630, Val Loss: 1.3499\n",
      "Epoch 64/200, Train Loss: 1.2310, Val Loss: 1.2467\n",
      "  New best validation loss: 1.2467\n",
      "Epoch 65/200, Train Loss: 1.1622, Val Loss: 1.2123\n",
      "  New best validation loss: 1.2123\n",
      "Epoch 66/200, Train Loss: 1.1133, Val Loss: 1.2501\n",
      "Epoch 67/200, Train Loss: 1.1746, Val Loss: 1.2763\n",
      "Epoch 68/200, Train Loss: 1.1411, Val Loss: 1.2064\n",
      "  New best validation loss: 1.2064\n",
      "Epoch 69/200, Train Loss: 1.1324, Val Loss: 1.1361\n",
      "  New best validation loss: 1.1361\n",
      "Epoch 70/200, Train Loss: 1.1363, Val Loss: 1.2301\n",
      "Epoch 71/200, Train Loss: 1.0945, Val Loss: 1.1815\n",
      "Epoch 72/200, Train Loss: 1.0664, Val Loss: 1.1975\n",
      "Epoch 73/200, Train Loss: 1.0468, Val Loss: 1.1688\n",
      "Epoch 74/200, Train Loss: 1.0415, Val Loss: 1.0329\n",
      "  New best validation loss: 1.0329\n",
      "Epoch 75/200, Train Loss: 0.9443, Val Loss: 0.9819\n",
      "  New best validation loss: 0.9819\n",
      "Epoch 76/200, Train Loss: 0.8758, Val Loss: 0.9433\n",
      "  New best validation loss: 0.9433\n",
      "Epoch 77/200, Train Loss: 0.8971, Val Loss: 0.9254\n",
      "  New best validation loss: 0.9254\n",
      "Epoch 78/200, Train Loss: 0.9761, Val Loss: 1.0134\n",
      "Epoch 79/200, Train Loss: 0.9800, Val Loss: 0.9271\n",
      "Epoch 80/200, Train Loss: 0.8743, Val Loss: 0.9975\n",
      "Epoch 81/200, Train Loss: 0.8445, Val Loss: 0.8978\n",
      "  New best validation loss: 0.8978\n",
      "Epoch 82/200, Train Loss: 0.8194, Val Loss: 0.9123\n",
      "Epoch 83/200, Train Loss: 0.8337, Val Loss: 0.8910\n",
      "  New best validation loss: 0.8910\n",
      "Epoch 84/200, Train Loss: 0.8198, Val Loss: 0.8414\n",
      "  New best validation loss: 0.8414\n",
      "Epoch 85/200, Train Loss: 0.9771, Val Loss: 1.2467\n",
      "Epoch 86/200, Train Loss: 0.9532, Val Loss: 1.0088\n",
      "Epoch 87/200, Train Loss: 0.8163, Val Loss: 0.9792\n",
      "Epoch 88/200, Train Loss: 0.8180, Val Loss: 0.8344\n",
      "  New best validation loss: 0.8344\n",
      "Epoch 89/200, Train Loss: 0.7809, Val Loss: 0.8099\n",
      "  New best validation loss: 0.8099\n",
      "Epoch 90/200, Train Loss: 0.7622, Val Loss: 0.7981\n",
      "  New best validation loss: 0.7981\n",
      "Epoch 91/200, Train Loss: 0.7333, Val Loss: 0.7872\n",
      "  New best validation loss: 0.7872\n",
      "Epoch 92/200, Train Loss: 0.7089, Val Loss: 0.8257\n",
      "Epoch 93/200, Train Loss: 0.7221, Val Loss: 0.7807\n",
      "  New best validation loss: 0.7807\n",
      "Epoch 94/200, Train Loss: 0.7928, Val Loss: 0.8078\n",
      "Epoch 95/200, Train Loss: 0.7393, Val Loss: 0.7904\n",
      "Epoch 96/200, Train Loss: 0.7171, Val Loss: 0.7710\n",
      "  New best validation loss: 0.7710\n",
      "Epoch 97/200, Train Loss: 0.6804, Val Loss: 0.7398\n",
      "  New best validation loss: 0.7398\n",
      "Epoch 98/200, Train Loss: 0.6874, Val Loss: 0.7128\n",
      "  New best validation loss: 0.7128\n",
      "Epoch 99/200, Train Loss: 0.6877, Val Loss: 0.7149\n",
      "Epoch 100/200, Train Loss: 0.6746, Val Loss: 0.7574\n",
      "Epoch 101/200, Train Loss: 0.7212, Val Loss: 0.6948\n",
      "  New best validation loss: 0.6948\n",
      "Epoch 102/200, Train Loss: 0.6849, Val Loss: 0.8099\n",
      "Epoch 103/200, Train Loss: 0.6759, Val Loss: 0.6730\n",
      "  New best validation loss: 0.6730\n",
      "Epoch 104/200, Train Loss: 0.6554, Val Loss: 0.6758\n",
      "Epoch 105/200, Train Loss: 0.6417, Val Loss: 0.7344\n",
      "Epoch 106/200, Train Loss: 0.7144, Val Loss: 0.8197\n",
      "Epoch 107/200, Train Loss: 0.6866, Val Loss: 0.7441\n",
      "Epoch 108/200, Train Loss: 0.6701, Val Loss: 0.6185\n",
      "  New best validation loss: 0.6185\n",
      "Epoch 109/200, Train Loss: 0.6823, Val Loss: 0.7146\n",
      "Epoch 110/200, Train Loss: 0.6449, Val Loss: 0.6990\n",
      "Epoch 111/200, Train Loss: 0.6030, Val Loss: 0.6260\n",
      "Epoch 112/200, Train Loss: 0.5938, Val Loss: 0.6286\n",
      "Epoch 113/200, Train Loss: 0.5973, Val Loss: 0.6619\n",
      "Epoch 114/200, Train Loss: 0.5757, Val Loss: 0.6270\n",
      "Epoch 115/200, Train Loss: 0.5755, Val Loss: 0.5681\n",
      "  New best validation loss: 0.5681\n",
      "Epoch 116/200, Train Loss: 0.5340, Val Loss: 0.5756\n",
      "Epoch 117/200, Train Loss: 0.5744, Val Loss: 0.5712\n",
      "Epoch 118/200, Train Loss: 0.5472, Val Loss: 0.5752\n",
      "Epoch 119/200, Train Loss: 0.5670, Val Loss: 0.5993\n",
      "Epoch 120/200, Train Loss: 0.5404, Val Loss: 0.7034\n",
      "Epoch 121/200, Train Loss: 0.5963, Val Loss: 0.6380\n",
      "Epoch 122/200, Train Loss: 0.5129, Val Loss: 0.5407\n",
      "  New best validation loss: 0.5407\n",
      "Epoch 123/200, Train Loss: 0.4985, Val Loss: 0.5651\n",
      "Epoch 124/200, Train Loss: 0.5009, Val Loss: 0.5569\n",
      "Epoch 125/200, Train Loss: 0.5193, Val Loss: 0.5932\n",
      "Epoch 126/200, Train Loss: 0.6017, Val Loss: 0.5735\n",
      "Epoch 127/200, Train Loss: 0.5342, Val Loss: 0.5574\n",
      "Epoch 128/200, Train Loss: 0.5284, Val Loss: 0.5267\n",
      "  New best validation loss: 0.5267\n",
      "Epoch 129/200, Train Loss: 0.5078, Val Loss: 0.5251\n",
      "  New best validation loss: 0.5251\n",
      "Epoch 130/200, Train Loss: 0.4839, Val Loss: 0.5167\n",
      "  New best validation loss: 0.5167\n",
      "Epoch 131/200, Train Loss: 0.4802, Val Loss: 0.5295\n",
      "Epoch 132/200, Train Loss: 0.4579, Val Loss: 0.4806\n",
      "  New best validation loss: 0.4806\n",
      "Epoch 133/200, Train Loss: 0.4767, Val Loss: 0.5373\n",
      "Epoch 134/200, Train Loss: 0.4972, Val Loss: 0.5922\n",
      "Epoch 135/200, Train Loss: 0.5551, Val Loss: 0.5944\n",
      "Epoch 136/200, Train Loss: 0.5355, Val Loss: 0.6356\n",
      "Epoch 137/200, Train Loss: 0.5348, Val Loss: 0.4987\n",
      "Epoch 138/200, Train Loss: 0.4849, Val Loss: 0.5062\n",
      "Epoch 139/200, Train Loss: 0.4755, Val Loss: 0.4992\n",
      "Epoch 140/200, Train Loss: 0.4610, Val Loss: 0.4668\n",
      "  New best validation loss: 0.4668\n",
      "Epoch 141/200, Train Loss: 0.4900, Val Loss: 0.5413\n",
      "Epoch 142/200, Train Loss: 0.5712, Val Loss: 0.5930\n",
      "Epoch 143/200, Train Loss: 0.5606, Val Loss: 0.5545\n",
      "Epoch 144/200, Train Loss: 0.5005, Val Loss: 0.5319\n",
      "Epoch 145/200, Train Loss: 0.5028, Val Loss: 0.4848\n",
      "Epoch 146/200, Train Loss: 0.4715, Val Loss: 0.5638\n",
      "Epoch 147/200, Train Loss: 0.4932, Val Loss: 0.5003\n",
      "Epoch 148/200, Train Loss: 0.4750, Val Loss: 0.5024\n",
      "Epoch 149/200, Train Loss: 0.5086, Val Loss: 0.5920\n",
      "Epoch 150/200, Train Loss: 0.5021, Val Loss: 0.5154\n",
      "Epoch 151/200, Train Loss: 0.4707, Val Loss: 0.4930\n",
      "Epoch 152/200, Train Loss: 0.4756, Val Loss: 0.4864\n",
      "Epoch 153/200, Train Loss: 0.4591, Val Loss: 0.4694\n",
      "Epoch 154/200, Train Loss: 0.4834, Val Loss: 0.4706\n",
      "Epoch 155/200, Train Loss: 0.4486, Val Loss: 0.4751\n",
      "Epoch 156/200, Train Loss: 0.4429, Val Loss: 0.5087\n",
      "Epoch 157/200, Train Loss: 0.4446, Val Loss: 0.4349\n",
      "  New best validation loss: 0.4349\n",
      "Epoch 158/200, Train Loss: 0.4193, Val Loss: 0.4243\n",
      "  New best validation loss: 0.4243\n",
      "Epoch 159/200, Train Loss: 0.4381, Val Loss: 0.4779\n",
      "Epoch 160/200, Train Loss: 0.4359, Val Loss: 0.4659\n",
      "Epoch 161/200, Train Loss: 0.4909, Val Loss: 0.6713\n",
      "Epoch 162/200, Train Loss: 0.5421, Val Loss: 0.4585\n",
      "Epoch 163/200, Train Loss: 0.5003, Val Loss: 0.5903\n",
      "Epoch 164/200, Train Loss: 0.4671, Val Loss: 0.5374\n",
      "Epoch 165/200, Train Loss: 0.4440, Val Loss: 0.4481\n",
      "Epoch 166/200, Train Loss: 0.4230, Val Loss: 0.4407\n",
      "Epoch 167/200, Train Loss: 0.4060, Val Loss: 0.4288\n",
      "Epoch 168/200, Train Loss: 0.4248, Val Loss: 0.4732\n",
      "Epoch 169/200, Train Loss: 0.4142, Val Loss: 0.4559\n",
      "Epoch 170/200, Train Loss: 0.4119, Val Loss: 0.4270\n",
      "Epoch 171/200, Train Loss: 0.3974, Val Loss: 0.4194\n",
      "  New best validation loss: 0.4194\n",
      "Epoch 172/200, Train Loss: 0.3897, Val Loss: 0.4610\n",
      "Epoch 173/200, Train Loss: 0.4087, Val Loss: 0.4532\n",
      "Epoch 174/200, Train Loss: 0.4040, Val Loss: 0.4126\n",
      "  New best validation loss: 0.4126\n",
      "Epoch 175/200, Train Loss: 0.4083, Val Loss: 0.4590\n",
      "Epoch 176/200, Train Loss: 0.3927, Val Loss: 0.4011\n",
      "  New best validation loss: 0.4011\n",
      "Epoch 177/200, Train Loss: 0.4266, Val Loss: 0.5000\n",
      "Epoch 178/200, Train Loss: 0.4129, Val Loss: 0.4190\n",
      "Epoch 179/200, Train Loss: 0.4025, Val Loss: 0.4242\n",
      "Epoch 180/200, Train Loss: 0.3924, Val Loss: 0.4292\n",
      "Epoch 181/200, Train Loss: 0.4061, Val Loss: 0.4292\n",
      "Epoch 182/200, Train Loss: 0.4252, Val Loss: 0.4048\n",
      "Epoch 183/200, Train Loss: 0.4317, Val Loss: 0.4946\n",
      "Epoch 184/200, Train Loss: 0.4290, Val Loss: 0.4959\n",
      "Epoch 185/200, Train Loss: 0.4016, Val Loss: 0.3867\n",
      "  New best validation loss: 0.3867\n",
      "Epoch 186/200, Train Loss: 0.3850, Val Loss: 0.4191\n",
      "Epoch 187/200, Train Loss: 0.3985, Val Loss: 0.4367\n",
      "Epoch 188/200, Train Loss: 0.4103, Val Loss: 0.3973\n",
      "Epoch 189/200, Train Loss: 0.3843, Val Loss: 0.3859\n",
      "  New best validation loss: 0.3859\n",
      "Epoch 190/200, Train Loss: 0.3945, Val Loss: 0.4598\n",
      "Epoch 191/200, Train Loss: 0.3907, Val Loss: 0.4627\n",
      "Epoch 192/200, Train Loss: 0.3931, Val Loss: 0.3829\n",
      "  New best validation loss: 0.3829\n",
      "Epoch 193/200, Train Loss: 0.3924, Val Loss: 0.4065\n",
      "Epoch 194/200, Train Loss: 0.3824, Val Loss: 0.4296\n",
      "Epoch 195/200, Train Loss: 0.3975, Val Loss: 0.3828\n",
      "  New best validation loss: 0.3828\n",
      "Epoch 196/200, Train Loss: 0.3902, Val Loss: 0.4132\n",
      "Epoch 197/200, Train Loss: 0.3614, Val Loss: 0.3741\n",
      "  New best validation loss: 0.3741\n",
      "Epoch 198/200, Train Loss: 0.3639, Val Loss: 0.4196\n",
      "Epoch 199/200, Train Loss: 0.3874, Val Loss: 0.3754\n",
      "Epoch 200/200, Train Loss: 0.3775, Val Loss: 0.4413\n",
      "\n",
      "Loaded best model (Val Loss: 0.3741) for final hidden state extraction.\n",
      "Saved best model for HIPPORNN_LegT to results/20250508_082527/HIPPORNN_LegT_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for HIPPORNN_LegT ---\n",
      "  Analyzing decodability for HIPPORNN_LegT...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for HIPPORNN_LegT - Test MSE: 0.0044, Test R2: 0.9979 (best alpha: 0.0162)\n",
      "Decodability (R2 score) for HIPPORNN_LegT: 0.9979\n",
      "\n",
      "--- Training NMRNN_Spatial_ModReadout ---\n",
      "Number of parameters: 66832\n",
      "Epoch 1/200, Train Loss: 4.8398, Val Loss: 5.2257\n",
      "  New best validation loss: 5.2257\n",
      "Epoch 2/200, Train Loss: 4.7008, Val Loss: 4.9290\n",
      "  New best validation loss: 4.9290\n",
      "Epoch 3/200, Train Loss: 4.6273, Val Loss: 4.7932\n",
      "  New best validation loss: 4.7932\n",
      "Epoch 4/200, Train Loss: 4.5279, Val Loss: 4.7123\n",
      "  New best validation loss: 4.7123\n",
      "Epoch 5/200, Train Loss: 4.4619, Val Loss: 4.6278\n",
      "  New best validation loss: 4.6278\n",
      "Epoch 6/200, Train Loss: 4.3261, Val Loss: 4.4514\n",
      "  New best validation loss: 4.4514\n",
      "Epoch 7/200, Train Loss: 4.1500, Val Loss: 4.2777\n",
      "  New best validation loss: 4.2777\n",
      "Epoch 8/200, Train Loss: 3.9685, Val Loss: 4.2263\n",
      "  New best validation loss: 4.2263\n",
      "Epoch 9/200, Train Loss: 3.8286, Val Loss: 4.1152\n",
      "  New best validation loss: 4.1152\n",
      "Epoch 10/200, Train Loss: 3.6924, Val Loss: 4.0171\n",
      "  New best validation loss: 4.0171\n",
      "Epoch 11/200, Train Loss: 3.6062, Val Loss: 3.8555\n",
      "  New best validation loss: 3.8555\n",
      "Epoch 12/200, Train Loss: 3.5600, Val Loss: 3.7416\n",
      "  New best validation loss: 3.7416\n",
      "Epoch 13/200, Train Loss: 3.3967, Val Loss: 3.6800\n",
      "  New best validation loss: 3.6800\n",
      "Epoch 14/200, Train Loss: 3.3378, Val Loss: 3.5075\n",
      "  New best validation loss: 3.5075\n",
      "Epoch 15/200, Train Loss: 3.1724, Val Loss: 3.4072\n",
      "  New best validation loss: 3.4072\n",
      "Epoch 16/200, Train Loss: 3.0981, Val Loss: 3.2661\n",
      "  New best validation loss: 3.2661\n",
      "Epoch 17/200, Train Loss: 2.9801, Val Loss: 3.3932\n",
      "Epoch 18/200, Train Loss: 2.9085, Val Loss: 3.2711\n",
      "Epoch 19/200, Train Loss: 2.9816, Val Loss: 3.3921\n",
      "Epoch 20/200, Train Loss: 2.9412, Val Loss: 3.1654\n",
      "  New best validation loss: 3.1654\n",
      "Epoch 21/200, Train Loss: 2.7590, Val Loss: 3.1803\n",
      "Epoch 22/200, Train Loss: 2.8214, Val Loss: 3.0417\n",
      "  New best validation loss: 3.0417\n",
      "Epoch 23/200, Train Loss: 2.6808, Val Loss: 2.9486\n",
      "  New best validation loss: 2.9486\n",
      "Epoch 24/200, Train Loss: 2.6283, Val Loss: 2.8970\n",
      "  New best validation loss: 2.8970\n",
      "Epoch 25/200, Train Loss: 2.5991, Val Loss: 2.8689\n",
      "  New best validation loss: 2.8689\n",
      "Epoch 26/200, Train Loss: 2.5347, Val Loss: 2.8122\n",
      "  New best validation loss: 2.8122\n",
      "Epoch 27/200, Train Loss: 2.6610, Val Loss: 3.0824\n",
      "Epoch 28/200, Train Loss: 2.5374, Val Loss: 2.7176\n",
      "  New best validation loss: 2.7176\n",
      "Epoch 29/200, Train Loss: 2.4867, Val Loss: 2.7296\n",
      "Epoch 30/200, Train Loss: 2.4578, Val Loss: 2.7211\n",
      "Epoch 31/200, Train Loss: 2.4000, Val Loss: 2.6019\n",
      "  New best validation loss: 2.6019\n",
      "Epoch 32/200, Train Loss: 2.3842, Val Loss: 2.5396\n",
      "  New best validation loss: 2.5396\n",
      "Epoch 33/200, Train Loss: 2.3359, Val Loss: 2.6283\n",
      "Epoch 34/200, Train Loss: 2.3219, Val Loss: 2.6559\n",
      "Epoch 35/200, Train Loss: 2.3256, Val Loss: 2.5792\n",
      "Epoch 36/200, Train Loss: 2.3052, Val Loss: 2.4982\n",
      "  New best validation loss: 2.4982\n",
      "Epoch 37/200, Train Loss: 2.2771, Val Loss: 2.4855\n",
      "  New best validation loss: 2.4855\n",
      "Epoch 38/200, Train Loss: 2.2000, Val Loss: 2.3938\n",
      "  New best validation loss: 2.3938\n",
      "Epoch 39/200, Train Loss: 2.1532, Val Loss: 2.3707\n",
      "  New best validation loss: 2.3707\n",
      "Epoch 40/200, Train Loss: 2.1630, Val Loss: 2.5207\n",
      "Epoch 41/200, Train Loss: 2.2278, Val Loss: 2.4800\n",
      "Epoch 42/200, Train Loss: 2.1565, Val Loss: 2.3140\n",
      "  New best validation loss: 2.3140\n",
      "Epoch 43/200, Train Loss: 2.1229, Val Loss: 2.3052\n",
      "  New best validation loss: 2.3052\n",
      "Epoch 44/200, Train Loss: 2.0625, Val Loss: 2.2989\n",
      "  New best validation loss: 2.2989\n",
      "Epoch 45/200, Train Loss: 2.1308, Val Loss: 2.6235\n",
      "Epoch 46/200, Train Loss: 2.2008, Val Loss: 2.4152\n",
      "Epoch 47/200, Train Loss: 2.0663, Val Loss: 2.2482\n",
      "  New best validation loss: 2.2482\n",
      "Epoch 48/200, Train Loss: 2.0063, Val Loss: 2.2478\n",
      "  New best validation loss: 2.2478\n",
      "Epoch 49/200, Train Loss: 1.9934, Val Loss: 2.2859\n",
      "Epoch 50/200, Train Loss: 2.0421, Val Loss: 2.1952\n",
      "  New best validation loss: 2.1952\n",
      "Epoch 51/200, Train Loss: 1.9606, Val Loss: 2.1617\n",
      "  New best validation loss: 2.1617\n",
      "Epoch 52/200, Train Loss: 1.9478, Val Loss: 2.1287\n",
      "  New best validation loss: 2.1287\n",
      "Epoch 53/200, Train Loss: 1.9338, Val Loss: 2.1247\n",
      "  New best validation loss: 2.1247\n",
      "Epoch 54/200, Train Loss: 1.9202, Val Loss: 2.1265\n",
      "Epoch 55/200, Train Loss: 1.8899, Val Loss: 2.0856\n",
      "  New best validation loss: 2.0856\n",
      "Epoch 56/200, Train Loss: 1.8527, Val Loss: 2.0469\n",
      "  New best validation loss: 2.0469\n",
      "Epoch 57/200, Train Loss: 1.8485, Val Loss: 2.1156\n",
      "Epoch 58/200, Train Loss: 1.8673, Val Loss: 1.9921\n",
      "  New best validation loss: 1.9921\n",
      "Epoch 59/200, Train Loss: 1.8489, Val Loss: 2.1486\n",
      "Epoch 60/200, Train Loss: 1.8687, Val Loss: 1.9288\n",
      "  New best validation loss: 1.9288\n",
      "Epoch 61/200, Train Loss: 1.8293, Val Loss: 1.9303\n",
      "Epoch 62/200, Train Loss: 1.8052, Val Loss: 2.0179\n",
      "Epoch 63/200, Train Loss: 1.7918, Val Loss: 1.9721\n",
      "Epoch 64/200, Train Loss: 1.8061, Val Loss: 1.9987\n",
      "Epoch 65/200, Train Loss: 1.7826, Val Loss: 1.9062\n",
      "  New best validation loss: 1.9062\n",
      "Epoch 66/200, Train Loss: 1.7455, Val Loss: 1.9503\n",
      "Epoch 67/200, Train Loss: 1.7478, Val Loss: 1.8749\n",
      "  New best validation loss: 1.8749\n",
      "Epoch 68/200, Train Loss: 1.7671, Val Loss: 2.1801\n",
      "Epoch 69/200, Train Loss: 1.7678, Val Loss: 1.8729\n",
      "  New best validation loss: 1.8729\n",
      "Epoch 70/200, Train Loss: 1.7321, Val Loss: 1.8812\n",
      "Epoch 71/200, Train Loss: 1.7575, Val Loss: 2.0063\n",
      "Epoch 72/200, Train Loss: 1.8267, Val Loss: 1.9652\n",
      "Epoch 73/200, Train Loss: 1.7756, Val Loss: 1.9522\n",
      "Epoch 74/200, Train Loss: 1.7086, Val Loss: 1.8476\n",
      "  New best validation loss: 1.8476\n",
      "Epoch 75/200, Train Loss: 1.7134, Val Loss: 1.8564\n",
      "Epoch 76/200, Train Loss: 1.6993, Val Loss: 1.8344\n",
      "  New best validation loss: 1.8344\n",
      "Epoch 77/200, Train Loss: 1.6812, Val Loss: 1.8534\n",
      "Epoch 78/200, Train Loss: 1.6709, Val Loss: 1.8139\n",
      "  New best validation loss: 1.8139\n",
      "Epoch 79/200, Train Loss: 1.7179, Val Loss: 1.9602\n",
      "Epoch 80/200, Train Loss: 1.7152, Val Loss: 1.7860\n",
      "  New best validation loss: 1.7860\n",
      "Epoch 81/200, Train Loss: 1.6592, Val Loss: 1.7626\n",
      "  New best validation loss: 1.7626\n",
      "Epoch 82/200, Train Loss: 1.6301, Val Loss: 1.9416\n",
      "Epoch 83/200, Train Loss: 1.6791, Val Loss: 1.8159\n",
      "Epoch 84/200, Train Loss: 1.6503, Val Loss: 1.7694\n",
      "Epoch 85/200, Train Loss: 1.6389, Val Loss: 1.7380\n",
      "  New best validation loss: 1.7380\n",
      "Epoch 86/200, Train Loss: 1.6518, Val Loss: 1.7503\n",
      "Epoch 87/200, Train Loss: 1.7295, Val Loss: 1.7845\n",
      "Epoch 88/200, Train Loss: 1.6487, Val Loss: 1.7673\n",
      "Epoch 89/200, Train Loss: 1.6294, Val Loss: 1.7605\n",
      "Epoch 90/200, Train Loss: 1.6005, Val Loss: 1.7106\n",
      "  New best validation loss: 1.7106\n",
      "Epoch 91/200, Train Loss: 1.5872, Val Loss: 1.8022\n",
      "Epoch 92/200, Train Loss: 1.7280, Val Loss: 1.7314\n",
      "Epoch 93/200, Train Loss: 1.6665, Val Loss: 1.7524\n",
      "Epoch 94/200, Train Loss: 1.6313, Val Loss: 1.8349\n",
      "Epoch 95/200, Train Loss: 1.6717, Val Loss: 1.7208\n",
      "Epoch 96/200, Train Loss: 1.5784, Val Loss: 1.7083\n",
      "  New best validation loss: 1.7083\n",
      "Epoch 97/200, Train Loss: 1.5737, Val Loss: 1.7838\n",
      "Epoch 98/200, Train Loss: 1.6188, Val Loss: 1.7421\n",
      "Epoch 99/200, Train Loss: 1.5910, Val Loss: 1.7167\n",
      "Epoch 100/200, Train Loss: 1.5804, Val Loss: 1.7007\n",
      "  New best validation loss: 1.7007\n",
      "Epoch 101/200, Train Loss: 1.5631, Val Loss: 1.7516\n",
      "Epoch 102/200, Train Loss: 1.6455, Val Loss: 1.8630\n",
      "Epoch 103/200, Train Loss: 1.6555, Val Loss: 1.7050\n",
      "Epoch 104/200, Train Loss: 1.5992, Val Loss: 1.9053\n",
      "Epoch 105/200, Train Loss: 1.6337, Val Loss: 1.6722\n",
      "  New best validation loss: 1.6722\n",
      "Epoch 106/200, Train Loss: 1.5428, Val Loss: 1.6320\n",
      "  New best validation loss: 1.6320\n",
      "Epoch 107/200, Train Loss: 1.5480, Val Loss: 1.6922\n",
      "Epoch 108/200, Train Loss: 1.5826, Val Loss: 1.7265\n",
      "Epoch 109/200, Train Loss: 1.5403, Val Loss: 1.6749\n",
      "Epoch 110/200, Train Loss: 1.5644, Val Loss: 1.7934\n",
      "Epoch 111/200, Train Loss: 1.5373, Val Loss: 1.6409\n",
      "Epoch 112/200, Train Loss: 1.5081, Val Loss: 1.6560\n",
      "Epoch 113/200, Train Loss: 1.5130, Val Loss: 1.6443\n",
      "Epoch 114/200, Train Loss: 1.4806, Val Loss: 1.6373\n",
      "Epoch 115/200, Train Loss: 1.5054, Val Loss: 1.6756\n",
      "Epoch 116/200, Train Loss: 1.5033, Val Loss: 1.6345\n",
      "Epoch 117/200, Train Loss: 1.4993, Val Loss: 1.6920\n",
      "Epoch 118/200, Train Loss: 1.5178, Val Loss: 1.6923\n",
      "Epoch 119/200, Train Loss: 1.4940, Val Loss: 1.6113\n",
      "  New best validation loss: 1.6113\n",
      "Epoch 120/200, Train Loss: 1.5062, Val Loss: 1.6353\n",
      "Epoch 121/200, Train Loss: 1.5136, Val Loss: 1.6576\n",
      "Epoch 122/200, Train Loss: 1.4981, Val Loss: 1.6416\n",
      "Epoch 123/200, Train Loss: 1.4741, Val Loss: 1.6201\n",
      "Epoch 124/200, Train Loss: 1.4583, Val Loss: 1.6113\n",
      "  New best validation loss: 1.6113\n",
      "Epoch 125/200, Train Loss: 1.5213, Val Loss: 1.6576\n",
      "Epoch 126/200, Train Loss: 1.4670, Val Loss: 1.6629\n",
      "Epoch 127/200, Train Loss: 1.4836, Val Loss: 1.5925\n",
      "  New best validation loss: 1.5925\n",
      "Epoch 128/200, Train Loss: 1.4851, Val Loss: 1.5845\n",
      "  New best validation loss: 1.5845\n",
      "Epoch 129/200, Train Loss: 1.4772, Val Loss: 1.6331\n",
      "Epoch 130/200, Train Loss: 1.4751, Val Loss: 1.6114\n",
      "Epoch 131/200, Train Loss: 1.4562, Val Loss: 1.6264\n",
      "Epoch 132/200, Train Loss: 1.4908, Val Loss: 1.6242\n",
      "Epoch 133/200, Train Loss: 1.4808, Val Loss: 1.5726\n",
      "  New best validation loss: 1.5726\n",
      "Epoch 134/200, Train Loss: 1.4231, Val Loss: 1.6016\n",
      "Epoch 135/200, Train Loss: 1.4364, Val Loss: 1.5995\n",
      "Epoch 136/200, Train Loss: 1.4372, Val Loss: 1.5740\n",
      "Epoch 137/200, Train Loss: 1.4656, Val Loss: 1.6121\n",
      "Epoch 138/200, Train Loss: 1.4628, Val Loss: 1.5488\n",
      "  New best validation loss: 1.5488\n",
      "Epoch 139/200, Train Loss: 1.4413, Val Loss: 1.5687\n",
      "Epoch 140/200, Train Loss: 1.4960, Val Loss: 1.6483\n",
      "Epoch 141/200, Train Loss: 1.4341, Val Loss: 1.7306\n",
      "Epoch 142/200, Train Loss: 1.5171, Val Loss: 1.8008\n",
      "Epoch 143/200, Train Loss: 1.5163, Val Loss: 1.5809\n",
      "Epoch 144/200, Train Loss: 1.4533, Val Loss: 1.5835\n",
      "Epoch 145/200, Train Loss: 1.4167, Val Loss: 1.5834\n",
      "Epoch 146/200, Train Loss: 1.4369, Val Loss: 1.6235\n",
      "Epoch 147/200, Train Loss: 1.4427, Val Loss: 1.6005\n",
      "Epoch 148/200, Train Loss: 1.4086, Val Loss: 1.5481\n",
      "  New best validation loss: 1.5481\n",
      "Epoch 149/200, Train Loss: 1.4170, Val Loss: 1.5554\n",
      "Epoch 150/200, Train Loss: 1.3982, Val Loss: 1.5312\n",
      "  New best validation loss: 1.5312\n",
      "Epoch 151/200, Train Loss: 1.3800, Val Loss: 1.5744\n",
      "Epoch 152/200, Train Loss: 1.3960, Val Loss: 1.6006\n",
      "Epoch 153/200, Train Loss: 1.3919, Val Loss: 1.5487\n",
      "Epoch 154/200, Train Loss: 1.4057, Val Loss: 1.6209\n",
      "Epoch 155/200, Train Loss: 1.4160, Val Loss: 1.5476\n",
      "Epoch 156/200, Train Loss: 1.3818, Val Loss: 1.5107\n",
      "  New best validation loss: 1.5107\n",
      "Epoch 157/200, Train Loss: 1.3695, Val Loss: 1.5503\n",
      "Epoch 158/200, Train Loss: 1.4191, Val Loss: 1.5727\n",
      "Epoch 159/200, Train Loss: 1.4037, Val Loss: 1.5356\n",
      "Epoch 160/200, Train Loss: 1.3826, Val Loss: 1.5062\n",
      "  New best validation loss: 1.5062\n",
      "Epoch 161/200, Train Loss: 1.3834, Val Loss: 1.5473\n",
      "Epoch 162/200, Train Loss: 1.3679, Val Loss: 1.5140\n",
      "Epoch 163/200, Train Loss: 1.3505, Val Loss: 1.5530\n",
      "Epoch 164/200, Train Loss: 1.3915, Val Loss: 1.5167\n",
      "Epoch 165/200, Train Loss: 1.3725, Val Loss: 1.5760\n",
      "Epoch 166/200, Train Loss: 1.3687, Val Loss: 1.4687\n",
      "  New best validation loss: 1.4687\n",
      "Epoch 167/200, Train Loss: 1.3720, Val Loss: 1.5820\n",
      "Epoch 168/200, Train Loss: 1.3975, Val Loss: 1.5468\n",
      "Epoch 169/200, Train Loss: 1.3629, Val Loss: 1.5134\n",
      "Epoch 170/200, Train Loss: 1.3392, Val Loss: 1.4802\n",
      "Epoch 171/200, Train Loss: 1.3441, Val Loss: 1.5373\n",
      "Epoch 172/200, Train Loss: 1.3355, Val Loss: 1.5115\n",
      "Epoch 173/200, Train Loss: 1.3505, Val Loss: 1.5235\n",
      "Epoch 174/200, Train Loss: 1.3362, Val Loss: 1.4799\n",
      "Epoch 175/200, Train Loss: 1.3227, Val Loss: 1.4785\n",
      "Epoch 176/200, Train Loss: 1.3156, Val Loss: 1.4740\n",
      "Epoch 177/200, Train Loss: 1.3175, Val Loss: 1.4675\n",
      "  New best validation loss: 1.4675\n",
      "Epoch 178/200, Train Loss: 1.3071, Val Loss: 1.4809\n",
      "Epoch 179/200, Train Loss: 1.3465, Val Loss: 1.5688\n",
      "Epoch 180/200, Train Loss: 1.3652, Val Loss: 1.5342\n",
      "Epoch 181/200, Train Loss: 1.3480, Val Loss: 1.4974\n",
      "Epoch 182/200, Train Loss: 1.3333, Val Loss: 1.4947\n",
      "Epoch 183/200, Train Loss: 1.3348, Val Loss: 1.4738\n",
      "Epoch 184/200, Train Loss: 1.3271, Val Loss: 1.4924\n",
      "Epoch 185/200, Train Loss: 1.2988, Val Loss: 1.5031\n",
      "Epoch 186/200, Train Loss: 1.3163, Val Loss: 1.5025\n",
      "Epoch 187/200, Train Loss: 1.3265, Val Loss: 1.4839\n",
      "Epoch 188/200, Train Loss: 1.3022, Val Loss: 1.4431\n",
      "  New best validation loss: 1.4431\n",
      "Epoch 189/200, Train Loss: 1.3130, Val Loss: 1.5032\n",
      "Epoch 190/200, Train Loss: 1.3285, Val Loss: 1.5265\n",
      "Epoch 191/200, Train Loss: 1.3356, Val Loss: 1.5007\n",
      "Epoch 192/200, Train Loss: 1.3172, Val Loss: 1.4904\n",
      "Epoch 193/200, Train Loss: 1.3269, Val Loss: 1.4495\n",
      "Epoch 194/200, Train Loss: 1.2955, Val Loss: 1.4627\n",
      "Epoch 195/200, Train Loss: 1.2818, Val Loss: 1.4610\n",
      "Epoch 196/200, Train Loss: 1.2751, Val Loss: 1.4599\n",
      "Epoch 197/200, Train Loss: 1.2862, Val Loss: 1.4885\n",
      "Epoch 198/200, Train Loss: 1.3247, Val Loss: 1.5257\n",
      "Epoch 199/200, Train Loss: 1.3111, Val Loss: 1.4659\n",
      "Epoch 200/200, Train Loss: 1.2760, Val Loss: 1.4988\n",
      "\n",
      "Loaded best model (Val Loss: 1.4431) for final hidden state extraction.\n",
      "Saved best model for NMRNN_Spatial_ModReadout to results/20250508_082527/NMRNN_Spatial_ModReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_Spatial_ModReadout ---\n",
      "  Analyzing decodability for NMRNN_Spatial_ModReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_Spatial_ModReadout - Test MSE: 0.0548, Test R2: 0.9732 (best alpha: 2.6367)\n",
      "Decodability (R2 score) for NMRNN_Spatial_ModReadout: 0.9732\n",
      "\n",
      "--- Training NMRNN_NoSpatial_ModReadout ---\n",
      "Number of parameters: 66832\n",
      "Epoch 1/200, Train Loss: 4.8267, Val Loss: 5.2125\n",
      "  New best validation loss: 5.2125\n",
      "Epoch 2/200, Train Loss: 4.7041, Val Loss: 4.9662\n",
      "  New best validation loss: 4.9662\n",
      "Epoch 3/200, Train Loss: 4.6626, Val Loss: 4.9032\n",
      "  New best validation loss: 4.9032\n",
      "Epoch 4/200, Train Loss: 4.5697, Val Loss: 4.8280\n",
      "  New best validation loss: 4.8280\n",
      "Epoch 5/200, Train Loss: 4.4826, Val Loss: 4.6918\n",
      "  New best validation loss: 4.6918\n",
      "Epoch 6/200, Train Loss: 4.3409, Val Loss: 4.5045\n",
      "  New best validation loss: 4.5045\n",
      "Epoch 7/200, Train Loss: 4.1676, Val Loss: 4.3979\n",
      "  New best validation loss: 4.3979\n",
      "Epoch 8/200, Train Loss: 4.0104, Val Loss: 4.1805\n",
      "  New best validation loss: 4.1805\n",
      "Epoch 9/200, Train Loss: 3.8999, Val Loss: 4.0638\n",
      "  New best validation loss: 4.0638\n",
      "Epoch 10/200, Train Loss: 3.7903, Val Loss: 4.0798\n",
      "Epoch 11/200, Train Loss: 3.7034, Val Loss: 3.9974\n",
      "  New best validation loss: 3.9974\n",
      "Epoch 12/200, Train Loss: 3.6387, Val Loss: 3.8468\n",
      "  New best validation loss: 3.8468\n",
      "Epoch 13/200, Train Loss: 3.4984, Val Loss: 3.7450\n",
      "  New best validation loss: 3.7450\n",
      "Epoch 14/200, Train Loss: 3.4129, Val Loss: 3.6062\n",
      "  New best validation loss: 3.6062\n",
      "Epoch 15/200, Train Loss: 3.2908, Val Loss: 3.5463\n",
      "  New best validation loss: 3.5463\n",
      "Epoch 16/200, Train Loss: 3.2439, Val Loss: 3.5358\n",
      "  New best validation loss: 3.5358\n",
      "Epoch 17/200, Train Loss: 3.1880, Val Loss: 3.4053\n",
      "  New best validation loss: 3.4053\n",
      "Epoch 18/200, Train Loss: 3.0861, Val Loss: 3.2678\n",
      "  New best validation loss: 3.2678\n",
      "Epoch 19/200, Train Loss: 2.9972, Val Loss: 3.1983\n",
      "  New best validation loss: 3.1983\n",
      "Epoch 20/200, Train Loss: 2.9295, Val Loss: 3.1810\n",
      "  New best validation loss: 3.1810\n",
      "Epoch 21/200, Train Loss: 2.8997, Val Loss: 3.1401\n",
      "  New best validation loss: 3.1401\n",
      "Epoch 22/200, Train Loss: 2.8964, Val Loss: 3.0706\n",
      "  New best validation loss: 3.0706\n",
      "Epoch 23/200, Train Loss: 2.9179, Val Loss: 3.0736\n",
      "Epoch 24/200, Train Loss: 2.8249, Val Loss: 3.0855\n",
      "Epoch 25/200, Train Loss: 2.7359, Val Loss: 3.1240\n",
      "Epoch 26/200, Train Loss: 2.6993, Val Loss: 3.0510\n",
      "  New best validation loss: 3.0510\n",
      "Epoch 27/200, Train Loss: 2.6683, Val Loss: 2.9562\n",
      "  New best validation loss: 2.9562\n",
      "Epoch 28/200, Train Loss: 2.6608, Val Loss: 3.1401\n",
      "Epoch 29/200, Train Loss: 2.6783, Val Loss: 2.8176\n",
      "  New best validation loss: 2.8176\n",
      "Epoch 30/200, Train Loss: 2.6020, Val Loss: 2.8533\n",
      "Epoch 31/200, Train Loss: 2.5469, Val Loss: 2.7304\n",
      "  New best validation loss: 2.7304\n",
      "Epoch 32/200, Train Loss: 2.5125, Val Loss: 2.7353\n",
      "Epoch 33/200, Train Loss: 2.4930, Val Loss: 2.6929\n",
      "  New best validation loss: 2.6929\n",
      "Epoch 34/200, Train Loss: 2.4669, Val Loss: 2.7830\n",
      "Epoch 35/200, Train Loss: 2.5092, Val Loss: 2.7144\n",
      "Epoch 36/200, Train Loss: 2.4461, Val Loss: 2.6226\n",
      "  New best validation loss: 2.6226\n",
      "Epoch 37/200, Train Loss: 2.3906, Val Loss: 2.5408\n",
      "  New best validation loss: 2.5408\n",
      "Epoch 38/200, Train Loss: 2.3557, Val Loss: 2.4888\n",
      "  New best validation loss: 2.4888\n",
      "Epoch 39/200, Train Loss: 2.3327, Val Loss: 2.5372\n",
      "Epoch 40/200, Train Loss: 2.3011, Val Loss: 2.4979\n",
      "Epoch 41/200, Train Loss: 2.2865, Val Loss: 2.4765\n",
      "  New best validation loss: 2.4765\n",
      "Epoch 42/200, Train Loss: 2.2984, Val Loss: 2.4458\n",
      "  New best validation loss: 2.4458\n",
      "Epoch 43/200, Train Loss: 2.2300, Val Loss: 2.3694\n",
      "  New best validation loss: 2.3694\n",
      "Epoch 44/200, Train Loss: 2.1926, Val Loss: 2.4123\n",
      "Epoch 45/200, Train Loss: 2.1958, Val Loss: 2.3953\n",
      "Epoch 46/200, Train Loss: 2.1730, Val Loss: 2.2828\n",
      "  New best validation loss: 2.2828\n",
      "Epoch 47/200, Train Loss: 2.0773, Val Loss: 2.2421\n",
      "  New best validation loss: 2.2421\n",
      "Epoch 48/200, Train Loss: 2.0683, Val Loss: 2.1687\n",
      "  New best validation loss: 2.1687\n",
      "Epoch 49/200, Train Loss: 2.0580, Val Loss: 2.3003\n",
      "Epoch 50/200, Train Loss: 2.0867, Val Loss: 2.3182\n",
      "Epoch 51/200, Train Loss: 2.0700, Val Loss: 2.4051\n",
      "Epoch 52/200, Train Loss: 2.1885, Val Loss: 2.3104\n",
      "Epoch 53/200, Train Loss: 2.1220, Val Loss: 2.3355\n",
      "Epoch 54/200, Train Loss: 2.2727, Val Loss: 2.6434\n",
      "Epoch 55/200, Train Loss: 2.1478, Val Loss: 2.2379\n",
      "Epoch 56/200, Train Loss: 2.0517, Val Loss: 2.1791\n",
      "Epoch 57/200, Train Loss: 1.9829, Val Loss: 2.2069\n",
      "Epoch 58/200, Train Loss: 1.9765, Val Loss: 2.1023\n",
      "  New best validation loss: 2.1023\n",
      "Epoch 59/200, Train Loss: 1.9110, Val Loss: 2.1569\n",
      "Epoch 60/200, Train Loss: 1.8960, Val Loss: 2.0127\n",
      "  New best validation loss: 2.0127\n",
      "Epoch 61/200, Train Loss: 1.8746, Val Loss: 2.1610\n",
      "Epoch 62/200, Train Loss: 1.8801, Val Loss: 2.1061\n",
      "Epoch 63/200, Train Loss: 1.9901, Val Loss: 1.9933\n",
      "  New best validation loss: 1.9933\n",
      "Epoch 64/200, Train Loss: 1.8266, Val Loss: 2.0322\n",
      "Epoch 65/200, Train Loss: 1.8318, Val Loss: 1.9380\n",
      "  New best validation loss: 1.9380\n",
      "Epoch 66/200, Train Loss: 1.8421, Val Loss: 1.9855\n",
      "Epoch 67/200, Train Loss: 1.8041, Val Loss: 1.9111\n",
      "  New best validation loss: 1.9111\n",
      "Epoch 68/200, Train Loss: 1.7745, Val Loss: 1.9924\n",
      "Epoch 69/200, Train Loss: 1.8154, Val Loss: 1.9323\n",
      "Epoch 70/200, Train Loss: 1.9112, Val Loss: 2.1050\n",
      "Epoch 71/200, Train Loss: 1.8538, Val Loss: 1.9394\n",
      "Epoch 72/200, Train Loss: 1.7685, Val Loss: 1.9091\n",
      "  New best validation loss: 1.9091\n",
      "Epoch 73/200, Train Loss: 1.7277, Val Loss: 1.8609\n",
      "  New best validation loss: 1.8609\n",
      "Epoch 74/200, Train Loss: 1.7704, Val Loss: 1.9405\n",
      "Epoch 75/200, Train Loss: 1.7431, Val Loss: 1.9046\n",
      "Epoch 76/200, Train Loss: 1.7173, Val Loss: 1.8449\n",
      "  New best validation loss: 1.8449\n",
      "Epoch 77/200, Train Loss: 1.7406, Val Loss: 2.1001\n",
      "Epoch 78/200, Train Loss: 1.7596, Val Loss: 1.8327\n",
      "  New best validation loss: 1.8327\n",
      "Epoch 79/200, Train Loss: 1.7648, Val Loss: 1.9998\n",
      "Epoch 80/200, Train Loss: 1.7784, Val Loss: 1.8280\n",
      "  New best validation loss: 1.8280\n",
      "Epoch 81/200, Train Loss: 1.7067, Val Loss: 1.8155\n",
      "  New best validation loss: 1.8155\n",
      "Epoch 82/200, Train Loss: 1.6890, Val Loss: 1.7800\n",
      "  New best validation loss: 1.7800\n",
      "Epoch 83/200, Train Loss: 1.6798, Val Loss: 1.7863\n",
      "Epoch 84/200, Train Loss: 1.7027, Val Loss: 2.0515\n",
      "Epoch 85/200, Train Loss: 1.7786, Val Loss: 1.8232\n",
      "Epoch 86/200, Train Loss: 1.6914, Val Loss: 1.7893\n",
      "Epoch 87/200, Train Loss: 1.6821, Val Loss: 1.7748\n",
      "  New best validation loss: 1.7748\n",
      "Epoch 88/200, Train Loss: 1.6422, Val Loss: 1.8324\n",
      "Epoch 89/200, Train Loss: 1.6921, Val Loss: 1.7966\n",
      "Epoch 90/200, Train Loss: 1.6516, Val Loss: 1.7208\n",
      "  New best validation loss: 1.7208\n",
      "Epoch 91/200, Train Loss: 1.6416, Val Loss: 1.8293\n",
      "Epoch 92/200, Train Loss: 1.6772, Val Loss: 1.7341\n",
      "Epoch 93/200, Train Loss: 1.6320, Val Loss: 1.7352\n",
      "Epoch 94/200, Train Loss: 1.6819, Val Loss: 1.8107\n",
      "Epoch 95/200, Train Loss: 1.6694, Val Loss: 1.8349\n",
      "Epoch 96/200, Train Loss: 1.7964, Val Loss: 2.1305\n",
      "Epoch 97/200, Train Loss: 1.7594, Val Loss: 1.7933\n",
      "Epoch 98/200, Train Loss: 1.6778, Val Loss: 1.7539\n",
      "Epoch 99/200, Train Loss: 1.7130, Val Loss: 1.7909\n",
      "Epoch 100/200, Train Loss: 1.6747, Val Loss: 1.9148\n",
      "Epoch 101/200, Train Loss: 1.6993, Val Loss: 1.7277\n",
      "Epoch 102/200, Train Loss: 1.6573, Val Loss: 1.7872\n",
      "Epoch 103/200, Train Loss: 1.6028, Val Loss: 1.6718\n",
      "  New best validation loss: 1.6718\n",
      "Epoch 104/200, Train Loss: 1.6079, Val Loss: 1.7453\n",
      "Epoch 105/200, Train Loss: 1.5777, Val Loss: 1.6489\n",
      "  New best validation loss: 1.6489\n",
      "Epoch 106/200, Train Loss: 1.5581, Val Loss: 1.6689\n",
      "Epoch 107/200, Train Loss: 1.5749, Val Loss: 1.7126\n",
      "Epoch 108/200, Train Loss: 1.5943, Val Loss: 1.6918\n",
      "Epoch 109/200, Train Loss: 1.5667, Val Loss: 1.6672\n",
      "Epoch 110/200, Train Loss: 1.5988, Val Loss: 1.7758\n",
      "Epoch 111/200, Train Loss: 1.6155, Val Loss: 1.8608\n",
      "Epoch 112/200, Train Loss: 1.6946, Val Loss: 1.7312\n",
      "Epoch 113/200, Train Loss: 1.6900, Val Loss: 1.8323\n",
      "Epoch 114/200, Train Loss: 1.6619, Val Loss: 1.9401\n",
      "Epoch 115/200, Train Loss: 1.6928, Val Loss: 1.7498\n",
      "Epoch 116/200, Train Loss: 1.6416, Val Loss: 1.7067\n",
      "Epoch 117/200, Train Loss: 1.6417, Val Loss: 1.7500\n",
      "Epoch 118/200, Train Loss: 1.6584, Val Loss: 1.7871\n",
      "Epoch 119/200, Train Loss: 1.5914, Val Loss: 1.6690\n",
      "Epoch 120/200, Train Loss: 1.5485, Val Loss: 1.6362\n",
      "  New best validation loss: 1.6362\n",
      "Epoch 121/200, Train Loss: 1.5318, Val Loss: 1.6468\n",
      "Epoch 122/200, Train Loss: 1.5292, Val Loss: 1.6118\n",
      "  New best validation loss: 1.6118\n",
      "Epoch 123/200, Train Loss: 1.5215, Val Loss: 1.6248\n",
      "Epoch 124/200, Train Loss: 1.5187, Val Loss: 1.6195\n",
      "Epoch 125/200, Train Loss: 1.5322, Val Loss: 1.6212\n",
      "Epoch 126/200, Train Loss: 1.5541, Val Loss: 1.6775\n",
      "Epoch 127/200, Train Loss: 1.5179, Val Loss: 1.6298\n",
      "Epoch 128/200, Train Loss: 1.5256, Val Loss: 1.6317\n",
      "Epoch 129/200, Train Loss: 1.5116, Val Loss: 1.6172\n",
      "Epoch 130/200, Train Loss: 1.5173, Val Loss: 1.6290\n",
      "Epoch 131/200, Train Loss: 1.5110, Val Loss: 1.6113\n",
      "  New best validation loss: 1.6113\n",
      "Epoch 132/200, Train Loss: 1.5700, Val Loss: 1.6472\n",
      "Epoch 133/200, Train Loss: 1.5295, Val Loss: 1.6167\n",
      "Epoch 134/200, Train Loss: 1.4840, Val Loss: 1.5823\n",
      "  New best validation loss: 1.5823\n",
      "Epoch 135/200, Train Loss: 1.5027, Val Loss: 1.5859\n",
      "Epoch 136/200, Train Loss: 1.5336, Val Loss: 1.6115\n",
      "Epoch 137/200, Train Loss: 1.5498, Val Loss: 1.6026\n",
      "Epoch 138/200, Train Loss: 1.5094, Val Loss: 1.5946\n",
      "Epoch 139/200, Train Loss: 1.5083, Val Loss: 1.6454\n",
      "Epoch 140/200, Train Loss: 1.5047, Val Loss: 1.6295\n",
      "Epoch 141/200, Train Loss: 1.4906, Val Loss: 1.7083\n",
      "Epoch 142/200, Train Loss: 1.5099, Val Loss: 1.6217\n",
      "Epoch 143/200, Train Loss: 1.5117, Val Loss: 1.6761\n",
      "Epoch 144/200, Train Loss: 1.5552, Val Loss: 1.6496\n",
      "Epoch 145/200, Train Loss: 1.5050, Val Loss: 1.6211\n",
      "Epoch 146/200, Train Loss: 1.5326, Val Loss: 1.6706\n",
      "Epoch 147/200, Train Loss: 1.4989, Val Loss: 1.6424\n",
      "Epoch 148/200, Train Loss: 1.5114, Val Loss: 1.5939\n",
      "Epoch 149/200, Train Loss: 1.4862, Val Loss: 1.6721\n",
      "Epoch 150/200, Train Loss: 1.5340, Val Loss: 1.7608\n",
      "Epoch 151/200, Train Loss: 1.5410, Val Loss: 1.5914\n",
      "Epoch 152/200, Train Loss: 1.4550, Val Loss: 1.5900\n",
      "Epoch 153/200, Train Loss: 1.4581, Val Loss: 1.5664\n",
      "  New best validation loss: 1.5664\n",
      "Epoch 154/200, Train Loss: 1.4517, Val Loss: 1.6547\n",
      "Epoch 155/200, Train Loss: 1.4838, Val Loss: 1.6381\n",
      "Epoch 156/200, Train Loss: 1.4894, Val Loss: 1.6587\n",
      "Epoch 157/200, Train Loss: 1.5620, Val Loss: 1.6130\n",
      "Epoch 158/200, Train Loss: 1.5082, Val Loss: 1.6194\n",
      "Epoch 159/200, Train Loss: 1.4608, Val Loss: 1.5464\n",
      "  New best validation loss: 1.5464\n",
      "Epoch 160/200, Train Loss: 1.4264, Val Loss: 1.5372\n",
      "  New best validation loss: 1.5372\n",
      "Epoch 161/200, Train Loss: 1.4383, Val Loss: 1.6428\n",
      "Epoch 162/200, Train Loss: 1.4527, Val Loss: 1.6493\n",
      "Epoch 163/200, Train Loss: 1.4663, Val Loss: 1.5555\n",
      "Epoch 164/200, Train Loss: 1.4205, Val Loss: 1.5430\n",
      "Epoch 165/200, Train Loss: 1.4134, Val Loss: 1.5557\n",
      "Epoch 166/200, Train Loss: 1.4169, Val Loss: 1.5257\n",
      "  New best validation loss: 1.5257\n",
      "Epoch 167/200, Train Loss: 1.4392, Val Loss: 1.5695\n",
      "Epoch 168/200, Train Loss: 1.4229, Val Loss: 1.5448\n",
      "Epoch 169/200, Train Loss: 1.4173, Val Loss: 1.5592\n",
      "Epoch 170/200, Train Loss: 1.4056, Val Loss: 1.5681\n",
      "Epoch 171/200, Train Loss: 1.4216, Val Loss: 1.5074\n",
      "  New best validation loss: 1.5074\n",
      "Epoch 172/200, Train Loss: 1.3987, Val Loss: 1.5923\n",
      "Epoch 173/200, Train Loss: 1.4045, Val Loss: 1.5526\n",
      "Epoch 174/200, Train Loss: 1.4091, Val Loss: 1.5625\n",
      "Epoch 175/200, Train Loss: 1.4306, Val Loss: 1.5841\n",
      "Epoch 176/200, Train Loss: 1.4046, Val Loss: 1.5148\n",
      "Epoch 177/200, Train Loss: 1.3972, Val Loss: 1.6275\n",
      "Epoch 178/200, Train Loss: 1.4814, Val Loss: 1.5986\n",
      "Epoch 179/200, Train Loss: 1.4357, Val Loss: 1.5267\n",
      "Epoch 180/200, Train Loss: 1.3911, Val Loss: 1.5557\n",
      "Epoch 181/200, Train Loss: 1.3697, Val Loss: 1.4909\n",
      "  New best validation loss: 1.4909\n",
      "Epoch 182/200, Train Loss: 1.4126, Val Loss: 1.6106\n",
      "Epoch 183/200, Train Loss: 1.4116, Val Loss: 1.5858\n",
      "Epoch 184/200, Train Loss: 1.4686, Val Loss: 1.7866\n",
      "Epoch 185/200, Train Loss: 1.4611, Val Loss: 1.5482\n",
      "Epoch 186/200, Train Loss: 1.3970, Val Loss: 1.5267\n",
      "Epoch 187/200, Train Loss: 1.3643, Val Loss: 1.5008\n",
      "Epoch 188/200, Train Loss: 1.3646, Val Loss: 1.4983\n",
      "Epoch 189/200, Train Loss: 1.3734, Val Loss: 1.4651\n",
      "  New best validation loss: 1.4651\n",
      "Epoch 190/200, Train Loss: 1.3980, Val Loss: 1.5343\n",
      "Epoch 191/200, Train Loss: 1.3416, Val Loss: 1.4756\n",
      "Epoch 192/200, Train Loss: 1.3558, Val Loss: 1.4957\n",
      "Epoch 193/200, Train Loss: 1.3433, Val Loss: 1.4884\n",
      "Epoch 194/200, Train Loss: 1.3369, Val Loss: 1.5230\n",
      "Epoch 195/200, Train Loss: 1.3775, Val Loss: 1.4823\n",
      "Epoch 196/200, Train Loss: 1.3895, Val Loss: 1.5845\n",
      "Epoch 197/200, Train Loss: 1.3833, Val Loss: 1.4738\n",
      "Epoch 198/200, Train Loss: 1.3417, Val Loss: 1.5238\n",
      "Epoch 199/200, Train Loss: 1.3473, Val Loss: 1.4965\n",
      "Epoch 200/200, Train Loss: 1.3378, Val Loss: 1.5045\n",
      "\n",
      "Loaded best model (Val Loss: 1.4651) for final hidden state extraction.\n",
      "Saved best model for NMRNN_NoSpatial_ModReadout to results/20250508_082527/NMRNN_NoSpatial_ModReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_NoSpatial_ModReadout ---\n",
      "  Analyzing decodability for NMRNN_NoSpatial_ModReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_NoSpatial_ModReadout - Test MSE: 0.0296, Test R2: 0.9856 (best alpha: 0.2976)\n",
      "Decodability (R2 score) for NMRNN_NoSpatial_ModReadout: 0.9856\n",
      "\n",
      "--- Training NMRNN_Spatial_FixedReadout ---\n",
      "Number of parameters: 66449\n",
      "Epoch 1/200, Train Loss: 4.9547, Val Loss: 5.1017\n",
      "  New best validation loss: 5.1017\n",
      "Epoch 2/200, Train Loss: 4.7681, Val Loss: 4.9848\n",
      "  New best validation loss: 4.9848\n",
      "Epoch 3/200, Train Loss: 4.6498, Val Loss: 4.9744\n",
      "  New best validation loss: 4.9744\n",
      "Epoch 4/200, Train Loss: 4.5583, Val Loss: 4.7323\n",
      "  New best validation loss: 4.7323\n",
      "Epoch 5/200, Train Loss: 4.4649, Val Loss: 4.6348\n",
      "  New best validation loss: 4.6348\n",
      "Epoch 6/200, Train Loss: 4.3317, Val Loss: 4.5233\n",
      "  New best validation loss: 4.5233\n",
      "Epoch 7/200, Train Loss: 4.1748, Val Loss: 4.4553\n",
      "  New best validation loss: 4.4553\n",
      "Epoch 8/200, Train Loss: 4.0285, Val Loss: 4.3778\n",
      "  New best validation loss: 4.3778\n",
      "Epoch 9/200, Train Loss: 3.8812, Val Loss: 4.1990\n",
      "  New best validation loss: 4.1990\n",
      "Epoch 10/200, Train Loss: 3.8068, Val Loss: 4.1288\n",
      "  New best validation loss: 4.1288\n",
      "Epoch 11/200, Train Loss: 3.7082, Val Loss: 4.0106\n",
      "  New best validation loss: 4.0106\n",
      "Epoch 12/200, Train Loss: 3.5043, Val Loss: 3.7815\n",
      "  New best validation loss: 3.7815\n",
      "Epoch 13/200, Train Loss: 3.3734, Val Loss: 3.6258\n",
      "  New best validation loss: 3.6258\n",
      "Epoch 14/200, Train Loss: 3.2796, Val Loss: 3.4758\n",
      "  New best validation loss: 3.4758\n",
      "Epoch 15/200, Train Loss: 3.1718, Val Loss: 3.4103\n",
      "  New best validation loss: 3.4103\n",
      "Epoch 16/200, Train Loss: 3.0445, Val Loss: 3.4812\n",
      "Epoch 17/200, Train Loss: 3.0812, Val Loss: 3.2006\n",
      "  New best validation loss: 3.2006\n",
      "Epoch 18/200, Train Loss: 2.8761, Val Loss: 3.1107\n",
      "  New best validation loss: 3.1107\n",
      "Epoch 19/200, Train Loss: 2.7468, Val Loss: 2.9030\n",
      "  New best validation loss: 2.9030\n",
      "Epoch 20/200, Train Loss: 2.6312, Val Loss: 3.0162\n",
      "Epoch 21/200, Train Loss: 2.6113, Val Loss: 2.7891\n",
      "  New best validation loss: 2.7891\n",
      "Epoch 22/200, Train Loss: 2.4734, Val Loss: 2.6305\n",
      "  New best validation loss: 2.6305\n",
      "Epoch 23/200, Train Loss: 2.4615, Val Loss: 2.6369\n",
      "Epoch 24/200, Train Loss: 2.3612, Val Loss: 2.5471\n",
      "  New best validation loss: 2.5471\n",
      "Epoch 25/200, Train Loss: 2.3134, Val Loss: 2.5064\n",
      "  New best validation loss: 2.5064\n",
      "Epoch 26/200, Train Loss: 2.2518, Val Loss: 2.4571\n",
      "  New best validation loss: 2.4571\n",
      "Epoch 27/200, Train Loss: 2.1869, Val Loss: 2.3842\n",
      "  New best validation loss: 2.3842\n",
      "Epoch 28/200, Train Loss: 2.1993, Val Loss: 2.3591\n",
      "  New best validation loss: 2.3591\n",
      "Epoch 29/200, Train Loss: 2.1872, Val Loss: 2.7976\n",
      "Epoch 30/200, Train Loss: 2.2386, Val Loss: 2.4040\n",
      "Epoch 31/200, Train Loss: 2.1281, Val Loss: 2.2658\n",
      "  New best validation loss: 2.2658\n",
      "Epoch 32/200, Train Loss: 2.0727, Val Loss: 2.2657\n",
      "  New best validation loss: 2.2657\n",
      "Epoch 33/200, Train Loss: 2.0772, Val Loss: 2.2379\n",
      "  New best validation loss: 2.2379\n",
      "Epoch 34/200, Train Loss: 1.9873, Val Loss: 2.1532\n",
      "  New best validation loss: 2.1532\n",
      "Epoch 35/200, Train Loss: 1.9707, Val Loss: 2.1692\n",
      "Epoch 36/200, Train Loss: 1.9364, Val Loss: 2.1106\n",
      "  New best validation loss: 2.1106\n",
      "Epoch 37/200, Train Loss: 1.9085, Val Loss: 2.0972\n",
      "  New best validation loss: 2.0972\n",
      "Epoch 38/200, Train Loss: 1.8899, Val Loss: 2.0417\n",
      "  New best validation loss: 2.0417\n",
      "Epoch 39/200, Train Loss: 1.8862, Val Loss: 2.1282\n",
      "Epoch 40/200, Train Loss: 1.8953, Val Loss: 2.0541\n",
      "Epoch 41/200, Train Loss: 1.8370, Val Loss: 2.0768\n",
      "Epoch 42/200, Train Loss: 1.8778, Val Loss: 2.0211\n",
      "  New best validation loss: 2.0211\n",
      "Epoch 43/200, Train Loss: 1.7953, Val Loss: 2.0245\n",
      "Epoch 44/200, Train Loss: 1.7861, Val Loss: 2.0078\n",
      "  New best validation loss: 2.0078\n",
      "Epoch 45/200, Train Loss: 1.7969, Val Loss: 1.9838\n",
      "  New best validation loss: 1.9838\n",
      "Epoch 46/200, Train Loss: 1.7623, Val Loss: 1.9301\n",
      "  New best validation loss: 1.9301\n",
      "Epoch 47/200, Train Loss: 1.7431, Val Loss: 1.9823\n",
      "Epoch 48/200, Train Loss: 1.7572, Val Loss: 1.9241\n",
      "  New best validation loss: 1.9241\n",
      "Epoch 49/200, Train Loss: 1.7010, Val Loss: 1.9116\n",
      "  New best validation loss: 1.9116\n",
      "Epoch 50/200, Train Loss: 1.6867, Val Loss: 1.9051\n",
      "  New best validation loss: 1.9051\n",
      "Epoch 51/200, Train Loss: 1.6727, Val Loss: 1.8707\n",
      "  New best validation loss: 1.8707\n",
      "Epoch 52/200, Train Loss: 1.6676, Val Loss: 1.8436\n",
      "  New best validation loss: 1.8436\n",
      "Epoch 53/200, Train Loss: 1.6596, Val Loss: 1.8604\n",
      "Epoch 54/200, Train Loss: 1.6476, Val Loss: 1.8289\n",
      "  New best validation loss: 1.8289\n",
      "Epoch 55/200, Train Loss: 1.6214, Val Loss: 1.8389\n",
      "Epoch 56/200, Train Loss: 1.6084, Val Loss: 1.8144\n",
      "  New best validation loss: 1.8144\n",
      "Epoch 57/200, Train Loss: 1.6380, Val Loss: 1.8895\n",
      "Epoch 58/200, Train Loss: 1.6326, Val Loss: 1.8017\n",
      "  New best validation loss: 1.8017\n",
      "Epoch 59/200, Train Loss: 1.6040, Val Loss: 1.8563\n",
      "Epoch 60/200, Train Loss: 1.6163, Val Loss: 1.8258\n",
      "Epoch 61/200, Train Loss: 1.5719, Val Loss: 1.7906\n",
      "  New best validation loss: 1.7906\n",
      "Epoch 62/200, Train Loss: 1.5519, Val Loss: 1.7810\n",
      "  New best validation loss: 1.7810\n",
      "Epoch 63/200, Train Loss: 1.5733, Val Loss: 1.7398\n",
      "  New best validation loss: 1.7398\n",
      "Epoch 64/200, Train Loss: 1.6127, Val Loss: 1.8349\n",
      "Epoch 65/200, Train Loss: 1.5932, Val Loss: 1.7830\n",
      "Epoch 66/200, Train Loss: 1.5450, Val Loss: 1.7883\n",
      "Epoch 67/200, Train Loss: 1.5358, Val Loss: 1.7270\n",
      "  New best validation loss: 1.7270\n",
      "Epoch 68/200, Train Loss: 1.5285, Val Loss: 1.7183\n",
      "  New best validation loss: 1.7183\n",
      "Epoch 69/200, Train Loss: 1.5110, Val Loss: 1.7459\n",
      "Epoch 70/200, Train Loss: 1.5175, Val Loss: 1.7007\n",
      "  New best validation loss: 1.7007\n",
      "Epoch 71/200, Train Loss: 1.5082, Val Loss: 1.6985\n",
      "  New best validation loss: 1.6985\n",
      "Epoch 72/200, Train Loss: 1.4775, Val Loss: 1.6698\n",
      "  New best validation loss: 1.6698\n",
      "Epoch 73/200, Train Loss: 1.4584, Val Loss: 1.6941\n",
      "Epoch 74/200, Train Loss: 1.4493, Val Loss: 1.6917\n",
      "Epoch 75/200, Train Loss: 1.4667, Val Loss: 1.6671\n",
      "  New best validation loss: 1.6671\n",
      "Epoch 76/200, Train Loss: 1.4555, Val Loss: 1.6394\n",
      "  New best validation loss: 1.6394\n",
      "Epoch 77/200, Train Loss: 1.4603, Val Loss: 1.6795\n",
      "Epoch 78/200, Train Loss: 1.4602, Val Loss: 1.7105\n",
      "Epoch 79/200, Train Loss: 1.4455, Val Loss: 1.6425\n",
      "Epoch 80/200, Train Loss: 1.4257, Val Loss: 1.6376\n",
      "  New best validation loss: 1.6376\n",
      "Epoch 81/200, Train Loss: 1.4153, Val Loss: 1.6739\n",
      "Epoch 82/200, Train Loss: 1.4110, Val Loss: 1.6119\n",
      "  New best validation loss: 1.6119\n",
      "Epoch 83/200, Train Loss: 1.4104, Val Loss: 1.6493\n",
      "Epoch 84/200, Train Loss: 1.3921, Val Loss: 1.6319\n",
      "Epoch 85/200, Train Loss: 1.3821, Val Loss: 1.5986\n",
      "  New best validation loss: 1.5986\n",
      "Epoch 86/200, Train Loss: 1.3882, Val Loss: 1.5836\n",
      "  New best validation loss: 1.5836\n",
      "Epoch 87/200, Train Loss: 1.3832, Val Loss: 1.6019\n",
      "Epoch 88/200, Train Loss: 1.3583, Val Loss: 1.5756\n",
      "  New best validation loss: 1.5756\n",
      "Epoch 89/200, Train Loss: 1.3498, Val Loss: 1.6143\n",
      "Epoch 90/200, Train Loss: 1.3510, Val Loss: 1.5714\n",
      "  New best validation loss: 1.5714\n",
      "Epoch 91/200, Train Loss: 1.3379, Val Loss: 1.6052\n",
      "Epoch 92/200, Train Loss: 1.3587, Val Loss: 1.5409\n",
      "  New best validation loss: 1.5409\n",
      "Epoch 93/200, Train Loss: 1.3193, Val Loss: 1.5509\n",
      "Epoch 94/200, Train Loss: 1.3063, Val Loss: 1.5285\n",
      "  New best validation loss: 1.5285\n",
      "Epoch 95/200, Train Loss: 1.2907, Val Loss: 1.5293\n",
      "Epoch 96/200, Train Loss: 1.2973, Val Loss: 1.4959\n",
      "  New best validation loss: 1.4959\n",
      "Epoch 97/200, Train Loss: 1.2902, Val Loss: 1.5698\n",
      "Epoch 98/200, Train Loss: 1.3235, Val Loss: 1.5189\n",
      "Epoch 99/200, Train Loss: 1.2811, Val Loss: 1.5174\n",
      "Epoch 100/200, Train Loss: 1.2760, Val Loss: 1.5084\n",
      "Epoch 101/200, Train Loss: 1.2748, Val Loss: 1.5612\n",
      "Epoch 102/200, Train Loss: 1.2678, Val Loss: 1.5237\n",
      "Epoch 103/200, Train Loss: 1.2743, Val Loss: 1.5565\n",
      "Epoch 104/200, Train Loss: 1.2572, Val Loss: 1.4707\n",
      "  New best validation loss: 1.4707\n",
      "Epoch 105/200, Train Loss: 1.2534, Val Loss: 1.4924\n",
      "Epoch 106/200, Train Loss: 1.2662, Val Loss: 1.4849\n",
      "Epoch 107/200, Train Loss: 1.2350, Val Loss: 1.5074\n",
      "Epoch 108/200, Train Loss: 1.2354, Val Loss: 1.4743\n",
      "Epoch 109/200, Train Loss: 1.2358, Val Loss: 1.5265\n",
      "Epoch 110/200, Train Loss: 1.2502, Val Loss: 1.5007\n",
      "Epoch 111/200, Train Loss: 1.2217, Val Loss: 1.4953\n",
      "Epoch 112/200, Train Loss: 1.2228, Val Loss: 1.4482\n",
      "  New best validation loss: 1.4482\n",
      "Epoch 113/200, Train Loss: 1.2122, Val Loss: 1.4639\n",
      "Epoch 114/200, Train Loss: 1.2003, Val Loss: 1.4767\n",
      "Epoch 115/200, Train Loss: 1.2101, Val Loss: 1.4908\n",
      "Epoch 116/200, Train Loss: 1.1981, Val Loss: 1.4490\n",
      "Epoch 117/200, Train Loss: 1.2190, Val Loss: 1.5164\n",
      "Epoch 118/200, Train Loss: 1.1958, Val Loss: 1.4287\n",
      "  New best validation loss: 1.4287\n",
      "Epoch 119/200, Train Loss: 1.1815, Val Loss: 1.4942\n",
      "Epoch 120/200, Train Loss: 1.1808, Val Loss: 1.4089\n",
      "  New best validation loss: 1.4089\n",
      "Epoch 121/200, Train Loss: 1.1710, Val Loss: 1.4353\n",
      "Epoch 122/200, Train Loss: 1.1531, Val Loss: 1.4067\n",
      "  New best validation loss: 1.4067\n",
      "Epoch 123/200, Train Loss: 1.1604, Val Loss: 1.4328\n",
      "Epoch 124/200, Train Loss: 1.1793, Val Loss: 1.4465\n",
      "Epoch 125/200, Train Loss: 1.1876, Val Loss: 1.3877\n",
      "  New best validation loss: 1.3877\n",
      "Epoch 126/200, Train Loss: 1.1595, Val Loss: 1.4352\n",
      "Epoch 127/200, Train Loss: 1.1547, Val Loss: 1.4157\n",
      "Epoch 128/200, Train Loss: 1.1584, Val Loss: 1.3886\n",
      "Epoch 129/200, Train Loss: 1.1279, Val Loss: 1.4516\n",
      "Epoch 130/200, Train Loss: 1.1445, Val Loss: 1.3863\n",
      "  New best validation loss: 1.3863\n",
      "Epoch 131/200, Train Loss: 1.1147, Val Loss: 1.3545\n",
      "  New best validation loss: 1.3545\n",
      "Epoch 132/200, Train Loss: 1.1247, Val Loss: 1.3930\n",
      "Epoch 133/200, Train Loss: 1.1073, Val Loss: 1.3662\n",
      "Epoch 134/200, Train Loss: 1.1295, Val Loss: 1.3644\n",
      "Epoch 135/200, Train Loss: 1.1403, Val Loss: 1.3774\n",
      "Epoch 136/200, Train Loss: 1.0947, Val Loss: 1.4074\n",
      "Epoch 137/200, Train Loss: 1.1048, Val Loss: 1.3669\n",
      "Epoch 138/200, Train Loss: 1.0785, Val Loss: 1.3443\n",
      "  New best validation loss: 1.3443\n",
      "Epoch 139/200, Train Loss: 1.0688, Val Loss: 1.3447\n",
      "Epoch 140/200, Train Loss: 1.0673, Val Loss: 1.3214\n",
      "  New best validation loss: 1.3214\n",
      "Epoch 141/200, Train Loss: 1.0767, Val Loss: 1.3490\n",
      "Epoch 142/200, Train Loss: 1.0585, Val Loss: 1.3640\n",
      "Epoch 143/200, Train Loss: 1.0560, Val Loss: 1.3333\n",
      "Epoch 144/200, Train Loss: 1.0483, Val Loss: 1.3160\n",
      "  New best validation loss: 1.3160\n",
      "Epoch 145/200, Train Loss: 1.0535, Val Loss: 1.2967\n",
      "  New best validation loss: 1.2967\n",
      "Epoch 146/200, Train Loss: 1.0385, Val Loss: 1.3095\n",
      "Epoch 147/200, Train Loss: 1.0483, Val Loss: 1.2968\n",
      "Epoch 148/200, Train Loss: 1.0614, Val Loss: 1.3130\n",
      "Epoch 149/200, Train Loss: 1.0427, Val Loss: 1.3221\n",
      "Epoch 150/200, Train Loss: 1.0273, Val Loss: 1.2867\n",
      "  New best validation loss: 1.2867\n",
      "Epoch 151/200, Train Loss: 1.0378, Val Loss: 1.3254\n",
      "Epoch 152/200, Train Loss: 1.0359, Val Loss: 1.2897\n",
      "Epoch 153/200, Train Loss: 1.0175, Val Loss: 1.2768\n",
      "  New best validation loss: 1.2768\n",
      "Epoch 154/200, Train Loss: 1.0009, Val Loss: 1.3101\n",
      "Epoch 155/200, Train Loss: 1.0116, Val Loss: 1.2953\n",
      "Epoch 156/200, Train Loss: 1.0015, Val Loss: 1.2799\n",
      "Epoch 157/200, Train Loss: 0.9997, Val Loss: 1.2755\n",
      "  New best validation loss: 1.2755\n",
      "Epoch 158/200, Train Loss: 0.9806, Val Loss: 1.2503\n",
      "  New best validation loss: 1.2503\n",
      "Epoch 159/200, Train Loss: 0.9723, Val Loss: 1.2556\n",
      "Epoch 160/200, Train Loss: 0.9663, Val Loss: 1.2070\n",
      "  New best validation loss: 1.2070\n",
      "Epoch 161/200, Train Loss: 0.9650, Val Loss: 1.2496\n",
      "Epoch 162/200, Train Loss: 0.9585, Val Loss: 1.2215\n",
      "Epoch 163/200, Train Loss: 0.9538, Val Loss: 1.2067\n",
      "  New best validation loss: 1.2067\n",
      "Epoch 164/200, Train Loss: 0.9503, Val Loss: 1.2198\n",
      "Epoch 165/200, Train Loss: 0.9387, Val Loss: 1.1855\n",
      "  New best validation loss: 1.1855\n",
      "Epoch 166/200, Train Loss: 0.9396, Val Loss: 1.1851\n",
      "  New best validation loss: 1.1851\n",
      "Epoch 167/200, Train Loss: 0.9259, Val Loss: 1.2000\n",
      "Epoch 168/200, Train Loss: 0.9326, Val Loss: 1.1765\n",
      "  New best validation loss: 1.1765\n",
      "Epoch 169/200, Train Loss: 0.9187, Val Loss: 1.1781\n",
      "Epoch 170/200, Train Loss: 0.9146, Val Loss: 1.1736\n",
      "  New best validation loss: 1.1736\n",
      "Epoch 171/200, Train Loss: 0.9094, Val Loss: 1.1669\n",
      "  New best validation loss: 1.1669\n",
      "Epoch 172/200, Train Loss: 0.9109, Val Loss: 1.2001\n",
      "Epoch 173/200, Train Loss: 0.9127, Val Loss: 1.1669\n",
      "Epoch 174/200, Train Loss: 0.9108, Val Loss: 1.1947\n",
      "Epoch 175/200, Train Loss: 0.9035, Val Loss: 1.1576\n",
      "  New best validation loss: 1.1576\n",
      "Epoch 176/200, Train Loss: 0.8930, Val Loss: 1.1473\n",
      "  New best validation loss: 1.1473\n",
      "Epoch 177/200, Train Loss: 0.8819, Val Loss: 1.1349\n",
      "  New best validation loss: 1.1349\n",
      "Epoch 178/200, Train Loss: 0.8823, Val Loss: 1.1435\n",
      "Epoch 179/200, Train Loss: 0.8758, Val Loss: 1.1386\n",
      "Epoch 180/200, Train Loss: 0.8708, Val Loss: 1.1214\n",
      "  New best validation loss: 1.1214\n",
      "Epoch 181/200, Train Loss: 0.8622, Val Loss: 1.0981\n",
      "  New best validation loss: 1.0981\n",
      "Epoch 182/200, Train Loss: 0.8562, Val Loss: 1.0877\n",
      "  New best validation loss: 1.0877\n",
      "Epoch 183/200, Train Loss: 0.8642, Val Loss: 1.0906\n",
      "Epoch 184/200, Train Loss: 0.8614, Val Loss: 1.1895\n",
      "Epoch 185/200, Train Loss: 0.8652, Val Loss: 1.0992\n",
      "Epoch 186/200, Train Loss: 0.8489, Val Loss: 1.0880\n",
      "Epoch 187/200, Train Loss: 0.8371, Val Loss: 1.0909\n",
      "Epoch 188/200, Train Loss: 0.8415, Val Loss: 1.0841\n",
      "  New best validation loss: 1.0841\n",
      "Epoch 189/200, Train Loss: 0.8367, Val Loss: 1.0785\n",
      "  New best validation loss: 1.0785\n",
      "Epoch 190/200, Train Loss: 0.8319, Val Loss: 1.1134\n",
      "Epoch 191/200, Train Loss: 0.8470, Val Loss: 1.0693\n",
      "  New best validation loss: 1.0693\n",
      "Epoch 192/200, Train Loss: 0.8155, Val Loss: 1.0880\n",
      "Epoch 193/200, Train Loss: 0.8215, Val Loss: 1.1075\n",
      "Epoch 194/200, Train Loss: 0.8121, Val Loss: 1.0871\n",
      "Epoch 195/200, Train Loss: 0.8192, Val Loss: 1.0646\n",
      "  New best validation loss: 1.0646\n",
      "Epoch 196/200, Train Loss: 0.8162, Val Loss: 1.1040\n",
      "Epoch 197/200, Train Loss: 0.8327, Val Loss: 1.0564\n",
      "  New best validation loss: 1.0564\n",
      "Epoch 198/200, Train Loss: 0.8035, Val Loss: 1.0366\n",
      "  New best validation loss: 1.0366\n",
      "Epoch 199/200, Train Loss: 0.7933, Val Loss: 1.0340\n",
      "  New best validation loss: 1.0340\n",
      "Epoch 200/200, Train Loss: 0.7838, Val Loss: 1.0241\n",
      "  New best validation loss: 1.0241\n",
      "\n",
      "Loaded best model (Val Loss: 1.0241) for final hidden state extraction.\n",
      "Saved best model for NMRNN_Spatial_FixedReadout to results/20250508_082527/NMRNN_Spatial_FixedReadout_best.pt\n",
      "\n",
      "--- Performing Decodability Analysis for NMRNN_Spatial_FixedReadout ---\n",
      "  Analyzing decodability for NMRNN_Spatial_FixedReadout...\n",
      "  Hidden states shape: torch.Size([160, 200, 128])\n",
      "  Coefficients shape: torch.Size([160, 5])\n",
      "  Processed hidden states shape for decoder: torch.Size([160, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RidgeCV Decoder for NMRNN_Spatial_FixedReadout - Test MSE: 0.1035, Test R2: 0.9497 (best alpha: 2.6367)\n",
      "Decodability (R2 score) for NMRNN_Spatial_FixedReadout: 0.9497\n",
      "Learning curves saved to results/20250508_082527/learning_curves.png\n",
      "\n",
      "Learning curves plotted to results/20250508_082527/learning_curves.png\n",
      "\n",
      "--- Decodability Results (R2 Score) ---\n",
      "ComplexOscillatorNet: 0.9709\n",
      "RNN_GRU: 0.9988\n",
      "Transformer: 0.9955\n",
      "HIPPORNN_LegT: 0.9979\n",
      "NMRNN_Spatial_ModReadout: 0.9732\n",
      "NMRNN_NoSpatial_ModReadout: 0.9856\n",
      "NMRNN_Spatial_FixedReadout: 0.9497\n",
      "\n",
      "Experiment finished. All results in results/20250508_082527\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Project specific imports\n",
    "from dataset import CompositionalDataset, create_dataloaders\n",
    "from model import NonlinearOscillatorNet, RNNModel, TransformerModel, HippoRNNModel, NMRNN_Spatial_ModulatedReadout, NMRNN_NoSpatial_ModulatedReadout, NMRNN_Spatial_FixedReadout\n",
    "from training import train_model_comparative\n",
    "from analysis import plot_learning_curves, perform_decodability_analysis\n",
    "from utils import set_seed, get_device, count_parameters\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"seed\": 0,\n",
    "    \"num_task_coefficients\": 5, \n",
    "    \"seq_length\": 200,          \n",
    "    \"train_samples\": 32 * 10,  # Reduced for quicker testing, increase for real runs\n",
    "    \"test_samples\": 32 * 5,   # Reduced for quicker testing\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200, # Reduced for quick test, increase for real runs (e.g., 50-200)\n",
    "    \"lr\": 1e-3,\n",
    "    \n",
    "    # Model-specific hidden sizes / main dimension\n",
    "    \"hidden_size_oscillator\": 64, \n",
    "    \"hidden_size_rnn\": 128,       # For standard GRU/LSTM\n",
    "    \"d_model_transformer\": 64,   \n",
    "    \"hidden_size_hippo\": 128,     # N for HIPPO\n",
    "    \"hidden_size_nm_rnn\": 128,    # n_rnn for nmRNN variants\n",
    "\n",
    "    # Transformer specific\n",
    "    \"nhead_transformer\": 1,\n",
    "    \"num_layers_transformer\": 1,\n",
    "    \n",
    "    # HIPPORNN specific\n",
    "    \"hippo_method\": 'legt', # 'legs' or 'legt'\n",
    "    \"hippo_theta\": 1.0,     # Required for 'legt'\n",
    "    \"hippo_dt\": 1.0 / 200,  # Discretization step for HIPPO (e.g., 1.0 / seq_length)\n",
    "    \"hippo_inv_eps\": 1e-6, # Epsilon for LegS matrix inversion regularization\n",
    "    \"hippo_clip_val\": 50.0, # Clipping for HIPPO state c_t\n",
    "\n",
    "    # nmRNN specific (shared for variants where applicable)\n",
    "    \"nm_N_NM\": 4,               # Number of neuromodulators\n",
    "    \"nm_activation\": 'tanh',    # 'relu', 'tanh' (original code had 'relu-tanh', simplified here)\n",
    "    \"nm_decay\": 0.05, # dt_sec / tau_rnn, e.g., (20ms/step) / (100ms tau) -> exp(-0.2)\n",
    "                                     # Original: math.exp(-20/100) - assuming 20ms step, 100ms tau\n",
    "    \"nm_bias\": True,\n",
    "    \"nm_keepW0_spatial\": False, # For the version with spatial connections\n",
    "    \"nm_keepW0_no_spatial\": False,\n",
    "    \"nm_grad_clip\": 1.0,\n",
    "    \"nm_spatial_ell\": 0.1,      # For SpatialWeight\n",
    "    \"nm_spatial_scale\": 1.0,    # For SpatialWeight\n",
    "\n",
    "    # General task params\n",
    "    \"output_dim\": 1, \n",
    "    \"input_dim\": 1,  \n",
    "    \"noise_level_data\": 0.01,\n",
    "    \"run_timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"results_dir\": \"results\"\n",
    "}\n",
    "\n",
    "def run_experiment():\n",
    "    \"\"\"\n",
    "    Runs the full comparative analysis experiment.\n",
    "    \"\"\"\n",
    "    set_seed(CONFIG[\"seed\"])\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    os.makedirs(CONFIG[\"results_dir\"], exist_ok=True)\n",
    "    run_results_dir = os.path.join(CONFIG[\"results_dir\"], CONFIG[\"run_timestamp\"])\n",
    "    os.makedirs(run_results_dir, exist_ok=True)\n",
    "    print(f\"Results will be saved in: {run_results_dir}\")\n",
    "\n",
    "    # --- 1. Dataset ---\n",
    "    print(\"Loading dataset...\")\n",
    "    train_loader, val_loader, test_loader, (input_basis, output_basis) = create_dataloaders(\n",
    "        num_train_samples=CONFIG[\"train_samples\"],\n",
    "        num_val_samples=CONFIG[\"test_samples\"], \n",
    "        num_test_samples=CONFIG[\"test_samples\"],\n",
    "        num_basis=CONFIG[\"num_task_coefficients\"],\n",
    "        seq_length=CONFIG[\"seq_length\"],\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        noise=CONFIG[\"noise_level_data\"]\n",
    "    )\n",
    "    print(\"Dataset loaded.\")\n",
    "\n",
    "    # --- 2. Models ---\n",
    "    models_to_test = {\n",
    "        \"ComplexOscillatorNet\": NonlinearOscillatorNet(\n",
    "            N_oscillators=CONFIG[\"hidden_size_oscillator\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            seq_length=CONFIG[\"seq_length\"], \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"RNN_GRU\": RNNModel(\n",
    "            hidden_size=CONFIG[\"hidden_size_rnn\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            num_layers=1, \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"Transformer\": TransformerModel(\n",
    "            d_model=CONFIG[\"d_model_transformer\"],\n",
    "            device=device,\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            num_heads=CONFIG[\"nhead_transformer\"],\n",
    "            num_layers=CONFIG[\"num_layers_transformer\"],\n",
    "            seq_length=CONFIG[\"seq_length\"], \n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"HIPPORNN_LegT\": HippoRNNModel( # Using LegT by default as per config\n",
    "            hidden_size=CONFIG[\"hidden_size_hippo\"],\n",
    "            outputdim=CONFIG[\"output_dim\"],\n",
    "            inputdim=CONFIG[\"input_dim\"],\n",
    "            method=CONFIG[\"hippo_method\"], \n",
    "            theta=CONFIG[\"hippo_theta\"],\n",
    "            dt=CONFIG[\"hippo_dt\"],\n",
    "            inv_eps=CONFIG[\"hippo_inv_eps\"],\n",
    "            clip_val=CONFIG[\"hippo_clip_val\"],\n",
    "            device=device, # Pass device\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_Spatial_ModReadout\": NMRNN_Spatial_ModulatedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"],\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_spatial\"],\n",
    "            spatial_ell=CONFIG[\"nm_spatial_ell\"],\n",
    "            spatial_scale=CONFIG[\"nm_spatial_scale\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_NoSpatial_ModReadout\": NMRNN_NoSpatial_ModulatedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"],\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_no_spatial\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "        \"NMRNN_Spatial_FixedReadout\": NMRNN_Spatial_FixedReadout(\n",
    "            input_size=CONFIG[\"input_dim\"],\n",
    "            hidden_size=CONFIG[\"hidden_size_nm_rnn\"],\n",
    "            output_size=CONFIG[\"output_dim\"],\n",
    "            N_nm=CONFIG[\"nm_N_NM\"], # N_nm still needed for the core recurrence, just not readout\n",
    "            activation_fn_name=CONFIG[\"nm_activation\"],\n",
    "            decay=CONFIG[\"nm_decay\"],\n",
    "            bias=CONFIG[\"nm_bias\"],\n",
    "            keepW0=CONFIG[\"nm_keepW0_spatial\"],\n",
    "            spatial_ell=CONFIG[\"nm_spatial_ell\"],\n",
    "            spatial_scale=CONFIG[\"nm_spatial_scale\"],\n",
    "            grad_clip=CONFIG[\"nm_grad_clip\"],\n",
    "            device=device,\n",
    "            seed=CONFIG[\"seed\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    all_val_losses = {}\n",
    "    decodability_results = {}\n",
    "    trained_models_paths = {}\n",
    "\n",
    "    # --- 3. Training & Evaluation Loop ---\n",
    "    for model_name, model in models_to_test.items():\n",
    "        print(f\"\\n--- Training {model_name} ---\")\n",
    "        model.to(device)\n",
    "        print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "\n",
    "        try:\n",
    "            val_losses, best_model_state, hidden_states_test, coeffs_test = train_model_comparative(\n",
    "                model,\n",
    "                model_name,\n",
    "                train_loader,\n",
    "                val_loader, \n",
    "                test_loader, \n",
    "                CONFIG[\"epochs\"],\n",
    "                CONFIG[\"lr\"],\n",
    "                device,\n",
    "                CONFIG[\"num_task_coefficients\"], \n",
    "                run_results_dir,\n",
    "                plot_intermediate_results=(len(models_to_test) == 1) \n",
    "            )\n",
    "            all_val_losses[model_name] = val_losses\n",
    "            \n",
    "            if best_model_state:\n",
    "                model_path = os.path.join(run_results_dir, f\"{model_name}_best.pt\")\n",
    "                torch.save(best_model_state, model_path)\n",
    "                trained_models_paths[model_name] = model_path\n",
    "                print(f\"Saved best model for {model_name} to {model_path}\")\n",
    "            else:\n",
    "                print(f\"No best model state saved for {model_name} (possibly due to training issues).\")\n",
    "\n",
    "            # --- 4. Decodability Analysis ---\n",
    "            if hidden_states_test is not None and coeffs_test is not None:\n",
    "                print(f\"\\n--- Performing Decodability Analysis for {model_name} ---\")\n",
    "                decodability_score = perform_decodability_analysis(\n",
    "                    model_name=model_name, \n",
    "                    hidden_states=hidden_states_test, \n",
    "                    coefficients=coeffs_test,       \n",
    "                    decoder_type='ridge', # Using RidgeCV as a robust default\n",
    "                    decoding_metric='r2', # R-squared is often more interpretable than MSE here\n",
    "                    results_dir=run_results_dir,\n",
    "                    device=device,\n",
    "                )\n",
    "                decodability_results[model_name] = decodability_score\n",
    "                print(f\"Decodability (R2 score) for {model_name}: {decodability_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"Skipping decodability for {model_name} due to missing hidden states or coefficients.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"!!!!!! ERROR during training or analysis for {model_name}: {e} !!!!!!\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            all_val_losses[model_name] = [float('nan')] * CONFIG[\"epochs\"] # Log error for this model\n",
    "            decodability_results[model_name] = float('nan')\n",
    "\n",
    "\n",
    "    # --- 5. Plot Learning Curves ---\n",
    "    if any(all_val_losses.values()): # Check if there's anything to plot\n",
    "        plot_learning_curves(all_val_losses, title=\"Validation Learning Curves\", save_path=os.path.join(run_results_dir, \"learning_curves.png\"))\n",
    "        print(f\"\\nLearning curves plotted to {os.path.join(run_results_dir, 'learning_curves.png')}\")\n",
    "\n",
    "    # --- 6. Report Decodability ---\n",
    "    print(\"\\n--- Decodability Results (R2 Score) ---\")\n",
    "    if decodability_results:\n",
    "        for model_name, score in decodability_results.items():\n",
    "            print(f\"{model_name}: {score:.4f}\")\n",
    "        with open(os.path.join(run_results_dir, \"decodability_summary.txt\"), \"w\") as f:\n",
    "            f.write(\"Model,R2_Score\\n\")\n",
    "            for model_name, score in decodability_results.items():\n",
    "                f.write(f\"{model_name},{score:.4f}\\n\")\n",
    "    else:\n",
    "        print(\"No decodability results to report.\")\n",
    "        \n",
    "    print(f\"\\nExperiment finished. All results in {run_results_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ec43da-6ade-4862-81be-ed723ffe257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951229424500714"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-1.0 / 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba850af2-8fe7-4ac3-a31b-87c844aef3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
